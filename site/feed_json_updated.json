{"version": "https://jsonfeed.org/version/1", "title": "mondalogue", "home_page_url": "https://avishek-mondal.github.io/digital-garden/", "feed_url": "https://avishek-mondal.github.io/digital-garden/feed_json_updated.json", "description": null, "icon": null, "authors": [], "language": "en", "items": [{"id": "https://avishek-mondal.github.io/digital-garden/blog/2024/08/09/various-observations-on-multiprocessing-in-python/", "url": "https://avishek-mondal.github.io/digital-garden/blog/2024/08/09/various-observations-on-multiprocessing-in-python/", "title": "Various observations on multiprocessing in Python", "content_html": "<p>Here are a number of observations about various \"interesting\" behaviours in Python I have collected over the years:</p>", "image": null, "date_modified": "2024-12-10T22:59:42.571326+00:00", "authors": [], "tags": []}, {"id": "https://avishek-mondal.github.io/digital-garden/blog/2024/07/02/rope-embeddings/", "url": "https://avishek-mondal.github.io/digital-garden/blog/2024/07/02/rope-embeddings/", "title": "RoPE embeddings", "content_html": "<p>Rotary positional embeddings (RoPE) is yet another tool people use to improve the performance of BERT based models.</p>\n<p>For a description of what embeddings are, and how they get used in attention/transformers, feel free to take a look at a previous post - <a href=\"./Self attention.md#embedding\">Self attention#Embedding</a> - for more information. The main point of an embedding module is to serve as a lookup table, mapping various kinds of integer ids - token ids, position ids etc. - into vectors that will then get used in the attention mechanism.</p>\n<p>While <em>token</em> embeddings are one part of the puzzle, it is also important to inject the notion of a token's position in a sequence in the inputs to the attention mechanism. This injection is the subject of various methods in <em>positional</em> embeddings/encodings. </p>", "image": null, "date_modified": "2024-08-06T16:13:05+00:00", "authors": [], "tags": []}, {"id": "https://avishek-mondal.github.io/digital-garden/blog/2024/08/05/lipschitz-constant-of-a-linear-transformation/", "url": "https://avishek-mondal.github.io/digital-garden/blog/2024/08/05/lipschitz-constant-of-a-linear-transformation/", "title": "Lipschitz constant of a linear transformation", "content_html": "<p>DISCLAIMER: If you spot an error, please feel free to email me. </p>\n<p>The \"Understanding Deep Learning\" book has recently come out (you can look at it <a href=\"https://udlbook.github.io/udlbook/\">here</a>, this post refers to the 2023-05-08 version), and is a great resource. In the appendices, it contains several statements which can be non-obvious to those of us who have been out of touch with linear algebra in our day jobs. </p>\n<p>Here is one such statement: </p>\n<p>\"The Lipschitz constant of a linear transformation $f[z] = Az + b$ is equal to the maximum eigenvalue of the matrix A.\"</p>\n<p>This is not an obvious result at all. Let us try and break this down step by step.</p>", "image": null, "date_modified": "2024-08-05T01:58:12+00:00", "authors": [], "tags": []}, {"id": "https://avishek-mondal.github.io/digital-garden/blog/2024/01/05/self-attention/", "url": "https://avishek-mondal.github.io/digital-garden/blog/2024/01/05/self-attention/", "title": "Self Attention", "content_html": "<h1>How does self attention work?</h1>\n<p>Self-attention is the cornerstone of a transformer block. We are going to go through the intuition behind the following steps:</p>\n<ol>\n<li>Tokenization</li>\n<li>Embedding</li>\n<li>The self attention mechanism</li>\n</ol>\n<p>by using raw, non-vectorised Python code as much as possible.</p>", "image": null, "date_modified": "2024-08-04T19:39:46+00:00", "authors": [], "tags": []}, {"id": "https://avishek-mondal.github.io/digital-garden/blog/2024/06/13/masking-in-transformers/", "url": "https://avishek-mondal.github.io/digital-garden/blog/2024/06/13/masking-in-transformers/", "title": "Masking in transformers", "content_html": "<p>Masking is pretty fundamental to how transformers work. The main purpose of masking is to make sure the model doesn't \"attend to\" some tokens. Feel free to read  my previous blog post about <a href=\"./Self attention.md\">Self attention</a>.</p>", "image": null, "date_modified": "2024-07-08T19:25:07+00:00", "authors": [], "tags": []}, {"id": "https://avishek-mondal.github.io/digital-garden/blog/2024/01/04/manacher-algo/", "url": "https://avishek-mondal.github.io/digital-garden/blog/2024/01/04/manacher-algo/", "title": "Manacher's Algorithm", "content_html": "<p>Super annoying algorithm, but it has uses in <a href=\"https://stackoverflow.com/questions/23861436/real-life-situations-using-palindrome-algorithm\">bioinfomatics</a>. </p>\n<p>Here's the task:\nGiven a string\u00a0<code>s</code>, return\u00a0the longest palindromic substring \u00a0in\u00a0<code>s</code>.</p>", "image": null, "date_modified": "2024-01-10T18:29:30+00:00", "authors": [], "tags": ["algorithms", "algorithms"]}]}