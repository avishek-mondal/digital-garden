{"version": "https://jsonfeed.org/version/1", "title": "mondalogue", "home_page_url": "https://avishek-mondal.github.io/digital-garden/", "feed_url": "https://avishek-mondal.github.io/digital-garden/feed_json_updated.json", "description": null, "icon": null, "authors": [], "language": "en", "items": [{"id": "https://avishek-mondal.github.io/digital-garden/blog/2024/07/02/rope-embeddings/", "url": "https://avishek-mondal.github.io/digital-garden/blog/2024/07/02/rope-embeddings/", "title": "RoPE embeddings", "content_html": "<p>Rotary positional embeddings (RoPE) is yet another tool people use to improve the performance of BERT based models.</p>\n<p>For a description of what embeddings are, and how they get used in attention/transformers, feel free to take a look at a previous post - <a href=\"./Self attention.md#embedding\">Self attention#Embedding</a> - for more information. The main point of an embedding module is to serve as a lookup table, mapping various kinds of integer ids - token ids, position ids etc. - into vectors that will then get used in the attention mechanism.</p>\n<p>While <em>token</em> embeddings are one part of the puzzle, it is also important to inject the notion of a token's position in a sequence in the inputs to the attention mechanism. This injection is the subject of various methods in <em>positional</em> embeddings/encodings. </p>", "image": null, "date_modified": "2024-08-04T19:39:46+00:00", "authors": [], "tags": []}, {"id": "https://avishek-mondal.github.io/digital-garden/blog/2024/01/05/self-attention/", "url": "https://avishek-mondal.github.io/digital-garden/blog/2024/01/05/self-attention/", "title": "Self Attention", "content_html": "<h1>How does self attention work?</h1>\n<p>Self-attention is the cornerstone of a transformer block. We are going to go through the intuition behind the following steps:</p>\n<ol>\n<li>Tokenization</li>\n<li>Embedding</li>\n<li>The self attention mechanism</li>\n</ol>\n<p>by using raw, non-vectorised Python code as much as possible.</p>", "image": null, "date_modified": "2024-08-04T19:39:46+00:00", "authors": [], "tags": []}, {"id": "https://avishek-mondal.github.io/digital-garden/blog/2024/06/13/masking-in-transformers/", "url": "https://avishek-mondal.github.io/digital-garden/blog/2024/06/13/masking-in-transformers/", "title": "Masking in transformers", "content_html": "<p>Masking is pretty fundamental to how transformers work. The main purpose of masking is to make sure the model doesn't \"attend to\" some tokens. Feel free to read  my previous blog post about <a href=\"./Self attention.md\">Self attention</a>.</p>", "image": null, "date_modified": "2024-07-08T19:25:07+00:00", "authors": [], "tags": []}, {"id": "https://avishek-mondal.github.io/digital-garden/blog/2024/01/04/manacher-algo/", "url": "https://avishek-mondal.github.io/digital-garden/blog/2024/01/04/manacher-algo/", "title": "Manacher's Algorithm", "content_html": "<p>Super annoying algorithm, but it has uses in <a href=\"https://stackoverflow.com/questions/23861436/real-life-situations-using-palindrome-algorithm\">bioinfomatics</a>. </p>\n<p>Here's the task:\nGiven a string\u00a0<code>s</code>, return\u00a0the longest palindromic substring \u00a0in\u00a0<code>s</code>.</p>", "image": null, "date_modified": "2024-01-10T18:29:30+00:00", "authors": [], "tags": ["algorithms", "algorithms"]}]}