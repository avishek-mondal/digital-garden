<?xml version="1.0" encoding="UTF-8" ?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"> <channel><title>mondalogue</title><link>https://avishek-mondal.github.io/digital-garden/</link><atom:link href="https://avishek-mondal.github.io/digital-garden/feed_rss_updated.xml" rel="self" type="application/rss+xml" /><language>en</language> <pubDate>Tue, 06 Aug 2024 14:22:59 -0000</pubDate> <lastBuildDate>Tue, 06 Aug 2024 14:22:59 -0000</lastBuildDate> <ttl>1440</ttl> <generator>MkDocs RSS plugin - v1.15.0</generator> <item> <title>RoPE embeddings</title> <description>&lt;p&gt;Rotary positional embeddings (RoPE) is yet another tool people use to improve the performance of BERT based models.&lt;/p&gt;&lt;p&gt;For a description of what embeddings are, and how they get used in attention/transformers, feel free to take a look at a previous post - &lt;a href=&#34;./Self attention.md#embedding&#34;&gt;Self attention#Embedding&lt;/a&gt; - for more information. The main point of an embedding module is to serve as a lookup table, mapping various kinds of integer ids - token ids, position ids etc. - into vectors that will then get used in the attention mechanism.&lt;/p&gt;&lt;p&gt;While &lt;em&gt;token&lt;/em&gt; embeddings are one part of the puzzle, it is also important to inject the notion of a token&#39;s position in a sequence in the inputs to the attention mechanism. This injection is the subject of various methods in &lt;em&gt;positional&lt;/em&gt; embeddings/encodings. &lt;/p&gt;</description><link>https://avishek-mondal.github.io/digital-garden/blog/2024/07/02/rope-embeddings/</link> <pubDate>Mon, 05 Aug 2024 02:04:49 +0000</pubDate><source url="https://avishek-mondal.github.io/digital-garden/feed_rss_updated.xml">mondalogue</source><guid isPermaLink="true">https://avishek-mondal.github.io/digital-garden/blog/2024/07/02/rope-embeddings/</guid> </item> <item> <title>Lipschitz constant of a linear transformation</title> <description>&lt;p&gt;DISCLAIMER: If you spot an error, please feel free to email me. &lt;/p&gt;&lt;p&gt;The &#34;Understanding Deep Learning&#34; book has recently come out (you can look at it &lt;a href=&#34;https://udlbook.github.io/udlbook/&#34;&gt;here&lt;/a&gt;, this post refers to the 2023-05-08 version), and is a great resource. In the appendices, it contains several statements which can be non-obvious to those of us who have been out of touch with linear algebra in our day jobs. &lt;/p&gt;&lt;p&gt;Here is one such statement: &lt;/p&gt;&lt;p&gt;&#34;The Lipschitz constant of a linear transformation $f[z] = Az + b$ is equal to the maximum eigenvalue of the matrix A.&#34;&lt;/p&gt;&lt;p&gt;This is not an obvious result at all. Let us try and break this down step by step.&lt;/p&gt;</description><link>https://avishek-mondal.github.io/digital-garden/blog/2024/08/05/lipschitz-constant-of-a-linear-transformation/</link> <pubDate>Mon, 05 Aug 2024 01:58:12 +0000</pubDate><source url="https://avishek-mondal.github.io/digital-garden/feed_rss_updated.xml">mondalogue</source><guid isPermaLink="true">https://avishek-mondal.github.io/digital-garden/blog/2024/08/05/lipschitz-constant-of-a-linear-transformation/</guid> </item> <item> <title>Self Attention</title> <description>&lt;h1&gt;How does self attention work?&lt;/h1&gt;&lt;p&gt;Self-attention is the cornerstone of a transformer block. We are going to go through the intuition behind the following steps:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Tokenization&lt;/li&gt;&lt;li&gt;Embedding&lt;/li&gt;&lt;li&gt;The self attention mechanism&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;by using raw, non-vectorised Python code as much as possible.&lt;/p&gt;</description><link>https://avishek-mondal.github.io/digital-garden/blog/2024/01/05/self-attention/</link> <pubDate>Sun, 04 Aug 2024 19:39:46 +0000</pubDate><source url="https://avishek-mondal.github.io/digital-garden/feed_rss_updated.xml">mondalogue</source><guid isPermaLink="true">https://avishek-mondal.github.io/digital-garden/blog/2024/01/05/self-attention/</guid> </item> <item> <title>Masking in transformers</title> <description>&lt;p&gt;Masking is pretty fundamental to how transformers work. The main purpose of masking is to make sure the model doesn&#39;t &#34;attend to&#34; some tokens. Feel free to read my previous blog post about &lt;a href=&#34;./Self attention.md&#34;&gt;Self attention&lt;/a&gt;.&lt;/p&gt;</description><link>https://avishek-mondal.github.io/digital-garden/blog/2024/06/13/masking-in-transformers/</link> <pubDate>Mon, 08 Jul 2024 19:25:07 +0000</pubDate><source url="https://avishek-mondal.github.io/digital-garden/feed_rss_updated.xml">mondalogue</source><guid isPermaLink="true">https://avishek-mondal.github.io/digital-garden/blog/2024/06/13/masking-in-transformers/</guid> </item> <item> <title>Manacher&#39;s Algorithm</title> <category>algorithms</category> <category>algorithms</category> <description>&lt;p&gt;Super annoying algorithm, but it has uses in &lt;a href=&#34;https://stackoverflow.com/questions/23861436/real-life-situations-using-palindrome-algorithm&#34;&gt;bioinfomatics&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;Here&#39;s the task:Given a string &lt;code&gt;s&lt;/code&gt;, return the longest palindromic substring  in &lt;code&gt;s&lt;/code&gt;.&lt;/p&gt;</description><link>https://avishek-mondal.github.io/digital-garden/blog/2024/01/04/manacher-algo/</link> <pubDate>Wed, 10 Jan 2024 18:29:30 +0000</pubDate><source url="https://avishek-mondal.github.io/digital-garden/feed_rss_updated.xml">mondalogue</source><guid isPermaLink="true">https://avishek-mondal.github.io/digital-garden/blog/2024/01/04/manacher-algo/</guid> </item> </channel></rss>