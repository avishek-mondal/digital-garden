{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"blog/","title":"Hello!","text":"<p>I post random things that are tangentially related to data structures and algorithms, deep learning, and other machine learning related topics.</p> <p>If you want more information about who I am, feel free to stalk me/reach out to me on LinkedIn.</p>"},{"location":"blog/2024/08/05/lipschitz-constant-of-a-linear-transformation/","title":"Lipschitz constant of a linear transformation","text":"<p>DISCLAIMER: If you spot an error, please feel free to email me. </p> <p>The \"Understanding Deep Learning\" book has recently come out (you can look at it here, this post refers to the 2023-05-08 version), and is a great resource. In the appendices, it contains several statements which can be non-obvious to those of us who have been out of touch with linear algebra in our day jobs. </p> <p>Here is one such statement: </p> <p>\"The Lipschitz constant of a linear transformation \\(f[z] = Az + b\\) is equal to the maximum eigenvalue of the matrix A.\"</p> <p>This is not an obvious result at all. Let us try and break this down step by step.</p>"},{"location":"blog/2024/08/05/lipschitz-constant-of-a-linear-transformation/#definitions","title":"Definitions","text":""},{"location":"blog/2024/08/05/lipschitz-constant-of-a-linear-transformation/#lipschitz-constant","title":"Lipschitz constant","text":"<p>A function \\(f[z]\\) is Lipschitz continuous if for all \\(z1,z2\\):</p> \\[ |f[z1] \u2212 f[z2]| \u2264 \u03b2|z1 \u2212 z2|, \\] <p>The value \\(\\beta\\) is called the Lipschitz constant. Both definitions are taken from the UDL book in the link above.</p>"},{"location":"blog/2024/08/05/lipschitz-constant-of-a-linear-transformation/#vector-norm","title":"Vector norm","text":"<p>The following definitions are lifted from this very useful resource here (Definition 3)</p> <p>Let \\(\\mathcal{V}\\) be a vector space over a field \\(\\mathbb{R}\\). A function \\(\\lvert{.}\\rvert : \\mathcal{V} \u2192 \\mathbb{R}\\) is called a (vector) norm if</p> <ol> <li>\\(\\lvert x \\rvert \u2265 0\\) for all \\(x \u2208 \\mathcal{V}\\) , with equality iff x = 0, [positivity]</li> <li>\\(\\lvert \u03bbx \\rvert = \\lvert\u03bb \\rvert \\lvert x \\rvert\\) for all \u03bb \u2208 \\(\\mathbb{K}\\) and x \u2208 V , [homogeneity] (reminder \\(\\mathbb{K} \u2208 \\{\\mathbb{R}, \\mathbb{C} \\}\\), i.e. either real or complex)</li> <li>\\(\\lvert x + y \\rvert \u2264 \\lvert x \\rvert + \\lvert y \\rvert\\) for all x, y \u2208 V . [triangle inequality]</li> </ol> <p>An example of such a norm would be an expression as follows for \\(p \\geq 1\\): </p> \\[ |x|_p = [|x_1|^p + |x_2|^p + |x_3|^p + ... ]^{\\frac{1}{p}} \\]"},{"location":"blog/2024/08/05/lipschitz-constant-of-a-linear-transformation/#matrix-norm","title":"Matrix norm","text":"<p>Also from the above resource (Definition 7)</p> <p>A function  \\(|| .||: \\mathcal{M} \\rightarrow \\mathbb{C}\\)  is a matrix norm if:</p> <ol> <li> <p>\\(||A|| \\geq 0\\) for all  \\(A \\in \\mathcal{M}\\)with equality iff x = 0 [positivity]</p> </li> <li> <p>\\(||\\lambda A|| = |\\lambda| ||A||\\) for all \\(\\lambda \\in \\mathbb{C}\\)and \\(A \\in \\mathcal{M}\\)[homogeneity]</p> </li> </ol> <p>3.\\(||A + B|| \\leq ||A|| + ||B||\\) for all \\(A, B \\in \\mathcal{M}\\) [triangle inequality]</p> <p>4.\\(||AB|| \\leq ||A|| ||B||\\) for all \\(A, B \\in \\mathcal{M}\\) [submultiplicativity]</p> <p>The last property, submultiplicativity, deserves some more attention. It uses a more general definition of matrix norm which is </p> \\[ ||A|| = \\text{sup}\\{|Ax|: x \\in \\mathcal{V}, |x| \\leq 1 \\} \\\\  = \\text{sup}\\{\\frac{|Ax|}{|x|}: x \\in \\mathcal{V}, x \\neq 0 \\} \\] <p>(lots of resources use this definition, for clarification of max vs sup, look here).</p> <p>An intermediate result we can now prove is the following (or you could also just look here):</p> \\[ |Ax| \\leq ||A|| |x|, \\, \\forall x \\in \\mathbb{R}^n \\] <p>The proof in the top answer of the above link from Stackoverflow is copied below for ease of reference: </p> <p>By definition \\(\u2016\ud835\udc34\u2016= \\text{sup} \\{|Ax|, x \u2208 \\mathbb{R}^n, |x| \u2264 1 \\}\\) and hence for any  \\(x \\in \\mathbb{R}^n\\) such that  \\(|x| \\leq 1\\) we must have by the definition of supremum that  \\(|Ax| \\leq ||A||\\)</p> <p>The \\(x = 0\\) case is trivial.</p> <p>For the \\(x \\neq 0\\) case: </p> <p>Let  \\(y = \\frac{x}{|x|}\\). It follows that \\(|y| = 1\\), and hence \\(|Ay| \\leq ||A||\\) from above.</p> <p>Now we have </p> \\[ |Ay| = | A \\frac{x}{|x|} | = \\frac{|Ax|}{|x|} \\] <p>Therefore, since \\(|x|&gt;0\\), we have \\(|Ax|=|Ay||x|\u2264 ||A|| |x|\\)</p>"},{"location":"blog/2024/08/05/lipschitz-constant-of-a-linear-transformation/#tying-it-all-together","title":"Tying it all together","text":"<p>Now, we can look at the Lipschitz constant of a linear transformation \\(f[z] = Az + b\\).</p> \\[ |f[z1] \u2212 f[z2]| = |Az_1 - Az_2| \\\\ = |A(z_1 - z_2)| \\\\ \\leq ||A|| |z_1 - z_2| \\] <p>The last step is using the same intermediate property we proved when showing the proof for submultiplicativity.</p> <p>This is the general case - the Lipschitz constant of a linear transform will be the matrix norm of the matrix \\(A\\).</p> <p>We need one more step to prove the claim in the book, and I think that involves deviating away from the general case.</p> <p>I am going to be following some of the material here. When the 2-norm is used, the induced matrix operator is the following (called the spectral norm):</p> \\[ ||A||_2 = \\text{max}_{x \\neq 0} \\{ \\frac{|Ax|_2}{|x|_2} \\} \\] <p>It is a known result for the 2-norm that  \\(|y|_2^2 = y^Ty\\)</p> <p>So let us take the square of the induced matrix operator.</p> \\[ ||A||_2^2 = \\text{max}_{x \\neq 0} \\{ \\frac{|Ax|_2^2}{|x|_2^2} \\} \\\\ = \\text{max}_{x \\neq 0} \\{ \\frac{x^TA^TAx}{x^Tx} \\} \\] <p>We will need to use concepts from singular value decomposition (SVD). This resource might help. The \"trick\" is to write A as</p> \\[ A = U\\Sigma V^T \\] <p>and then write \\(A^TA = (U\\Sigma V^T)^T (U\\Sigma V^T) = V \\Sigma^T \\Sigma V\\), where \\(\\Sigma\\) is a diagonal matrix with the eigenvalues of A in the diagonal.</p> <p>Let \\(\\lambda_1\\) be the largest eigenvalue of A. Then, the maximum value of the ratio of \\(\\frac{x^TA^TAx}{x^Tx}\\) can be shown to be \\(\\lambda_1^2\\).</p> <p>That was the final step - showing  \\(||A||_2^2 = \\lambda_1^2\\), i.e. \\(||A||_2 = \\lambda_1\\)</p> <p>So therefore, </p> \\[ |f[z1] \u2212 f[z2]|_2 = |Az_1 - Az_2|_2 \\\\ = |A(z_1 - z_2)|_2 \\\\ \\leq ||A||_2 |z_1 - z_2|_2 \\\\ = \\lambda_1 |z_1 - z_2| \\] <p>Other resources:  1. I found this from Drexel University to also be quite helpful</p>"},{"location":"blog/2024/06/13/masking-in-transformers/","title":"Masking in transformers","text":"<p>Masking is pretty fundamental to how transformers work. The main purpose of masking is to make sure the model doesn't \"attend to\" some tokens. Feel free to read  my previous blog post about Self attention.</p> <p>What are the kind of tokens we may not want a model to \"attend to\"? They're usually:</p> <ol> <li>Padding tokens - These tokens are added in some architectures (such as BERT) such that all input token sequences are of the same length, and you do not want them to affect your final output.  </li> <li>Future tokens - when training some decoder architectures, you want the \"weighting\" by the attention mechanism placed on the value vectors corresponding to tokens further along in the sequence in the training data to be zero. Because if it is non-zero your model is basically \"cheating\" during training by looking at what it is supposed to predict, and will not perform as well during inference time.</li> </ol> <p>With this in mind, let's see how masking tends to be implemented in practice. </p>"},{"location":"blog/2024/06/13/masking-in-transformers/#masking-the-basics","title":"Masking - the basics","text":"<p>The main way to make sure the tokens you want to mask are not being attended to is by manipulating the inputs to the softmax function. As a reminder, the softmax function is defined as the following - given an array of inputs \\([x_0, x_1, ... x_i, ..., x_N]\\) the softmax function returns an array of \\([p_0, p_1, ... p_i, ... p_N]\\) where </p> \\[ p_i = \\frac{e^{x_i}}{\\sum^{N}_{i=1}{e^{x_i}}} \\] <p>and \\(\\sum_ip_i = 1\\). </p> <p>This softmax function gets applied on the output of the \\(k^Tq\\) operation in the attention. The main \"trick\" when it comes to masking in a vectorized way is basically then overwriting this value with a large negative value. </p> <p>To see this in action, let us see how you'd make sure future tokens aren't being attended to in pure Python for loops. </p> <p>First, let's set up the toy example:</p> <pre><code>import numpy as np\n\ndef softmax(items_in: list):\n  e_x = np.exp(items_in - np.max(items_in))\n  return e_x / e_x.sum()\n\n\n# Set seed so we get the same random numbers\nnp.random.seed(3)\n# Number of inputs\nN = 4\n# Number of dimensions of each input\nD = 3\n\nall_x = []\n# Create elements x_n and append to list\nfor n in range(N):\n  all_x.append(np.random.normal(size=(D,1)))\n\nprint(\"all_x: \")\nprint(all_x)\nA_q = np.random.normal(size=(D,D))\nA_k = np.random.normal(size=(D,D))\nA_v = np.random.normal(size=(D,D))\n\n# all the biases are of dimension Dx1\nb_q = np.random.normal(size=(D,1))\nb_k = np.random.normal(size=(D,1))\nb_v = np.random.normal(size=(D,1))\n\nall_queries = []\nall_keys = []\nall_values = []\n# For every input\nfor x in all_x:\n  query = A_q @ x + b_q\n  key = A_k @ x + b_k \n  value = A_v @ x + b_v\n\n  all_queries.append(query)\n  all_keys.append(key)\n  all_values.append(value)\n</code></pre> <p>In this above example, the columns of <code>all_x</code> are the <code>D</code> dimensioned data points, and there are <code>N=3</code> of them. This can be thought of as the output of the embedding layer in a standard transformer block.</p> <p>Assume this is a causal mask we are trying to apply, i.e. for the <code>i_th</code> input, there should only be non-zero attention weights for values with an index <code>&lt;=i</code>. If that was confusing, let's illustrate this in code:</p> <pre><code>all_outs = []\n\nfor i in range(N):\n    print(\"==\"*10)\n    print(f\"i: {i}\")\n    all_kj_qi = []\n    q_i = all_queries[i]\n    for j in range(i + 1):  # &lt;- no future tokens will be attended to. If you want to attent to all tokens, this line would change to range(N) instead of range(i + 1) \n        key_j = all_keys[j]\n        dot_product = np.dot(key_j.T, q_i).squeeze()\n        all_kj_qi.append(dot_product)\n\n    print(\"before softmax:\")\n    print(all_kj_qi)\n    attention = softmax(all_kj_qi)\n    print(f\"attentions: {attention}\")\n    out_i = sum(attention[i] * all_values[i] for i in range(len(attention)))\n    all_outs.append(out_i)\n    print(\"==\"*10)\n</code></pre> <p>The output would be:  <pre><code>====================\ni: 0\nbefore softmax:\n[array(8.0985502)]\nattentions: [1.]\n====================\n====================\ni: 1\nbefore softmax:\n[array(1.33534917), array(1.04085729)]\nattentions: [0.57309546 0.42690454]\n====================\n====================\ni: 2\nbefore softmax:\n[array(2.95774823), array(0.4307023), array(2.98146278)]\nattentions: [0.47530942 0.0379747  0.48671588]\n====================\n====================\ni: 3\nbefore softmax:\n[array(7.39220366), array(0.99822195), array(6.05577164), array(5.18771535)]\nattentions: [0.72739962 0.00121591 0.19114723 0.08023724]\n====================\n</code></pre></p> <p>This basically means the following - </p> \\[ out_0 = 1.0 \\times v_0 \\] \\[ out_1 = 0.573 \\times v_0 + 0.427 \\times v_1 \\] \\[ out_2 = 0.475 \\times v_0 + 0.038 \\times v_1 + 0.487 \\times v_2 \\] <p>and so forth. As you can see, \\(out_i\\) only depends on \\(v_j\\) if \\(j&lt;=i\\). </p> <p>So far, so good, but these pure Python <code>for</code> loops are slow, and you want to do this in a vectorized way. How would you do that?</p> <p>As mentioned previously, you do this by \"forcing\" the pre-softmax co-efficients to be large negative values at the positions you want to mask, so that the post-softmax coefficients at these positions are 0. This is best illustrated in this snippet of code: </p> <pre><code>all_outs = []\n\nfor i in range(N):\n    print(\"==\"*10)\n    print(f\"i: {i}\")\n    all_kj_qi = [] # &lt;-- will be a 1 x N vector\n    q_i = all_queries[i]\n    for j in range(N):\n        key_j = all_keys[j]\n        dot_product = np.dot(key_j.T, q_i).squeeze()\n        all_kj_qi.append(dot_product)\n\n    print(\"before adding:\")\n    print(all_kj_qi)\n    to_add = np.array([0.] * (i + 1) + [-np.inf] * (N - i - 1))\n    all_kj_qi += to_add\n    print(\"after adding:\")\n    print(all_kj_qi)\n    attention = softmax(all_kj_qi) # &lt;-- 1 x N vector that sums to 1\n    print(f\"attentions: {attention}\")\n    out_i = sum(attention[i] * all_values[i] for i in range(N))\n    all_outs.append(out_i)\n    print(\"==\"*10)\n</code></pre> <p>resulting in the output: </p> <pre><code>====================\ni: 0\nbefore adding:\n[array(8.0985502), array(-1.41964676), array(0.40887655), array(-4.51894638)]\nafter adding:\n[8.0985502      -inf      -inf      -inf]\nattentions: [1. 0. 0. 0.]\n====================\n====================\ni: 1\nbefore adding:\n[array(1.33534917), array(1.04085729), array(3.15550058), array(4.36428589)]\nafter adding:\n[1.33534917 1.04085729       -inf       -inf]\nattentions: [0.57309546 0.42690454 0.         0.        ]\n====================\n====================\ni: 2\nbefore adding:\n[array(2.95774823), array(0.4307023), array(2.98146278), array(2.65666126)]\nafter adding:\n[2.95774823 0.4307023  2.98146278       -inf]\nattentions: [0.47530942 0.0379747  0.48671588 0.        ]\n====================\n====================\ni: 3\nbefore adding:\n[array(7.39220366), array(0.99822195), array(6.05577164), array(5.18771535)]\nafter adding:\n[7.39220366 0.99822195 6.05577164 5.18771535]\nattentions: [0.72739962 0.00121591 0.19114723 0.08023724]\n====================\n</code></pre> <p>You can see that the attention values are the same as the previous attention values! This second snippet is much simpler to write in a vectorized manner. First let's set everything up as matrices instead of vectors:</p> <pre><code>X = np.array(all_x).squeeze()\nQ = X @ A_q.T + b_q.T  # &lt;- N x D matrix\nK = X @ A_k.T + b_k.T  # &lt;- N x D matrix\nV = X @ A_v.T + b_v.T  # &lt;- N x D matrix\n\n# show that our set up above using pure python for loops, and this matrix set up are equivalent\nassert (Q == np.array(all_queries).squeeze()).all()\n\ndef softmax_cols(data_in):\n    # Exponentiate all of the values\n    # keepdims=True IS VERY IMPORTANT\n    _data_in = data_in - np.max(data_in, axis=1, keepdims=True)\n    exp_values = np.exp(_data_in)\n    # Sum over columns\n    denom = np.sum(exp_values, axis=1, keepdims=True)\n    # Compute softmax\n    softmax = exp_values / denom\n    # return the answer\n    return softmax\n</code></pre> <p>Now, to look at what the pre-softmax values are, you can inspect what <code>Q @ K.T</code> is: </p> <pre><code>&gt;&gt; Q@K.T\narray([[ 8.0985502 , -1.41964676,  0.40887655, -4.51894638],\n       [ 1.33534917,  1.04085729,  3.15550058,  4.36428589],\n       [ 2.95774823,  0.4307023 ,  2.98146278,  2.65666126],\n       [ 7.39220366,  0.99822195,  6.05577164,  5.18771535]])\n</code></pre> <p>note that each row is basically the array that was printed out as part of the <code>before adding:</code> part in the previous output snippet! For illustration:</p> <p></p> <p>And now you need the attention mask: </p> <pre><code>&gt;&gt; to_add = np.triu(-np.inf * np.ones((N, N)), k=1)\n&gt;&gt; to_add\n\narray([[  0., -inf, -inf, -inf],\n       [  0.,   0., -inf, -inf],\n       [  0.,   0.,   0., -inf],\n       [  0.,   0.,   0.,   0.]])\n</code></pre> <p>Then you'd add this mask to the \\(QK^T\\) value.  <pre><code>&gt;&gt;pre_softmax = Q@K.T + to_add\n&gt;&gt;pre_softmax\narray([[8.0985502 ,       -inf,       -inf,       -inf],\n       [1.33534917, 1.04085729,       -inf,       -inf],\n       [2.95774823, 0.4307023 , 2.98146278,       -inf],\n       [7.39220366, 0.99822195, 6.05577164, 5.18771535]])\n</code></pre></p> <p>Once done, just put it all through the softmax function! <pre><code>&gt;&gt;softmax_cols(pre_softmax)\narray([[1.        , 0.        , 0.        , 0.        ],\n       [0.57309546, 0.42690454, 0.        , 0.        ],\n       [0.47530942, 0.0379747 , 0.48671588, 0.        ],\n       [0.72739962, 0.00121591, 0.19114723, 0.08023724]])\n</code></pre></p> <p>This is your attention matrix! It is the same output we get as when we used the pure Python loops.</p> <p>As a recap, in a vectorized way, all you need is just the following lines of code:</p> <pre><code>Q = X @ A_q.T + b_q.T  # &lt;- N x D matrix\nK = X @ A_k.T + b_k.T  # &lt;- N x D matrix\nV = X @ A_v.T + b_v.T  # &lt;- N x D matrix\n\nto_add = np.triu(-np.inf * np.ones((N, N)), k=1)\npre_softmax = Q@K.T + to_add\nattention = softmax_cols(pre_softmax)\nout = attention @ V\n\nassert (out == np.array(all_outs).squeeze()).all()  # &lt;- shows equivalence\n</code></pre>"},{"location":"blog/2024/06/13/masking-in-transformers/#masking-in-the-hf-library","title":"Masking in the HF library","text":"<p>Now that you know the basics of how masking is supposed to work, let's take a look at how the Huggingface library implements this, specifically in the BERT modules. </p> <p>Let's set up this toy example: </p> <pre><code>from transformers import BertTokenizerFast\ntokenizer = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-uncased\")\n\nwords = [\n    [\"word1*word2|word3\"],\n    [\"word1*\", \"word2|\", \"word3\"],\n]\ntokenizer_out = tokenizer(\n    text=words,\n    is_split_into_words=True,\n    return_offsets_mapping=True,\n    return_overflowing_tokens=True,\n    truncation=\"longest_first\",\n    padding=\"max_length\",\n    return_tensors=\"pt\",\n    max_length=512,\n    stride=0,\n    return_length=True,\n)\n# Input IDs and attention mask\ninput_ids = tokenizer_out[\"input_ids\"]\nattention_mask = tokenizer_out[\"attention_mask\"]\n</code></pre> <p>If you look at what input_ids and the attention_mask are:  <pre><code>input_ids\ntensor([[ 101, 2773, 2487, 1008, 2773, 2475, 1064, 2773, 2509,  102,    0, 0, ...],\n        [ 101, 2773, 2487, 1008, 2773, 2475, 1064, 2773, 2509,  102,    0, 0, ...]])\n\nattention_mask\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...]])\n</code></pre></p> <p>The <code>1</code>s in the <code>attention_mask</code> are used to indicate tokens that should be attended to, while the <code>0</code>s are used to indicate tokens that you don't want the model to attend to.</p> <p>Where does this attention mask get computed in the <code>BertTokenizerFast</code>? You have to chase down the <code>return_offsets_mapping</code>, and you will go all the way to the <code>_batch_encode_plus</code> method of the <code>PreTrainedTokenizerFast</code> class, which calls <code>self._tokenzier.encode_batch</code>. As part of the <code>fast</code> implementation of tokenizers, this method is implemented in Rust for performance reasons!</p> <p>You can still follow the logic of where the attention mask is computed in the non-fast implementation called <code>BertTokenizer</code>. To do so, you'll have to chase down the functions in the <code>forward</code> method all the way till you reach the <code>_pad</code> method of the <code>PreTrainedTokenizerBase</code> class, where you can see how Huggingface deals with the various padding strategies available. In the snippet above, we chose <code>max_length</code>, the relevant parts of the code are: </p> <pre><code># Initialize attention mask if not present.\n    if return_attention_mask and \"attention_mask\" not in encoded_inputs:\n        encoded_inputs[\"attention_mask\"] = [1] * len(required_input)\n    difference = max_length - len(required_input)\n    encoded_inputs[\"attention_mask\"] = encoded_inputs[\"attention_mask\"] + [0] * difference\n</code></pre> <p>We next turn our attention to the <code>forward</code> method of the <code>BertModel</code> to see what is going on. You can go through the forward pass with -</p> <pre><code>model = BertModel.from_pretrained(model_name)\n\n# Inference with the mask\nmodel.eval()\noutputs = model(input_ids, attention_mask=attention_mask)\n</code></pre> <p>Once it enters the <code>forward</code> method, you'll see that there's a method called <code>get_extended_attention_mask</code> that is part of the <code>BertModel</code> (actually its inheritance is a bit more complicated - it is defined in the <code>ModuleUtilsMixin</code> and the chain of inheritance is something like <code>ModuleUtilsMixin</code> -&gt; <code>PreTrainedModel</code> -&gt; <code>BertPreTrainedModel</code> -&gt; <code>BertModel</code>, where the <code>-&gt;</code> is shorthand for \"is inherited by\").</p> <p>The conversion of this array of 0s and 1s to the form of <code>to_add</code> is done here in this line:</p> <pre><code>extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(dtype).min\n</code></pre> <p>where the 1s in the <code>attention_mask</code> get converted to 0s and the <code>0</code>s get converted into a large negative value (like the <code>-np.inf</code> in the code snippets I provided). </p> <p>This extended mask then gets passed trough <code>BertEncoder</code>, to <code>BertAttention</code> to <code>BertSelfAttention</code> where it is finally used in the <code>forward</code> method: </p> <pre><code>class BertSelfAttention(nn.Module):\n    ...\n    def forward(\n        self,\n        ...,\n        attention_mask,\n        ...,\n    ):\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        ...\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n        ...\n        return outputs\n</code></pre> <p>The <code>...</code> is there to get rid of all the \"noise\" that clutters our understanding of how masking is being used in the underlying torch modules in the HF library on Bert. </p> <p>As you can see, the structure is very similar to the code snippets that I wrote from scratch above!</p> <p>Hopefully, you now have more clarity about how this often overlooked (but important) part of any transformer block functions. </p>"},{"location":"blog/2025/02/06/notes-on-information-theory-for-meta-learning/","title":"Notes on information theory for meta-learning","text":"<p>When reading the \"Meta-Learning without Memorization\" paper, there were several derivations that caught me off-guard. So, I'm going to put some basic derivations to paper to refer to in the future that will make reading papers easier. </p> <p>The main aim of this post is to figure out why for this plate diagram from the paper: </p> <p></p> <p>results in equation 2: </p> \\[ \\newcommand{\\y}{\\hat{y}^*} \\newcommand{\\z}{z^*} \\newcommand{\\x}{x^*} \\newcommand{\\D}{\\mathcal{D}} \\newcommand{\\T}{\\mathcal{T}} \\newcommand{\\xx}{\\mathbb{x}} \\newcommand{\\yy}{\\mathbb{y}} \\begin{aligned} &amp;I(\\hat{y}^*;\\mathcal{D}|z^*,\\theta) \\\\ &amp;\\geq I(x^*;\\hat{y}^*|\\theta,z^*) \\\\ &amp;= I(x^*;\\hat{y}^*|\\theta) - I(x^*;z^*|\\theta) + I(x^*;z^*|\\hat{y}^*,\\theta) \\\\ &amp;\\geq I(x^*;\\hat{y}^*|\\theta) - I(x^*;z^*|\\theta) \\\\ &amp;= I(x^*;\\hat{y}^*|\\theta) - \\mathbb{E}_{p(x^*)q(z^*|x^*,\\theta)}\\left[\\log\\frac{q(z^*|x^*,\\theta)}{q(z^*|\\theta)}\\right] \\\\ &amp;\\geq I(x^*;\\hat{y}^*|\\theta) - \\mathbb{E}\\left[\\log\\frac{q(z^*|x^*,\\theta)}{r(z^*)}\\right] = I(x^*;\\hat{y}^*|\\theta) - \\mathbb{E}[D_{KL}(q(z^*|x^*,\\theta)||r(z^*))] \\end{aligned} \\] <p>As a recap, the meta-learning setup is the following according to the paper: We assume tasks \\(\\T_i\\) are sampled from a distribution of tasks \\(p(\\T)\\) - for example the Omniglot dataset. For each task during meta-training, the model sees a set of training data \\(\\D_i = (\\mathbb{x}_i, \\mathbb{y}_i)\\) and a set of test data \\(D^*_i = (\\mathbb{x}^*_i, \\mathbb{y}^*_i)\\) where \\(\\xx_i, \\yy_i, \\xx^*_i, \\yy^*_i\\) are sampled from \\(p(x, y|\\T_i)\\). The model's prediction at test time for the task is denoted as \\(\\y\\). </p> <p>Obviously, don't show the rest of the article below to mathematicians, they'll probably nitpick and be generally miserable like they all are about not being pedantic around some notation or another.</p>"},{"location":"blog/2025/02/06/notes-on-information-theory-for-meta-learning/#background","title":"Background","text":"<p>Let's cover a few background maths information we need.</p>"},{"location":"blog/2025/02/06/notes-on-information-theory-for-meta-learning/#jensens-inequality","title":"Jensen's inequality","text":"<p>The first is Jensen's inequality. But even before we get to that, let's show some basic things. </p>"},{"location":"blog/2025/02/06/notes-on-information-theory-for-meta-learning/#definition-of-convex-functions","title":"Definition of convex functions","text":"<p>Basically, a convex function are functions like the one below: </p> <p></p> <p>In these functions, a line passing through two points of the function will be above the graph of the function itself. </p> <p>To be more rigorous about this, let's consider a function \\(f(x)\\), and 2 points \\(x_1, x_2\\). Let's now consider a third point \\(x'\\) somewhere between \\(x_1, x2\\). We can arbitrarily represent \\(x' := x_1 + \\alpha(x_2 - x_1)\\) for some \\(a \\in [0, 1]\\) - basically, if \\(\\alpha = 0 \\implies x' = x_1, \\alpha = 1 \\implies x' = x_2\\). </p> <p>In the diagram above \\(y_{\\text{curve}} = f(x') = f(x_1 + \\alpha(x_2 - x_1)) = f((1 -\\alpha)x_1 + \\alpha x_2)\\).</p> <p>And finally, </p> \\[ \\frac{y_{\\text{line}} - f(x_1)}{x' - x_1} =  \\frac{f(x_2) - f(x_1)}{x_2 - x_1} \\] \\[ \\implies y_{\\text{line}} = \\frac{f(x_2) - f(x_1)}{x_2 - x_1}(x' - x_1) + f(x_1) \\] \\[ = \\frac{f(x_2) - f(x_1)}{x_2 - x_1}(x_1 + \\alpha(x_2 - x_1) - x_1) + f(x_1) \\] \\[ = \\alpha(f(x_2) - f(x_1)) + f(x_1) = (1 - \\alpha)f(x_1) + f(x_2) \\] <p>The definition of a convex function is basically \\(y_{\\text{line}} &gt;= y_{\\text{curve}}, \\forall \\alpha \\in [0, 1]\\)</p> \\[ \\implies (1 - \\alpha)f(x_1) + f(x_2) \\geq f((1 -\\alpha)x_1 + \\alpha x_2) \\] <p>Examples of concave functions are log functions, while examples of convex functions are exponential functions. </p>"},{"location":"blog/2025/02/06/notes-on-information-theory-for-meta-learning/#extension-to-the-general-x_n-case","title":"Extension to the general \\(x_n\\) case","text":"<p>Following the definition of a convex function, we now make the following statement:  Given a convex function \\(f\\), and \\(a_1, a_2, \\dots a_n\\) for some \\(n\\) such that \\(\\sum_i a_i = 1\\),</p> \\[ f(a_1x_1 + a_2x_2 + \\dots a_nx_n) \\leq a_1f(x_1) + a_2f(x_2) + \\dots + a_nf(x_n) \\] <p>To show the above result, you can use induction, by using the definition of a convex function as the base case. Assuming the \\(k^{\\text{th}}\\) case is true, let's consider the \\((k + 1)^{\\text{th}}\\) case. </p> <p>By definition we have \\(a_1 + a_2 + \\dots + a_k + a_{k + 1} = 1\\). Defining \\(b := a_1 + a_2 + ... + a_k\\), we have \\(b + a_{k+1} = 1\\).</p> <p>We now look at the right hand side:</p> \\[ a_1f(x_1) + a_2f(x_2) + \\dots + a_kfx_k) + a_{k+1}f(x_{k+1})) = a_{k+1}f(x_{k+1})) + b\\sum_i^{k}\\frac{a_i}{b}f(x_i) \\] <p>The last term \\(\\sum_i^{k}\\frac{a_i}{b}f(x_i)\\), if we cast \\(\\frac{a_i}{b} = a'_i\\), you get \\(\\sum_i^k \\frac{a_i}{b} = 1\\). So basically, the extreme right term can be bounded by the \\(n=k\\) case which we are assuming to be true as part of our induction proof. </p> <p>So now, you have </p> \\[ \\begin{aligned} &amp;a_1f(x_1) + a_2f(x_2) + \\dots + a_kfx_k + a_{k+1}f(x_{k+1}) \\\\ &amp;= a_{k+1}f(x_{k+1}) + b\\sum_i^{k}\\frac{a_i}{b}f(x_i) \\\\ &amp;\\geq a_{k+1}f(x_{k+1}) + bf(\\sum_i^ka'_ix_i) \\leftarrow \\text{And this is basically the 2 point case} \\\\ &amp;\\geq f(a_{k+1}x_{k+1} + b\\sum_i^{k}a'_ix_i) \\\\ &amp;= f(\\sum_i^{k+1}a_ix_i) \\\\ \\end{aligned} \\] <p>Thus proving the \\(k+1\\) case, and therefore the statement. Look at the Elements of Information Theory if you don't trust what I've written here.</p>"},{"location":"blog/2025/02/06/notes-on-information-theory-for-meta-learning/#putting-it-all-together","title":"Putting it all together","text":"<p>You notice that the definition of probability basically matches that of \\(a_i\\) in the previous part. Substituting \\(a\\) for \\(p\\), you get </p> \\[ f(p_1x_1 + p_2x_2 + \\dots p_nx_n) = f(\\mathbb{E[x]}) \\leq p_1f(x_1) + p_2f(x_2) + \\dots + p_nf(x_n) \\] \\[ p_1f(x_1) + p_2f(x_2) + \\dots + p_nf(x_n) = \\mathbb{E}[f(x)] \\] <p>and that's where you get Jensen's inequality from </p> \\[ f(p_1x_1 + p_2x_2 + \\dots p_nx_n) \\leq p_1f(x_1) + p_2f(x_2) + \\dots + p_nf(x_n) \\implies f(\\mathbb{E[x]}) \\leq \\mathbb{E}[f(x)] \\] <p>This will hold for the continuous case as well.</p> <p>For concave functions \\(f\\), it is the opposite:</p> <p>$$ f(\\mathbb{E[x]}) \\geq \\mathbb{E}[f(x)] $$ In particular, let's remember that logarithmic functions are concave, so </p> \\[ \\text{log}(\\mathbb{E}[x]) \\geq \\mathbb{E}[\\text{log(x)}] \\]"},{"location":"blog/2025/02/06/notes-on-information-theory-for-meta-learning/#mutual-information-entropy-et-al","title":"Mutual information, entropy et al","text":"<p>To understand the equations, we need some more background on mutual information, entropy etc. </p> <p>Again, refer back to the Elements of Information Theory book for a more thorough treatment. </p>"},{"location":"blog/2025/02/06/notes-on-information-theory-for-meta-learning/#definitions-and-some-results","title":"Definitions and some results","text":"<p>The entropy H(X) of a random variable X is defined by </p> <p>$$ H(X) = -\\sum_{x\\in \\mathcal{X}}p(x)\\log p(x) $$ which is also sometimes written as \\(H(p)\\) because we're all friends here. </p> <p>We can also extend this definition to a pair of random variables. There's nothing really \"new\" because you can consider \\((X, Y)\\) to be a single vector-valued random function, but it's still useful to write it all out:</p> <p>Definition: The joint entropy \\(H(X, Y)\\) of a pair of discrete random. variables (X, Y) with a joint distribution \\(p(x,y)\\) is</p> \\[  H(X, Y) = - \\sum_{x \\in \\mathcal{X}}\\sum_{y \\in \\mathcal{Y}}p(x, y) \\log p(x, y) \\] <p>Another thing to remember is the definition of conditional entropy</p> <p>Definition: If $(X, Y) \\sim p(x, y), then the conditional entropy \\(H(Y|X)\\) is defined as</p> \\[  H(Y | X) = \\sum_{x \\in \\mathcal{X}} p(x)H(Y|X=x) \\] \\[  = - \\sum_{x \\in \\mathcal{X}}p(x) \\sum_{y\\in \\mathcal{Y}}p(y|x)\\log p(y|x)  \\] \\[ = - \\sum_{x \\in \\mathcal{X}}\\sum_{y\\in \\mathcal{Y}}p(x, y)\\log p(y|x)  \\] \\[ = \\mathbb{E} \\log p(Y|X) \\] <p>One last thing to also remember is the chain rule of entropy, which is as follows: </p> \\[ H(X, Y) = H(X) + H(Y|X) = H(Y) + H(X|Y) \\] <p>Proof:</p> \\[ \\begin{aligned} H(X,Y) &amp;= -\\sum\\sum p(x,y)\\log p(x,y) \\\\ &amp;= -\\sum\\sum p(x,y)\\log p(x)p(y|x) \\\\ &amp;= -\\sum\\sum p(x,y)\\log p(x) - \\sum\\sum p(x,y)\\log p(y|x) \\\\ &amp;= -\\sum p(x)\\log p(x) - \\sum\\sum p(x,y)\\log p(y|x) \\\\ &amp;= H(X) + H(Y|X) \\end{aligned} \\] <p>The corollary of this is </p> <p>$$ H(X, Y | Z) = H(X | Z) + H(Y | X, Z) $$ Ok, so that is a big chunk. </p> <p>Let's move along a bit quicker now. The Kullback-Leibler distance between 2 distributions \\(p(x), q(x)\\) is given by: </p> <p>$$ \\begin{aligned} D(p||q) &amp;= \\sum_{x\\in\\mathcal{X}} p(x)\\log\\frac{p(x)}{q(x)} \\ &amp;= E_p\\log\\frac{p(X)}{q(X)} \\end{aligned} $$ With this definition, we can move on to what mutual information is. </p> <p>The mutual information between 2 random variables \\(X, Y\\) with a join probability mass function \\(p(x, y)\\) and marginals \\(p(x), p(y)\\) is the KL distance between the joint distribution and the product distribution \\(p(x)p(y)\\)</p> <p>In equations, this means: </p> \\[ \\begin{aligned} I(X;Y) = D(p(x, y) || p(x)p(y)) &amp;= \\sum_{x\\in\\mathcal{X}}\\sum_{y\\in\\mathcal{Y}} p(x,y)\\log\\frac{p(x,y)}{p(x)p(y)} \\\\ &amp;= D(p(x,y)||p(x)p(y)) \\\\ &amp;= E_{p(x,y)}\\log\\frac{p(X,Y)}{p(X)p(Y)} \\end{aligned} \\] <p>And if you keep turning the crank, this basically becomes </p> <p>$$ I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) = H(X) + H(Y) - H(X, Y) $$ There is another definition we need to talk about - the conditional mutual information of random variables \\(X, Y\\) given a third variable \\(Z\\), and this is defined by: </p> \\[ \\begin{aligned} I(X;Y|Z) &amp;= H(X|Z) - H(X|Y,Z) \\\\ &amp;= E_{p(x,y,z)}\\log\\frac{p(X,Y|Z)}{p(X|Z)p(Y|Z)} \\end{aligned} \\]"},{"location":"blog/2025/02/06/notes-on-information-theory-for-meta-learning/#chain-rules","title":"Chain rules","text":"<p>We're nearly there! Let's go through some chain rules now. </p> <p>Chain rules for entropy is basically the following: Let \\(X_1, X_2, \\dots X_n\\) be drawn according to \\(p(x_1, x_2, \\dots x_n)\\). Then you'll have</p> <p>$$ H(X_1, X_2, \\ldots, X_n) = \\sum_{i=1}^n H(X_i|X_{i-1}, \\ldots, X_1). $$ The proof of this is if you write </p> \\[ \\begin{aligned} p(x_1,\\ldots,x_n) &amp;= \\prod_{i=1}^n p(x_i|x_{i-1},\\ldots,x_1) \\\\ \\implies &amp; H(X_1, X_2,\\ldots, X_n) \\\\ &amp;= -\\sum_{x_1,x_2,\\ldots,x_n} p(x_1,x_2,\\ldots,x_n)\\log p(x_1,x_2,\\ldots,x_n) \\\\ &amp;= -\\sum_{x_1,x_2,\\ldots,x_n} p(x_1,x_2,\\ldots,x_n)\\log\\prod_{i=1}^n p(x_i|x_{i-1},\\ldots,x_1) \\\\ &amp;= -\\sum_{x_1,x_2,\\ldots,x_n}\\sum_{i=1}^n p(x_1,x_2,\\ldots,x_n)\\log p(x_i|x_{i-1},\\ldots,x_1) \\\\ &amp;= -\\sum_{i=1}^n\\sum_{x_1,x_2,\\ldots,x_n} p(x_1,x_2,\\ldots,x_n)\\log p(x_i|x_{i-1},\\ldots,x_1) \\\\ &amp;= -\\sum_{i=1}^n\\sum_{x_1,x_2,\\ldots,x_i} p(x_1,x_2,\\ldots,x_i)\\log p(x_i|x_{i-1},\\ldots,x_1) \\\\ &amp;= \\sum_{i=1}^n H(X_i|X_{i-1},\\ldots,X_1). \\quad \\square \\end{aligned} \\] <p>Similarly, for mutual information you have</p> <p>$$ I(X_1,X_2,\\ldots,X_n;Y) = \\sum_{i=1}^n I(X_i;Y|X_{i-1},X_{i-2},\\ldots,X_1). $$ You can prove this using the same cranks, or just refer to the book\u2122. </p>"},{"location":"blog/2025/02/06/notes-on-information-theory-for-meta-learning/#back-to-the-equation","title":"Back to the equation","text":"<p>Ok, that was a lot of equations, but there's going to be more!</p> <p>Let us remember what we're trying to do again. We want to show this: </p> \\[ \\begin{aligned} &amp;I(\\hat{y}^*;\\mathcal{D}|z^*,\\theta) \\\\ &amp;\\geq I(x^*;\\hat{y}^*|\\theta,z^*) \\\\ &amp;= I(x^*;\\hat{y}^*|\\theta) - I(x^*;z^*|\\theta) + I(x^*;z^*|\\hat{y}^*,\\theta) \\\\ &amp;\\geq I(x^*;\\hat{y}^*|\\theta) - I(x^*;z^*|\\theta) \\\\ &amp;= I(x^*;\\hat{y}^*|\\theta) - \\mathbb{E}_{p(x^*)q(z^*|x^*,\\theta)}\\left[\\log\\frac{q(z^*|x^*,\\theta)}{q(z^*|\\theta)}\\right] \\\\ &amp;\\geq I(x^*;\\hat{y}^*|\\theta) - \\mathbb{E}\\left[\\log\\frac{q(z^*|x^*,\\theta)}{r(z^*)}\\right] = I(x^*;\\hat{y}^*|\\theta) - \\mathbb{E}[D_{KL}(q(z^*|x^*,\\theta)||r(z^*))] \\end{aligned} \\] <p>for the following plate diagram:</p> <p></p> <p>The first non-obvious part is showing the first jump itself: </p> \\[ I(\\hat{y}^*;\\mathcal{D}|z^*,\\theta) \\geq I(x^*;\\hat{y}^*|\\theta,z^*) \\] <p>How does this come about?</p> <p>We first have to remember the following: </p> \\[ \\begin{aligned} I(\\y;\\D|\\z,\\theta) &amp;= H(\\y |\\theta, \\z) - H(y|\\D, \\theta, \\z) \\\\ I(\\y;\\x|\\D,\\theta, \\z) &amp;= H(\\y | \\D, \\theta, \\z) - H(y| \\x, \\D, \\theta, \\z) \\\\ I(\\y;\\x, \\D |\\theta, \\z) &amp;= H(\\y |\\theta, \\z) - H(y| \\x, \\D, \\theta, \\z) \\\\ \\implies I(\\y;\\x, \\D |\\theta, \\z) &amp;= I(\\y;\\x|\\D,\\theta, \\z) + I(\\y;\\D|\\z,\\theta) \\end{aligned} \\] <p>But if you look at the plate diagram, you have </p> \\[ I(\\y;\\x|\\D,\\theta, \\z) = 0 \\] <p>this means that we can now write</p> \\[ \\begin{aligned} I(\\hat{y}^*;\\mathcal{D}|z^*,\\theta) &amp;= I(\\hat{y}^*;\\mathcal{D}|z^*,\\theta) + I(\\hat{y}^*;x^*|\\mathcal{D},\\theta,z^*) \\\\ &amp;= I(\\hat{y}^*;x^*,\\mathcal{D}|\\theta,z^*) \\leftarrow \\text{Now what?}\\\\ \\end{aligned} \\] <p>To further carry on, we need to prove some intermediate results. The first is that \\(I(X; Y|Z) = I(Y; X|Z)\\). This is because: </p> \\[ \\begin{align} I(X; Y|Z) &amp;= H(X|Z) - H(X|Y,Z) \\\\ &amp;= H(X,Z) - H(Z) - (H(X, Y, Z) - H(Y, Z)) \\\\ &amp;= H(Y,Z) - H(Z) - ((H(X, Y, Z) - H(X, Z))) \\\\ &amp;= H(Y|Z) - H(Y|X, Z) \\\\ &amp;= I(Y; X|Z) \\end{align} \\] <p>The next one to show is the following:</p> \\[ I(X; Y,Z|W) = I(X; Y|W) + I(X; Z|Y, W) \\] <p>Why does this work? Expanding out the right hand side:</p> \\[ \\begin{align} I(X; Y,Z |W) &amp;= H(X|W) - H(X|Y,Z,W) \\\\ &amp;= H(X|W) - H(X|Y, W) + H(X|Y, W) - H(X|Y,Z,W) \\\\ &amp;= I(X; Y|W) + I(X; Z|Y, W) \\end{align} \\] <p>which is the left hand side. </p> <p>Let's go back to where we left off - </p> \\[ \\begin{aligned} &amp;I(\\hat{y}^*;x^*,\\mathcal{D}|\\theta,z^*) \\leftarrow \\text{Use second result to expand}\\\\ &amp;= I(\\y;\\x|\\theta, \\z) + I(\\y; \\D|\\x, \\theta, \\z) \\leftarrow \\text{Use first result to swap around variables in first term}\\\\ &amp;= I(x^*;\\hat{y}^*|\\theta,z^*) + I(\\hat{y}^*;\\mathcal{D}|x^*,\\theta,z^*) \\end{aligned} \\] <p>Giving a final:</p> \\[ \\begin{aligned} I(\\hat{y}^*;\\mathcal{D}|z^*,\\theta) &amp;= I(\\hat{y}^*;\\mathcal{D}|z^*,\\theta) + I(\\hat{y}^*;x^*|\\mathcal{D},\\theta,z^*) \\\\ &amp;= I(\\hat{y}^*;x^*,\\mathcal{D}|\\theta,z^*) \\\\ &amp;= I(x^*;\\hat{y}^*|\\theta,z^*) + I(\\hat{y}^*;\\mathcal{D}|x^*,\\theta,z^*) \\\\ &amp;\\geq I(x^*;\\hat{y}^*|\\theta,z^*) \\end{aligned} \\] <p>and thus, now we can understand Equation 2 from the paper in its entirety: </p> <p>$$ \\begin{aligned} &amp;I(\\hat{y}<sup>*;\\mathcal{D}|z</sup>,\\theta) \\ &amp;\\geq I(x<sup>*;\\hat{y}</sup>|\\theta,z^) \\ &amp;= I(x<sup>*;\\hat{y}</sup>|\\theta) - I(x<sup>*;z</sup>|\\theta) + I(x<sup>*;z</sup>|\\hat{y}^,\\theta) \\ &amp;\\geq I(x<sup>*;\\hat{y}</sup>|\\theta) - I(x<sup>*;z</sup>|\\theta) \\ &amp;= I(x<sup>*;\\hat{y}</sup>|\\theta) - \\mathbb{E}_{p(x<sup>*)q(z</sup>|x<sup>*,\\theta)}\\left[\\log\\frac{q(z</sup>|x<sup>*,\\theta)}{q(z</sup>|\\theta)}\\right] \\ &amp;\\geq I(x<sup>*;\\hat{y}</sup>|\\theta) - \\mathbb{E}\\left[\\log\\frac{q(z<sup>*|x</sup>,\\theta)}{r(z^)}\\right] = I(x<sup>*;\\hat{y}</sup>|\\theta) - \\mathbb{E}[D_{KL}(q(z<sup>*|x</sup>,\\theta)||r(z^*))] \\end{aligned} $$ where \\(r(\\z)\\) is a variational approximation of the marginal \\(q(\\z|\\theta)\\) </p> <p>What does this all mean?</p> <p>The whole aim of meta-regularisers It means that if you want to force the model to learn from the training dataset for each \\(\\T_i\\), one \"tractable\" way you can do it is by increasing the mutual information between \\(\\x\\) and \\(\\y\\) while at the same time restricting the information flow from \\(\\x\\) to \\(\\y\\) through a stochastic variable \\(z\\).</p> <p>Equation 2 from the paper is just one of many equations presented by the authors of the paper, but now, equipped with the various relationships between entropy, mutual information etc., you too can understand the authors' motivation of setting up the optimization functions and training losses they way they have set it up!</p>"},{"location":"blog/2024/07/02/rope-embeddings/","title":"RoPE embeddings","text":"<p>Rotary positional embeddings (RoPE) is yet another tool people use to improve the performance of BERT based models.</p> <p>For a description of what embeddings are, and how they get used in attention/transformers, feel free to take a look at a previous post - Self attention#Embedding - for more information. The main point of an embedding module is to serve as a lookup table, mapping various kinds of integer ids - token ids, position ids etc. - into vectors that will then get used in the attention mechanism.</p> <p>While token embeddings are one part of the puzzle, it is also important to inject the notion of a token's position in a sequence in the inputs to the attention mechanism. This injection is the subject of various methods in positional embeddings/encodings. </p> <p>Here we'll go through the following - </p> <ol> <li> <p>Quickly recap the following:</p> <ol> <li>The encoding scheme of proposed in the original transformers paper</li> <li>The embedding scheme in the original BERT model<ol> <li>How it is implemented in HuggingFace</li> </ol> </li> </ol> </li> <li> <p>Rotary positional embeddings (RoPE)</p> <ol> <li>Brief description behind wanting to use relative positional information</li> <li>How it is implemented in code</li> </ol> </li> </ol> <p>NOTE: This post, like all other posts is best viewed in dark mode.</p>"},{"location":"blog/2024/07/02/rope-embeddings/#sinusoidal-embeddings-from-transformers-paper","title":"Sinusoidal embeddings from transformers paper","text":"<p>As a recap the positional encoding in the original transformers paper was:</p> \\[ \\text{PE}_{(pos, 2i)} = \\text{sin}(\\text{pos} \\times \\theta_{i}) \\] \\[ \\text{PE}_{(pos, 2i + 1)} = \\text{cos}(\\text{pos} \\times \\theta_{i}) \\] <p>where </p> \\[ \\theta_{i} = 10,000^{\\frac{-2i}{d}} \\] <p>and \\(d\\) is the dimension of the vector. </p> <p>To visualise these quantities, let's plot some of them and how they vary. Let's start with \\(\\theta_i\\) and how it varies with \\(i \\in [0, d]\\), with \\(d=64\\).   </p> Plotting code <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n\ndim = 64\n\nindices = np.arange(0, dim // 2)\nbase = 10_000\npower_term = np.power(base, -2 * indices / dim)\n\nplt.figure(figsize=(15,10))\nplt.ylabel(r'$\\mathrm{\\theta_i = 10000^{-\\frac{2i}{d}}}$')\nplt.xlabel(\"i\")\nplt.plot(power_term)\nplt.show()\n</code></pre> <p></p> <p>Now, let's see how \\(\\sin(m\\theta_i)\\) varies for different values of \\(m\\), restricting ourselves to \\(m=100\\). Recall that \\(m\\) here represents the positions of tokens. </p> Plotting code  <pre><code>fig, axs = plt.subplots(2, 2, figsize=(20, 15))\nfig.suptitle(\"Sine term for different values of m\", fontsize=16)\n\nm_values = [1, 10, 20, 99]  # Different values of m for each subplot\n\nfor i, ax in enumerate(axs.flat):\n    m = m_values[i]\n    ax.plot(np.sin(m * power_term))\n    ax.set_title(f\"m = {m}\")\n    ax.set_xlabel(\"i\")\n    ax.set_ylabel(r'$\\mathrm{\\sin(m \\times 10000^{-\\frac{2i}{d}})}$')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p> Each subplot here represents the positional encoding for a data input. For each subplot, the x-axis represents the \\(i^{th}\\) position of the \\(d\\) dimensional vector in position \\(m\\). I show these plots because it is not intuitive what the shape of a sinusoid of a exponential term would be. </p> <p>With this in mind, let us see a 2D plot of this encoding scheme, using  <code>seq_len = 10</code> and <code>dim=64</code>. </p> Plotting code <pre><code>def sinusoidal_embeddings(positions: np.array, dim: int, base=10000):\n    \"\"\"Interleaved sinusoidal position embeddings.\n\n    Like in the original transformer paper - interleaved sin and cos.\n    \"\"\"\n    indices = np.arange(0, dim // 2)\n    power_term = np.power(base, -2 * indices / dim)\n    angle = np.einsum(\"...,d-&gt;...d\", positions, power_term)\n    embeddings = np.stack([np.sin(angle), np.cos(angle)], axis=-1)\n    embeddings = embeddings.reshape(list(embeddings.shape[:-2]) + [-1])\n    return embeddings\n\nseq_len = 10\npositions = np.arange(seq_len)\nembeddings = sinusoidal_embeddings(positions=positions, dim=dim)\n\nplt.figure(figsize=(12,8))\nplt.pcolormesh(embeddings, cmap='viridis')\nplt.xlabel('Embedding Dimensions')\nplt.xlim((0, dim))\nplt.ylim((0, seq_len))\nplt.ylabel('Token position')\nplt.colorbar()\nplt.show()\n</code></pre> <p></p> <p>Each row represents the encoding that will be added to the embedding of a token.</p> <p>But even this plot is not useful in visualising how this \"helps\" in injecting position information into the inputs. One visualisation that may help is by looking at the Euclidean distance of each of the vectors to a vector at a particular position index. The plot below can provide some intution (here <code>seq_len</code> is 100 again):</p> Plotting Code <pre><code>seq_len = 10\npositions = np.arange(seq_len)\nembeddings = sinusoidal_embeddings(positions=positions, dim=dim)\n\nfig, axs = plt.subplots(2, 2, figsize=(20, 15))\nfig.suptitle(\"Euclidean of all vectors to a vector at a particular position\", fontsize=16)\n\nvector_idxes = [0, 10, 20, 75]  # different position for each subplot\n\nfor i, ax in enumerate(axs.flat):\n    idx = vector_idxes[i]\n    distances = np.linalg.norm(embeddings - embeddings[idx], axis=1)\n    ax.plot(np.arange(distances.shape[0]), distances, marker='o')\n    ax.set_title(f'Distance of Vectors to the {idx}th Vector')\n    ax.set_xlabel('Vector Index')\n    ax.set_ylabel('Euclidean Distance')\n    ax.grid(True)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>Here you can see that these graphs have a \"V\" shape - for a given vector, vectors that are close to it positionally generally have a smaller Euclidean distance than those that are further away positionally. </p> <p>You can also look at the \"frequency\" of the sinusoids at each \\(i\\) as \\(m\\) varies:</p> Plotting Code <pre><code>fig, axs = plt.subplots(2, 2, figsize=(20, 15))\nfig.suptitle(\"Frequency variation for each i as m varies\", fontsize=16)\n\ndim_idxes = [1, 10, 20, 30]  # Different values of m for each subplot\n\nfor i, ax in enumerate(axs.flat):\n    dim_idx = dim_idxes[i]\n    ax.plot(embeddings[:, dim_idx])\n    ax.set_title(f'i = {dim_idx}')\n    ax.set_xlabel('m')\n    ax.set_ylabel(r'$\\mathrm{\\theta_i = 10000^{-\\frac{2i}{d}}}$')\n    ax.grid(True)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>This makes sense because the frequency of the sinusoid is \\(\\theta_i\\). \\(\\theta_i\\) has a higher value for when \\(i\\) is smaller, and a smaller value when \\(i\\) is larger. </p>"},{"location":"blog/2024/07/02/rope-embeddings/#what-are-the-limitations-of-of-the-sinusoidal-scheme-proposed-in-the-original-transformers-paper","title":"What are the limitations of of the sinusoidal scheme proposed in the original transformers paper?","text":"<p>The most obvious issues are as listed: 1. The main problem with the scheme presented above is that everything is fixed. None of the components - the variation in the frequency range, the sinusoidal function itself etc. - gets updated during training.  2. The different sinusoids, while expressive, may not be the correct kind of expressive for all sequences and may not be capturing the complex positional relationships in the data. 3. For longer sequence lengths, you can see that the difference blurs for vectors that are further away. </p>"},{"location":"blog/2024/07/02/rope-embeddings/#bert-embeddings","title":"BERT embeddings","text":"<p>The BERT paper uses learned positional embeddings (though the paper doesn't explicitly state it). The main idea is this - instead of using predefined sinusoids like the original transformers paper used, the BERT paper defers the learning of how best to deal with a token's position to the model itself, so that the model learns this in conjunction with the other weights. </p> <p>Yet another embedding layer is defined, and instead of serving as a look up table for token ids to \\(d\\) dimensional vectors, this embedding layer serves as a look up table for a token's position in the sequence to a \\(d\\) dimensional vector.</p> <p>That is it, there's nothing more complicated. Let's quickly look at how the HuggingFace library does it.</p>"},{"location":"blog/2024/07/02/rope-embeddings/#bert-positional-embeddings-in-the-huggingface-library","title":"BERT positional embeddings in the HuggingFace library","text":"<p>Let's start with the <code>BertEmbeddings</code> class to see what happens. In the <code>BertEmbeddings</code> class, the <code>self.position_ids</code> is basically a 1 dimensional tensor that goes from <code>0</code> to <code>max_tokens - 1</code>. In the default case, <code>max_tokens</code> is 512, so <code>self.position_ids</code> is a \\(1 \\times 512\\) <code>tensor([[ 0, 1, 2, 3, ... 511]])</code> (in code, this happens here). Remember that for BERT, the number of input tokens is fixed, and shorter token sequences are padded.  In the <code>forward</code> method of <code>BertEmbeddings</code>, you can see that this tensor is then passed through a <code>torch.nn.Embedding</code> module, which is nothing more than a look up table casting ints to vectors. The result is then added to the token embeddings.</p> <pre><code>class BertEmbeddings(nn.Module):\n    def __init__(self, config):\n        ...\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n        ...\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n\n        # same as self.position_ids = torch.arange(config.max_position_embeddings).expand((1, -1))\n        self.register_buffer(\n            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n        )\n\n        ...\n\n    def forward(self, input_ids, ...):\n        ...\n        inputs_embeds = self.word_embeddings(input_ids)\n        ...\n        position_embeddings = self.position_embeddings(position_ids)\n\n        embeddings += position_embeddings  # &lt;- main line here\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n</code></pre> <p>That is basically how embedding is done in <code>BertEmbeddings</code>.  As an aside, let's also quickly look at how the output of this module is used. Looking at the <code>forward</code> method of the <code>BertModel</code> in Huggingface, you'll see that these embedding outputs become the <code>hidden_state</code>:</p> <p>In <code>BertModel</code>:</p> <pre><code>def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.Tensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -&gt; Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n        ...\n        encoder_outputs = self.encoder(\n                embedding_output,  # &lt;- this is \"hidden_state\" for the encoder\n                attention_mask=extended_attention_mask,\n                head_mask=head_mask,\n                encoder_hidden_states=encoder_hidden_states,\n                encoder_attention_mask=encoder_extended_attention_mask,\n                past_key_values=past_key_values,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n        )\n</code></pre> <p>and then you go into the <code>BertEncoder</code>:  <pre><code>def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = False,\n        output_hidden_states: Optional[bool] = False,\n        return_dict: Optional[bool] = True,\n    ) -&gt; Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n        ...\n        layer_outputs = layer_module(\n                hidden_states,  # &lt;- this contains the embeddings\n                attention_mask,\n                layer_head_mask,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                past_key_value,\n                output_attentions,\n        )\n</code></pre></p> <p>and now go all the way into the <code>BertSelfAttention</code> and investigate it line by line:</p> <pre><code>class BertSelfAttention(nn.Module):\n    def __init__(self, config, position_embedding_type=None):\n        super().__init__()\n        ...\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n\n        def forward(\n                self,\n                hidden_states: torch.Tensor,  # &lt;- this contains the embeddings, used as input to the attention mechanism\n                attention_mask: Optional[torch.FloatTensor] = None,\n                head_mask: Optional[torch.FloatTensor] = None,\n                encoder_hidden_states: Optional[torch.FloatTensor] = None,\n                encoder_attention_mask: Optional[torch.FloatTensor] = None,\n                past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n                output_attentions: Optional[bool] = False,\n        ) -&gt; Tuple[torch.Tensor]:\n                mixed_query_layer = self.transpose_for_scores(self.query(hidden_states))\n                ...\n                key_layer = self.transpose_for_scores(self.key(hidden_states))\n                value_layer = self.transpose_for_scores(self.value(hidden_states))\n</code></pre>"},{"location":"blog/2024/07/02/rope-embeddings/#rope-embeddings","title":"RoPE embeddings","text":""},{"location":"blog/2024/07/02/rope-embeddings/#background","title":"Background","text":"<p>In the embedding/encoding schemes above, the model has to learn the absolute position embeddings. But this is often not very useful under the following circumstances:</p> <ol> <li>Often times when training on some datasets, shorter unrelated token sequences are stacked together and trained on together. Here absolute positional embeddings make no sense because the actual first token of a sentence would have some arbitrary positional information based on what came before it.</li> <li>Other times, long token sequences are broken up into shorter sequences before passing through the attention layers. In this scenario absolute positions don't correspond to the actual positions of the tokens in the original sequences.</li> <li>In the standard attention mechanism, the dot product no longer cares about the positions of tokens. This means the attention mechanism is only ever indirectly dependent on the position of tokens because the positional embeddings are added to the token embeddings before the matrix multiplication.</li> </ol> <p></p> <p>For such cases, we want an embedding scheme that efficiently does some kind of relative positional embedding, and also explicitly \"injects\" this knowledge into the attention mechanism. What does relative positional embedding mean? We basically want a function \\(f(\\mathbf{x}, l)\\) that takes in as arguments an input token sequence \\(\\mathbf{x}\\) and its position \\(l\\) such that the dot product between 2 vectors \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) in positions \\(l_1\\) and \\(l_2\\) is only sensitive to \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\), and the relative position \\(l_1 - l_2\\). </p> <p>There are many ways to do this, but the scheme that most of the field has settled on was first proposed in the RoFormer paper, which we talk about in the next section.</p>"},{"location":"blog/2024/07/02/rope-embeddings/#what-are-rope-embeddings","title":"What are RoPE embeddings?","text":"<p>The RoFormer paper, and the EleutherAI blogpost that the paper mentions, explain the intuition behind rotational positional embeddings in detail. But both contains a lot of detail that obfuscated the essence (the \"Visual Intuition\" section of the blogpost continues to confuse me) for me.</p> <p>If you have found yourself on the same boat, below is hopefully a simpler and more practical explanation of it.</p> <p>The easiest way to to preserve the notion of some kind of \"relative\" distance between two token embeddings in the pre-softmax attention step, is to make use of the angle between them in their \\(d\\) dimensional space. This is because the dot product of two vectors is proportional to the cosine of the angle between them, and the pre-softmax matrix multiplication step is nothing but a series of dot products. </p> <p>A cunning way to do so would be to multiply the token embedding in the \\(m^{th}\\) position in the sequence by the following rotational matrix (using 1 as the first index for clarity + ease of using latex):</p> \\[ \\mathbf{R}_{d, m} = \\tiny{\\begin{bmatrix} \\cos(m\\theta_1) &amp; - \\sin(m\\theta_1) &amp; 0 &amp; 0 &amp; ...  &amp; 0 &amp; 0 \\\\ \\sin(m\\theta_1) &amp; \\cos(m\\theta_1) &amp; 0 &amp; 0 &amp; ...  &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\cos(m\\theta_2) &amp; - \\sin(m\\theta_2) &amp; ...  &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sin(m\\theta_2) &amp; \\cos(m\\theta_2) &amp; ...  &amp; 0 &amp; 0 \\\\ ... \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; ...  &amp; \\cos(m\\theta_{d/2}) &amp; - \\sin(m\\theta_{d/2}) \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; ...  &amp; \\sin(m\\theta_{d/2}) &amp; \\cos(m\\theta_{d/2}) \\\\ \\end{bmatrix}} \\] <p>where </p> \\[ \\theta_i = 10,000 ^{\\frac{-2i}{d}} \\] <p>i.e. the original definition of the angle as in the transformer paper.</p> <p>So now, your key values become: </p> \\[ \\mathbf{k}'_{m} = \\mathbf{R}_{d, m}\\mathbf{k}_{m} \\] <p>and your query values become </p> \\[ \\mathbf{q}'_{n} = \\mathbf{R}_{d, n}\\mathbf{q}_{n} \\] <p>These are now inputs to your attention mechanism. To see why multiplying your keys and queries with this rotational matrix is cunning, let's see what happens when you take the dot product of a key vector in position \\(m\\) and a query vector in position \\(n\\) (as is usual in the attention mechanism):</p> \\[ \\mathbf{k'}^{T}_{m}\\mathbf{q'}_{n} = \\mathbf{k}^T_{m}\\mathbf{R}^T_{d, m}\\mathbf{R}_{d, n}\\mathbf{q}_{n} \\] <p>The quantity \\(\\mathbf{R}^T_{d, m}\\mathbf{R}_{d, n}\\) is nothing but another rotation matrix! You can see the derivation pretty simply:</p> \\[ \\mathbf{R}^T_{d, m}\\mathbf{R}_{d, n} =  \\] \\[ \\tiny{ \\begin{bmatrix} \\cos(m\\theta_1) &amp;  \\sin(m\\theta_1) &amp; 0 &amp; 0 &amp; ...  &amp; 0 &amp; 0 \\\\ - \\sin(m\\theta_1) &amp; \\cos(m\\theta_1) &amp; 0 &amp; 0 &amp; ...  &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\cos(m\\theta_2) &amp;  \\sin(m\\theta_2) &amp; ...  &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -\\sin(m\\theta_2) &amp; \\cos(m\\theta_2) &amp; ...  &amp; 0 &amp; 0 \\\\ ... \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; ...  &amp; \\cos(m\\theta_{d/2}) &amp;  \\sin(m\\theta_{d/2}) \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; ...  &amp; -\\sin(m\\theta_{d/2}) &amp; \\cos(m\\theta_{d/2}) \\\\ \\end{bmatrix} \\begin{bmatrix} \\cos(n\\theta_1) &amp; - \\sin(n\\theta_1) &amp; 0 &amp; 0 &amp; ...  &amp; 0 &amp; 0 \\\\ \\sin(n\\theta_1) &amp; \\cos(n\\theta_1) &amp; 0 &amp; 0 &amp; ...  &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\cos(n\\theta_2) &amp; - \\sin(n\\theta_2) &amp; ...  &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sin(n\\theta_2) &amp; \\cos(n\\theta_2) &amp; ...  &amp; 0 &amp; 0 \\\\ ... \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; ...  &amp; \\cos(n\\theta_{d/2}) &amp; - \\sin(n\\theta_{d/2}) \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; ...  &amp; \\sin(n\\theta_{d/2}) &amp; \\cos(n\\theta_{d/2}) \\\\ \\end{bmatrix} } \\] \\[ = \\tiny{\\begin{bmatrix} \\cos(m\\theta_1)\\cos(n\\theta_1) + \\sin(m\\theta_1)\\sin(n\\theta_1) &amp;  -\\cos(m\\theta_1)\\sin(n\\theta_1) + \\sin(m\\theta_1)\\cos(n\\theta_1) &amp; 0 &amp; 0 &amp; ...  &amp; 0 &amp; 0 \\\\ - \\sin(m\\theta_1)\\cos(n\\theta_1) + \\cos(m\\theta_1)\\sin(n\\theta_1) &amp; \\cos(m\\theta_1)\\cos(n\\theta_1) + \\sin(m\\theta_1)\\sin(n\\theta_1) &amp; 0 &amp; 0 &amp; ...  &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; ... &amp;  ... &amp; ...  &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; ... &amp; ... &amp; ...  &amp; 0 &amp; 0 \\\\ ... \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; ...  &amp; \\cos(m\\theta_{d/2})\\cos(n\\theta_{d/2}) + \\sin(m\\theta_{d/2})\\sin(n\\theta_{d/2}) &amp;  -\\cos(m\\theta_{d/2})\\sin(n\\theta_{d/2}) + \\sin(m\\theta_{d/2})\\cos(n\\theta_{d/2})\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; ...  &amp; - \\sin(m\\theta_{d/2})\\cos(n\\theta_{d/2}) + \\cos(m\\theta_{d/2})\\sin(n\\theta_{d/2}) &amp; \\cos(m\\theta_{d/2})\\cos(n\\theta_{d/2}) + \\sin(m\\theta_{d/2})\\sin(n\\theta_{d/2})\\\\ \\end{bmatrix} } \\] \\[ = \\tiny{\\begin{bmatrix} \\cos(n-m)\\theta_1 &amp; - \\sin(n-m)\\theta_1 &amp; 0 &amp; 0 &amp; ...  &amp; 0 &amp; 0 \\\\ \\sin(n-m)\\theta_1 &amp; \\cos(n-m)\\theta_1 &amp; 0 &amp; 0 &amp; ...  &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\cos(n-m)\\theta_2 &amp; - \\sin(n-m)\\theta_2 &amp; ...  &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sin(n-m)\\theta_2 &amp; \\cos(n-m)\\theta_2 &amp; ...  &amp; 0 &amp; 0 \\\\ ... \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; ...  &amp; \\cos(n-m)\\theta_{d/2} &amp; - \\sin(n-m)\\theta_{d/2} \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; ...  &amp; \\sin(n-m)\\theta_{d/2} &amp; \\cos(n-m)\\theta_{d/2} \\\\ \\end{bmatrix} } \\] \\[ = \\mathbf{R}_{d, n-m} \\] <p>So we now basically have a way of \"injecting\" the relative distance between 2 tokens in a sequence into the attention mechanism!  All this theory still does not completely explain exactly how this leads to better training, as is common with all transformers behaviour. But the common consensus in the field has been that this injection, and corresponding multiplicative and sinusoidal relationship between relative position and the learned weights, makes it \"easier\" (more data efficient) for the model to learn sequences - a consensus that has been proved empirically over and over again.</p> <p>Now, let's see some basic plots of this. </p> <p>First, here's the code to generate \\(\\mathbf{R}_{d, m}\\) </p> <pre><code>def get_rot_matrix(dim: int, m: int):\n    # same as \n    # indices = np.arange(0, dim // 2)\n    # base = 10_000\n    # theta = np.power(base, -2 * indices / dim)\n    theta = 1.0 / (10000 ** (np.arange(0, dim, 2) / dim))\n    theta = m * theta\n    cos_theta = np.cos(theta)\n    sin_theta = np.sin(theta)\n    main_diagonal = np.repeat(cos_theta, 2)\n    rot = np.diag(main_diagonal)\n    off_diagonal = np.zeros(dim)\n    off_diagonal[::2] = sin_theta\n    rot += np.diag(-off_diagonal[:-1], k=1)\n    rot += np.diag(off_diagonal[:-1], k=-1)\n    return rot\n</code></pre> <p>To see how this affects the pre-softmax layer, let's take 2 identity vectors of dimension \\(d=64\\),</p> <pre><code>dim = 64\nk = np.ones((dim, 1))\nq = np.ones((dim, 1))\n</code></pre> <p>Without the rotation matrix, the value of \\(\\mathbf{k}_m^T\\mathbf{q}_n\\) has a constant value of 64 regardless of the value of \\(m\\) and \\(n\\). With the rotational matrix, lets's see how the value of \\(\\mathbf{k}^T_{m}\\mathbf{R}_{d, n-m}\\mathbf{q}_{n}\\) varies with \\(n-m\\) with the following plot:</p> Plotting code <pre><code>x = np.arange(0, 100)\nplt.plot(x, [(k.T@get_rot_matrix(dim=dim, m=_m)@q).item() for _m in x])\nplt.xlabel(r'Relative distance $\\mathrm{n - m}$')\nplt.ylabel(r'$\\mathbf{k}^T_{m}\\mathbf{R}_{d, n-m}\\mathbf{q}_{n}$')\nplt.axhline(y=64, color='r', linestyle=':')\nplt.text(x=70, y=62, s=\"y=64\", color='r', fontsize=12)\n</code></pre> <p></p> <p>This by itself doesn't actually show the mechanics of what will happen during training. During training, the vectors \\(\\mathbf{k}_m\\) and \\(\\mathbf{q}_n\\) are themselves the result of multiplying a weight vector \\(\\mathbf{W}_k\\) (for the key values), \\(\\mathbf{W}_q\\) (for the query values) to the token embedding at positions \\(m\\) and \\(n\\) respectively. This means that the pre-softmax value is actually </p> \\[ \\mathbf{k}^T_{m}\\mathbf{R}_{d, n-m}\\mathbf{q}_{n} = \\mathbf{x}_m^T\\mathbf{W}_k^T\\mathbf{R}_{d, n-m}\\mathbf{W}_q\\mathbf{x}_n \\] <p>Where \\(\\mathbf{W}_k, \\mathbf{W}_q\\) are learned weights. This means that the exact nature of the sinusoidal patterns we see in the plot as \\(n-m\\) increases can get very finely tuned during the training process.</p>"},{"location":"blog/2024/07/02/rope-embeddings/#implementation-in-practice","title":"Implementation in practice","text":"<p>So far, the code we have looked at is at a vector level. Now let's see how this would get implemented when you have to do it for an entire sequence of tokens. </p> <p>The query matrix \\(\\mathbf{Q} \\in \\mathbb{R}^{N \\times d}\\), where \\(N\\) is the number of tokens in the sequence and \\(d\\) is the dimension of each data point, can be represented as (switching back to 0th index because we're going to convert everything to code):</p> \\[ \\mathbf{Q} = \\left[ \\begin{array}{c} \\hphantom{-}--- q_0^T--- \\hphantom{-} \\\\ \\hphantom{-}--- q_1^T--- \\hphantom{-} \\\\ \\hphantom{-}--- q_2^T--- \\hphantom{-} \\\\ ...\\\\ \\hphantom{-}--- q_{N-1}^T--- \\hphantom{-} \\\\ \\end{array} \\right] \\] <p>where \\(q_i \\in \\mathbb{R}^d, q_i = \\mathbf{W}_q\\mathbf{x}_i\\) represents the query vector of the embedding of the token in the \\(i^{th}\\) position \\(\\mathbf{x}_i\\).</p> <p>Similarly for the key values:</p> \\[ \\mathbf{K} = \\left[ \\begin{array}{c} \\hphantom{-}--- k_0^T --- \\hphantom{-} \\\\ \\hphantom{-}--- k_1^T --- \\hphantom{-} \\\\ \\hphantom{-}--- k_2^T --- \\hphantom{-} \\\\ ...\\\\ \\hphantom{-}--- k_{N-1}^T --- \\hphantom{-} \\\\ \\end{array} \\right] \\] <p>Let's focus only on the query matrix for now. After applying the rotational matrix to each of the vectors, let's call the result \\(\\mathbf{Q'}\\):</p> \\[ \\mathbf{Q'} = \\left[ \\begin{array}{c} \\hphantom{-}--- q_0^{,T}--- \\hphantom{-} \\\\ \\hphantom{-}--- q_1^{,T}--- \\hphantom{-} \\\\ \\hphantom{-}--- q_2^{,T}--- \\hphantom{-} \\\\ ...\\\\ \\hphantom{-}--- q_{N-1}^{,T}--- \\hphantom{-} \\\\ \\end{array} \\right] \\] \\[ = \\left[ \\begin{array}{c} \\hphantom{-}--- \\mathbf{R}^T_{d, 0}q_0^T--- \\hphantom{-} \\\\ \\hphantom{-}--- \\mathbf{R}^T_{d, 1}q_1^T--- \\hphantom{-} \\\\ \\hphantom{-}--- \\mathbf{R}^T_{d, 2}q_2^T--- \\hphantom{-} \\\\ ...\\\\ \\hphantom{-}--- \\mathbf{R}^T_{d, N-1}q_{N-1}^T--- \\hphantom{-} \\\\ \\end{array} \\right] \\] <p>The \"naive\" way to get the rotary positional embeddings of the key/query matrix is simply - </p> <ol> <li>Multiplying each row \\(\\mathbf{q}_i\\) with the rotational matrix \\(\\mathbf{R}_{d, i}\\). </li> <li>Setting the result of the previous step as the \\(i^{th}\\) row of the resultant matrix.</li> </ol> <p>In naive python, the code would be like this:</p> <pre><code>np.random.seed(3)\nQ = np.random.randn(5, 4)\nQ_prime = np.zeros_like(Q)\nfor pos, q_i in enumerate(Q):\n    rot = get_rot_matrix(Q_prime.shape[-1], pos)\n    Q_prime[pos] = rot @ q_i.T\nQ_prime\n</code></pre> <p>and the result you get is <pre><code>array([[ 1.78862847,  0.43650985,  0.09649747, -1.8634927 ],\n       [ 0.1486459 , -0.42509122, -0.07646744, -0.62779673],\n       [ 0.45216792,  0.15874903, -1.33129326,  0.85816992],\n       [-1.11375321, -1.5680929 ,  0.06214963, -0.40299454],\n       [-0.81390684,  1.4235748 ,  1.02561261, -1.06090267]])\n</code></pre></p> <p>But there's a more cunning way of doing everything in a vectorised way and getting rid of the loop in the code snippet. Let's take a closer look at the multiplication of the rotational matrix with a query vector:</p> \\[ \\mathbf{R}_{d, m}\\mathbf{q}_m = \\tiny{\\begin{bmatrix} \\cos(m\\theta_0) &amp; - \\sin(m\\theta_0) &amp; 0 &amp; 0 &amp; ...  &amp; 0 &amp; 0 \\\\ \\sin(m\\theta_0) &amp; \\cos(m\\theta_0) &amp; 0 &amp; 0 &amp; ...  &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\cos(m\\theta_1) &amp; - \\sin(m\\theta_1) &amp; ...  &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sin(m\\theta_1) &amp; \\cos(m\\theta_1) &amp; ...  &amp; 0 &amp; 0 \\\\ ... \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; ...  &amp; \\cos(m\\theta_{\\frac{d}{2} -1}) &amp; - \\sin(m\\theta_{\\frac{d}{2} -1}) \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; ...  &amp; \\sin(m\\theta_{\\frac{d}{2} -1}) &amp; \\cos(m\\theta_{\\frac{d}{2} -1}) \\\\ \\end{bmatrix} \\begin{bmatrix} q^m_1 \\\\ q^m_2 \\\\ ... \\\\ q^m_{d-1} \\\\ q^m_d \\\\ \\end{bmatrix} } \\] <p>(let's drop the \\(m\\) superscript showing the sequence position on the individual elements of the \\(\\mathbf{q}\\) vector)</p> \\[ =\\tiny{ \\begin{bmatrix} \\cos({m\\theta_0})q_0 - \\sin({m\\theta_0})q_1 \\\\ \\sin({m\\theta_0})q_0 + \\cos({m\\theta_0})q_1 \\\\ \\cos({m\\theta_1})q_2 - \\sin({m\\theta_1})q_3 \\\\ \\sin({m\\theta_1})q_2 + \\cos({m\\theta_1})q_3 \\\\ ... \\\\ \\cos({m\\theta_{\\frac{d}{2} -1}})q_{d-1} - \\sin({m\\theta_{\\frac{d}{2} -1}})q_d \\\\ \\sin({m\\theta_{\\frac{d}{2} -1}})q_{d-1} + \\cos({m\\theta_{\\frac{d}{2} -1}})q_d \\\\ \\end{bmatrix} } \\] \\[ = \\tiny{ \\begin{bmatrix} \\cos({m\\theta_0}) \\\\ \\cos({m\\theta_0}) \\\\ \\cos({m\\theta_1}) \\\\ \\cos({m\\theta_1}) \\\\  ... \\\\ \\cos({m\\theta_{\\frac{d}{2} -1}}) \\\\ \\cos({m\\theta_{\\frac{d}{2} -1}}) \\end{bmatrix} \\odot \\begin{bmatrix} q_0 \\\\ q_1 \\\\ q_2 \\\\ q_3 \\\\ ... \\\\ q_{d-2} \\\\ q_{d-1} \\\\ \\end{bmatrix} } \\] \\[ + \\] \\[  \\tiny{ \\begin{bmatrix} \\sin({m\\theta_0}) \\\\ \\sin({m\\theta_0}) \\\\ \\sin({m\\theta_1}) \\\\ \\sin({m\\theta_1}) \\\\  ... \\\\ \\sin({m\\theta_{\\frac{d}{2} -1}}) \\\\ \\sin({m\\theta_{\\frac{d}{2} -1}}) \\end{bmatrix} \\odot \\begin{bmatrix} - q_1 \\\\ q_0 \\\\ - q_3 \\\\ q_2 \\\\ ... \\\\ - q_{d - 1} \\\\ q_{d - 2} \\\\ \\end{bmatrix} } \\] <p>where \\(\\odot\\) is the element-wise multiplication/Hadamard product.</p> <p>So now, </p> \\[ \\mathbf{Q'} = \\left[ \\begin{array}{c} \\hphantom{-}--- q_0^{,T}--- \\hphantom{-} \\\\ \\hphantom{-}--- q_1^{,T}--- \\hphantom{-} \\\\ \\hphantom{-}--- q_2^{,T}--- \\hphantom{-} \\\\ ...\\\\ \\hphantom{-}--- q_{N-1}^{,T}--- \\hphantom{-} \\\\ \\end{array} \\right] \\] \\[ = \\tiny{ \\begin{bmatrix} \\cos({0\\theta_0}) &amp; \\cos({0\\theta_0}) &amp; \\cos({0\\theta_1}) &amp; \\cos({0\\theta_1}) &amp;  ... &amp; \\cos({0\\theta_{\\frac{d}{2} -1}}) &amp; \\cos({0\\theta_{\\frac{d}{2} -1}}) \\\\ \\cos({1\\theta_0}) &amp; \\cos({1\\theta_0}) &amp; \\cos({1\\theta_1}) &amp; \\cos({1\\theta_1}) &amp;  ... &amp; \\cos({1\\theta_{\\frac{d}{2} -1}}) &amp; \\cos({1\\theta_{\\frac{d}{2} -1}}) \\\\ . \\\\ .\\\\ . \\\\ \\cos({(N-1)\\theta_1}) &amp; \\cos({(N-1)\\theta_1}) &amp; \\cos({(N-1)\\theta_2}) &amp; \\cos({(N-1)\\theta_2}) &amp;  ... &amp; \\cos({(N-1)\\theta_{\\frac{d}{2} -1}}) &amp; \\cos({(N-1)\\theta_{\\frac{d}{2} -1}}) \\\\ \\end{bmatrix} \\odot  \\left[ \\begin{array}{c} \\hphantom{-}--- q_0^{,T}--- \\hphantom{-} \\\\ \\hphantom{-}--- q_1^{,T}--- \\hphantom{-} \\\\ \\hphantom{-}--- q_2^{,T}--- \\hphantom{-} \\\\ ...\\\\ \\hphantom{-}--- q_{N-1}^{,T}--- \\hphantom{-} \\\\ \\end{array} \\right] } \\] \\[ + \\] \\[ \\tiny{ \\begin{bmatrix} \\sin({0\\theta_0}) &amp; \\sin({0\\theta_0}) &amp; \\sin({0\\theta_1}) &amp; \\sin({0\\theta_1}) &amp;  ... &amp; \\sin({0\\theta_{\\frac{d}{2} -1}}) &amp; \\sin({0\\theta_{\\frac{d}{2} -1}}) \\\\ \\sin({1\\theta_0}) &amp; \\sin({1\\theta_0}) &amp; \\sin({1\\theta_1}) &amp; \\sin({1\\theta_1}) &amp;  ... &amp; \\sin({1\\theta_{\\frac{d}{2} -1}}) &amp; \\sin({1\\theta_{\\frac{d}{2} -1}}) \\\\ . \\\\ .\\\\ . \\\\ \\sin({(N-1)\\theta_0}) &amp; \\sin({(N-1)\\theta_0}) &amp; \\sin({(N-1)\\theta_1}) &amp; \\sin({(N-1)\\theta_1}) &amp;  ... &amp; \\sin({(N-1)\\theta_{\\frac{d}{2} -1}}) &amp; \\sin({(N-1)\\theta_{\\frac{d}{2} -1}}) \\\\ \\end{bmatrix} \\odot  \\left[ \\begin{array}{c} \\hphantom{-}--- \\text{rearranged-q}_0^{,T}--- \\hphantom{-} \\\\ \\hphantom{-}--- \\text{rearranged-q}_1^{,T}--- \\hphantom{-} \\\\ \\hphantom{-}--- \\text{rearranged-q}_2^{,T}--- \\hphantom{-} \\\\ ...\\\\ \\hphantom{-}--- \\text{rearranged-q}_{N-1}^{,T}--- \\hphantom{-} \\\\ \\end{array} \\right] } \\] <p>where \\(\\text{rearranged-q}_i\\) is </p> \\[ \\begin{bmatrix} - q^i_1 \\\\ q^i_0 \\\\ - q^i_3 \\\\ q^i_2 \\\\ ... \\\\ - q^i_{d-1} \\\\ q^i_{d - 2} \\\\ \\end{bmatrix} \\] <p>So now, you can see pretty quickly that the way to implement rotary positional embeddings in a vectorised way is going to be something similar to:</p> <pre><code>from einops import repeat, rearrange\nimport numpy as np\n\n\ndef rotate_every_two(x):\n    \"\"\"\n    Similar to what EleutherAI's implementation of the MeshTransformer is in JAX\n    \"\"\"\n    x1 = x[:, ::2]\n    x2 = x[:, 1::2]\n    x = np.stack((-x2, x1), axis=-1)\n    return rearrange(x, \"... d j -&gt; ... (d j)\")\n\n\ndef apply_rotary_pos_emb(x: np.array, seq_idx=1, dim_idx=-1) -&gt; np.array:\n    \"\"\"apply rotary positional embedding\n\n    x in an input array, and the assumption that the shape is\n    (batch, seq_len, ..., dim)\n\n    Args:\n        x: the input array\n        seq_idx: the index in x.shape that shows the sequence length.\n            By default we assume it is 1\n        dim_idx: the index in x.shape that shows the number of dimensions\n            By default we assume it is -1\n\n    Returns:\n        a numpy array with rotary positional embedding applied\n    \"\"\"\n    dim = x.shape[dim_idx]\n    inv_freq = 1.0 / (10000 ** (np.arange(0, dim, 2) / dim))\n\n    sinusoid_inp = np.einsum(\"i , j -&gt; i j\", np.arange(x.shape[seq_idx]), inv_freq)\n    sines = np.sin(sinusoid_inp)\n    cosines = np.cos(sinusoid_inp)\n\n    def _apply_repeat(_arr: np.array) -&gt; np.array:\n        return repeat(_arr, \"b n -&gt; b (n j)\", j=2)\n    sin = _apply_repeat(sines)\n    cos = _apply_repeat(cosines)\n    return (x * cos) + (rotate_every_two(x) * sin)\n\napply_rotary_pos_emb(Q, seq_idx=0)\n</code></pre> <p>and the result is something like</p> <pre><code>array([[ 1.78862847,  0.43650985,  0.09649747, -1.8634927 ],\n       [ 0.1486459 , -0.42509122, -0.07646744, -0.62779673],\n       [ 0.45216792,  0.15874903, -1.33129326,  0.85816992],\n       [-1.11375321, -1.5680929 ,  0.06214963, -0.40299454],\n       [-0.81390684,  1.4235748 ,  1.02561261, -1.06090267]])\n</code></pre> <p>which is the same result you get using the \"naive\" python approach! This approach is also how the HuggingFace repository and other repositories implement rotary positional embeddings.</p> <p>However, due to computational efficiency reasons, you'll see the <code>rotate_every_two</code> function not implemented in that interleaved way. For example in the HuggingFace code for Llama, you'll see this function instead:</p> <p><pre><code>def rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n\n    Args:\n        q (`torch.Tensor`): The query tensor.\n        k (`torch.Tensor`): The key tensor.\n        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n        sin (`torch.Tensor`): The sine part of the rotary embedding.\n        position_ids (`torch.Tensor`, *optional*):\n            Deprecated and unused.\n        unsqueeze_dim (`int`, *optional*, defaults to 1):\n            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n    Returns:\n        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n    \"\"\"\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n</code></pre> So instead of interleaving the (negative) even and odd dimensional elements of each input tensor as is the case in \\(\\text{rearranged-q}_i\\), they just take the last half of the tensor, negate it, and concatenate it with the first half. So in this case \\(\\text{rearranged-q}_i\\) is </p> \\[ \\begin{bmatrix} - q_{\\frac{d}{2}} \\\\ - q_{\\frac{d}{2} + 1} \\\\ - q_{\\frac{d}{2} + 2} \\\\ ... \\\\ - q_{d - 1} \\\\ q_0 \\\\ q_1 \\\\ ... \\\\ q_{\\frac{d}{2} - 1} \\\\ \\end{bmatrix} \\] <p>While it doesn't do the exact same thing, the main aim - that \\(\\mathbf{k'}^{T}_{m}\\mathbf{q'}_{n}\\) is dependent on \\(m - n\\) is still achieved. You can see that this is indeed the case by substituting \\(\\text{rearranged-q}_n\\) and \\(\\text{rearranged-k}_n\\) back into \\(\\mathbf{q'}_{n}, \\mathbf{k'}^{T}_{m}\\) respectively, and then writing out the multiplication. To see that this is the case.</p> <p>Hopefully this has been a useful discourse in rotary positional embeddings!</p>"},{"location":"blog/2024/01/05/self-attention/","title":"How does self attention work?","text":"<p>Self-attention is the cornerstone of a transformer block. We are going to go through the intuition behind the following steps:</p> <ol> <li>Tokenization</li> <li>Embedding</li> <li>The self attention mechanism</li> </ol> <p>by using raw, non-vectorised Python code as much as possible.</p> <p>The original transformer paper here is actually easy to read for a technical paper, but difficult to wrap your head around if this is the first time you are dipping your toes into neural networks. Other resources, like \"The Illustrated Transformer\" don't really have code. One resource that does have code is the \"Understanding Deep Learning\" (UDL) book, that I will be leaning on heavily for this post.</p> <p>Before we get to the famous equation -</p> \\[ \\text{selfAttention}[X] = V[X]\\text{softmax}(\\frac{QK^T}{\\sqrt{D_q}}) \\] <p>let's see what the steps often are before a matrix/tensor gets to the self-attention mechanism. </p> <p>Many images are from the UDL book, and a lot of the code is covered in the notebooks  published alongside the book. </p> <p>Before diving into architectural details, we have to remind ourselves what the problem it is that transformers had set out to solve: translation </p>"},{"location":"blog/2024/01/05/self-attention/#brief-introduction-to-the-problem-of-translation","title":"Brief introduction to the problem of translation","text":"<p>Translation is the \"flagship\" natural language processing task. The general inputs and outputs of the problem are incredibly simple to describe - given a sequence of words in one language, return a sequence of words in another language automatically. </p> <pre><code>Input: I am a student.\n\nOutput: \u0986\u09ae\u09bf \u098f\u0995\u099c\u09a8 \u099b\u09be\u09a4\u09cd\u09b0\u0964\n</code></pre> <p>Before deep learning methods were used to tackle this problem, companies like Google used statistical, phrase-based systems that did not work that well, and did not scale to many languages.</p> <p>Nowadays, all dominant translation systems use neural networks to do their translation. </p>"},{"location":"blog/2024/01/05/self-attention/#how-do-you-train-a-neural-machine-translation-system","title":"How do you train a  neural machine translation system?","text":"<p>There are many approaches to neural machine translation, but what I'll cover here is sequence to sequence way the \"Attention is all you need\" paper did it.</p> <p>We'll walk through the following broad sections: 1. Tokenization 2. Embedding 3. Self attention</p>"},{"location":"blog/2024/01/05/self-attention/#tokenization-brief-remarks","title":"Tokenization - brief remarks","text":"<p>To use any neural based system, inputs and outputs must be vectors of floats.  The process of converting words/sequence of words into vectors of floats is called tokenization. Tokenization won't be covered in detail here, and there are several resources that go over the different techniques (my personal favourite is the series of lectures from Stanford). The main thing to remember for tokenization is the input and the output - (example is from HuggingFace): </p> <pre><code>from transformers import BertTokenizer\n\ninp = \"I love gpus!\"\n\ntokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n\ninp_str = \"I have a new GPU!\"\ntokenizer.tokenize(inp_str)\n\n# ['i', 'have', 'a', 'new', 'gp', '##u', '!'] =&gt; len= 7\n\n# Huggingface's tokenizers' .encode() method tends to add a special start and end tokens\n\ntokenizer.encode(inp_str)\n# each number here represents the index of each token in the tokenizer's entire vocabulary\n# [101, 1045, 2031, 1037, 2047, 14246, 2226, 999, 102] =&gt; len= 9 (7+2)\n\ntokenizer.decode([101, 1045, 2031, 1037, 2047, 14246, 2226, 999, 102])\n\n# '[CLS] i have a new gpu! [SEP]'\n\"\"\"\nCLS -&gt; Special CLASSIFY token usually added to the beginning of a sequence\nIn HF tokenizers, this is usually 101\n\nSEP -&gt; separator token, usually added at the end of a sequence\nIn HF tokenizers, this is usually 102\n\"\"\"\n</code></pre> <p>There are several popular tokenization schemes: the original Attention paper doesn't actually describe what tokenizing scheme it used, but it is commonly believed they used byte pair encoding (BPE). The GPT family of models also use some variation of BPE, see for example tiktoken, which does BPE at a byte level. Doing BPE at a byte level (as opposed to the more classic Unicode word/character level tokenization) means you don't really have the \"out of vocabulary\" problem when you move from one modality to another.</p> <p>Other popular tokenization methods are WordPiece (which is used in the BERT paper), and SentencePiece which can be used to tokenize multiple languages including non-Latin languages (which Google's T5 uses).</p> <p>Depending on your use case, you can also implement your own tokenization scheme! Before the transformers paper for example, a briefly popular paper was the ByteNet paper that did precisely this and implemented a character-level tokenization scheme. </p>"},{"location":"blog/2024/01/05/self-attention/#embedding","title":"Embedding","text":"<p>What happens after tokenziation? In the Huggingface transformer's library, after tokenisation, you get a sequence of \\(N\\) tokens, each of which is an integer that represents the index in the tokeniser's vocabulary. </p> <p>What we then want to do is convert each of these integers into a vector \\(\\in \\mathbb{R}^D\\) where \\(D\\) is some embedding dimension that is smaller than the size of the vocabulary in the tokenizer. This means that the sequence of integers would be converted into a sequence of vectors, i.e. a matrix.</p> <p>The way this is done in most architectures is through a simple look up table. If the size of the tokenizer's vocabulary is \\(N_{\\text{vocab}}\\) all we need is a \\(N_{\\text{vocab}} \\times D\\) matrix, where row \\(i\\) corresponds to the \\(D\\) dimensional representation of token index \\(i\\). This \\(N_{\\text{vocab}} \\times D\\) matrix is learnable. </p> <p>Let's see how it happens in code:</p> <p>Let's have a look at the inputs and outputs of <code>torch</code>'s <code>nn.Embedding</code> : <pre><code>import torch.nn as nn\nimport torch\nembedding_dim = 3\nvocab_size = 5\nembedding_layer = nn.Embedding(vocab_size, embedding_dim)\n\n# toy example - how would a sequence of ones [1., 1., ...] be embedded?\nout = embedding_layer(torch.ones((6,), dtype=int))\nout\n</code></pre> prints the following: <pre><code>tensor([\n        [-0.8257, 0.0528, 1.3637],\n        [-0.8257, 0.0528, 1.3637],\n        [-0.8257, 0.0528, 1.3637],\n        [-0.8257, 0.0528, 1.3637],\n        [-0.8257, 0.0528, 1.3637],\n        [-0.8257, 0.0528, 1.3637]],\n        grad_fn=&lt;EmbeddingBackward0&gt;)\n</code></pre> a 6x3 matrix. If you want to dig further into the nature of the embedding layer, you can see the following:</p> <pre><code>embedding_layer.weight\n\ntensor([\n    [ 0.2668, -1.0410, -1.5245],\n    [-0.8257, 0.0528, 1.3637],\n    [-1.7534, -0.4505, -1.0951],\n    [-0.6984, -1.7775, -1.3832],\n    [ 2.5235, -0.7539, -2.1454]], requires_grad=True)\n</code></pre> <p>The output of  <code>embedding_layer(torch.ones((6,), dtype=int))</code> is basically the 2nd row (index of 1) <code>[-0.8257, 0.0528, 1.3637]</code> repeated 6 times. So all the embedding layer \\(N_{\\text{vocab}} \\times D\\) does is act as a look-up matrix.</p> <p>In pure python, the code for the embedding layer will be something as simple as:</p> <pre><code>import random \n\nclass PurePythonEmbeddingLayer:\n    def __init__(self, vocab_size: int, embedding_dim: int):\n        self.weight = [[random.random() for _ in range(embedding_dim)] for _ in range(vocab_size)]  # &lt;- the trainable lookup!\n\n    def forward(self, inputs: list[int]):\n        out = []\n        for inp in inputs:\n            out.append(self.weight[inp])\n        return out\n    ...\n</code></pre> <p>From this simple building block, you can make things as complicated as you want. The <code>BertEmbeddings</code> class in the <code>transformers</code> library shows you that for transformers, you will need to encode the position information of the text as well among other things. </p>"},{"location":"blog/2024/01/05/self-attention/#positional-embeddingencoding","title":"Positional Embedding/Encoding","text":"<p>The original transformers paper also embeds the positional information into the input sequence to the self attention layers. What  does positional embeddings do? Imagine if we didn't have the positional embeddings. When the self-attention mechanism is presented in the next section, you'll realise that each token embedding is treated independently of its position in a sentence. We need a way to \"inject\" the fact that a token's position is important before the embeddings enter as input into the self attention mechanism, where they will be further projected into many more different kind of vectors in different spaces. </p> <p>A motivating example would be something like processing a sentence - <code>a person ate the fish</code>. This has a completely different meaning to another ordering of the words - <code>a fish ate the person</code> - the positions of the words <code>fish</code> and <code>person</code> change the meaning of the phrases, and we need a mechanism to preserve that.</p> <p>The way the original transformers paper did this was by using sinusoidal encodings. The scheme they use is something like </p> \\[ \\text{PE}_{(pos, 2i)} = sin(\\frac{pos}{10000^{\\frac{2i}{d_\\text{model}}}}) \\] <p>for tokens in the even indexes of the hidden dimension, and</p> \\[ \\text{PE}_{(pos, 2i + 1)} = cos(\\frac{pos}{10000^{\\frac{2i}{d_\\text{model}}}}) \\] <p>for tokens in the odd index of the hidden dimension. </p> <p>These encodings would then be added to the learned embeddings. </p> <p>In pure python code, this would look something like this</p> <pre><code>def positional_encoding(seq_len: int, d_model: int) -&gt; list[list[float]]:\n    \"\"\"Given a sequence length, and a model embedding size, return a matrix\n    of seq_len x d_model encoding the positions\n\n    Args:\n        seq_len: number of tokens\n        d_model: dimensions of the input to the embedding vector\n\n    Returns:\n        a matrix of seq_len x d_model\n    \"\"\"\n    def _get_angle(pos, d_i):\n        denom = 10_000 ** ((2 * (d_i // 2)) / d_model)\n        return pos / denom\n\n    out = []\n    for pos in range(seq_len):\n        row = []\n        for d_i in range(d_model):\n            _fn = math.sin if d_i % 2 == 0 else math.cos\n            angle = _get_angle(pos, d_i)\n            row.append(_fn(angle))\n        out.append(row)\n\n    return out\n</code></pre> <p>When you plot it for <code>seq_len = 10</code> and <code>d_model = 64</code> (just toy values), you will get something like </p> <p> You can look at Jay Alamar's code to see how to do this in a vectorised manner, which I have reimplemented here in numpy: </p> <pre><code>def sinusoidal_encodings(pos, dim, base=10000):\n    \"\"\"Interleaved sinusoidal position embeddings.\n\n    Like in the original transformer paper - interleaved sin and cos.\n    \"\"\"\n    indices = np.arange(0, dim // 2)\n    power_term = np.power(base, -2 * indices / dim)\n    angle = np.einsum(\"...,d-&gt;...d\", pos, power_term)\n    encodings = np.stack([np.sin(angle), np.cos(angle)], axis=-1)\n    encodings = encodings.reshape(list(encodings.shape[:-2]) + [-1])\n    return encodings\n</code></pre> <p>In the old tensor2tensor library, the way positional encoding was done was actually by not interleaving the sines and cosines, but instead by doing something like this instead:</p> <pre><code>def positional_encoding_concat(seq_len: int, d_model: int) -&gt; list[list[float]]:\n    \"\"\"Given a sequence length, and a model embedding size, return a matrix\n    of seq_len x d_model encoding the positions.\n\n    The difference this time is that you don't interleave the sines and the cosines\n    but follow the tensor2tensor style of just concatenating sines and cosines\n\n    Args:\n        seq_len: number of tokens\n        d_model: dimensions of the input to the embedding vector\n\n    Returns:\n        a matrix of seq_len x d_model\n    \"\"\"\n    def _get_angle(pos, d_i):\n        denom = 10_000 ** (d_i / (d_model/2))\n        return pos / denom\n\n    out = []\n    for pos in range(seq_len):\n        row = []\n        for d_i in range(d_model//2):\n            angle = _get_angle(pos, d_i)\n            row.append(math.sin(angle))\n        for d_i in range(d_model//2):\n            angle = _get_angle(pos, d_i)\n            row.append(math.cos(angle))\n        out.append(row)\n\n    return out\n</code></pre> <p>for which you'd get the plot  The main difference being in 1. the <code>_get_angle</code> method 2. the for loop where the values are being appended</p> <p>According to the Tensorflow tutorial on transformers, these 2 are functionally equivalent. You can see why - the main aim of positional encodings is to make sure that embeddings that are near each other have a \"similar\" positional encoding, while those far away have different positional encodings. The second implementation is easier to do in a vectorised manner.</p> <p>The advantage of positional encodings that use such a sinusoidal pattern is that it can generalise to sequence lengths not seen before during training, because it is a deterministic function with respect to the token's position in the input.</p> <p>So the final embedding layer that includes positional encoding will look something like: </p> <pre><code>import random \n\nclass PurePythonEmbeddingLayer:\n    def __init__(self, vocab_size: int, embedding_dim: int):\n        self.weight = [[random.random() for _ in range(embedding_dim)] for _ in range(vocab_size)]  # &lt;- the trainable lookup!\n        self.embedding_dim = embedding_dim\n\n    def forward(self, inputs: list[int]):\n        out = []\n        positional_encodings = positional_encoding_concat(seq_len=len(inputs), d_model=self.embedding_dim)\n        for i, inp in enumerate(inputs):\n            token_embedding = self.weight[inp]\n            positional_encoding = positional_encodings[i]\n            out.append([_t + _p for _t, _p in zip(token_embedding, positional_encoding)])\n        return out\n    ...\n</code></pre> <p>Why don't we just make the model learn how to embed position instead of hardcoding it?  The answer to that is, you can! The BERT paper, which basically takes the original transformer paper and scales it up immensely, does precisely this. The <code>positional_embedding</code> is just another learnable parameter in the BERT encoder, as opposed to the deterministic vector you see in the original transformer paper. This increases the number of parameters a model has. The disadvantage of a scheme like this is that at inference time, you cannot have sequence lengths that are longer than what the model has been trained at </p> <p>Other more sophisticated embedding schemes include rotary positional embeddings (RoPE). RoPE is what the later versions of the GPT class of models use. This will not be discussed in greater detail here. </p> <p>Now that you have the embeddings ready, we are finally in a position to explain how the self attention mechanisms works. </p>"},{"location":"blog/2024/01/05/self-attention/#self-attention","title":"Self-attention","text":""},{"location":"blog/2024/01/05/self-attention/#why-self-attention-why-not-just-use-fully-connected-layers","title":"Why self-attention? Why not just use fully connected layers?","text":"<p>The UDL book makes a very compelling argument, that will be reproduced in part here. </p> <p>Here's an example of an actual review from Amazon:</p> <pre>\nin 1989, I managed a crummy bicycle shop, \"Full Cycle\" in Boulder, Colorado. The Samples had just recorded this album and they played most nights, at \"Tulagi's\" - a bar on 13th street. They told me they had been so broke and hungry, that they lived on the free samples at the local supermarkets - thus, the name. i used to fix their bikes for free, and even feed them, but they won't remember. That Sean Kelly is a gifted songwriter and singer.\n</pre> <p>There are immediately 3 observations you can make about using just fully connected layers:</p> <ol> <li>This is 83 word review, and roughly 110 tokens (remember - everything is in terms of token length!) worth of text. Imagine now that each token is represented by a vector of size 1024 (the embedding dimension \\(\\textbf{D}\\)). If you were to have a model consisting only of fully connected layers, each layer would contain \\(N^2D^2\\)  parameters, and with \\(k\\) such layers, you'd have \\(kN^2D^2\\) such parameters. This is a lot of parameters.</li> <li>Each review on Amazon can have a variable number of inputs. How would you take that into account when designing a model consisting only of fully connected layers? You could technically just fix the upper bound of N to some arbitrarily large value, and fill the positions with no text with some arbitrary \"pad\" tokens, but this feels like an inefficient way of learning representations. </li> <li>Language is inherently ambiguous, and it is unclear from syntax alone what the semantic meaning of each token is. <ol> <li>In the example above, the word <code>That</code> in the last sentence doesn't specifically refer to any object or subject. </li> <li>This means that we want an architecture where there must be connections between word representations and that the strength of these connections should depend on the words themselves. </li> <li>Technically you could have fully connected layers that do the same thing. In such a case, you'd hope the superfluous weights go to zero, and all weights that connect neurons that belong to the same group of tokens to other such groups would all be the same during the learning process. But this is an incredibly inefficient way of learning. This is probably easier to see in the diagram below. <ol> <li>In the top part, you want all the lines of the same colour to be the same weight.</li> <li>In the second part, you see that it is also technically possible to do this with fully connected layers, but this adds a lot of parameters for training without adding any real value, because we know the constraints we would like them to fulfill. </li> </ol> </li> </ol> </li> </ol> <p></p>"},{"location":"blog/2024/01/05/self-attention/#self-attention-block","title":"Self attention block","text":"<p>We can now turn our attention to what the self attention block does. We will proceed from the outputs of the embedding layer as input to this block</p> <p>If we were to use a standard neural network, each \\(D\\times1\\) input would have a linear transformation applied to it, followed by a non-linear activation function (softmax, ReLU etc.) applied to it. We can represent the mapping of a standard neural network on an input \\(\\mathbf{x}\\) as </p> \\[ f(\\mathbf{x}) = \\text{someNonLinearFunction}(\\mathbf{Ax + b}) \\] <p>How is a self attention block different? The simplest way to think of a self attention block is in 2 steps.</p> <p>Assume an input of \\(N\\) vectors \\(x_1, x_2, .... x_N\\), each of dimension \\(D\\). </p> <p>The first step in an a self-attention block is the following - for each of the \\(N\\) vectors, a corresponding value is computed using the standard way - </p> \\[ \\mathbf{v}_m = \\mathbf{A}_v\\mathbf{x}_m + \\mathbf{b}_v \\] <p>where \\(\\mathbf{A} \\in \\mathbb{R}^{D\\times D}, \\mathbf{b} \\in \\mathbb{R}^{D}\\) , are shared across all input vectors. You can see this in the description section of the original transformers paper. </p> <p>The second step is then computing a weighted sum across this set of values \\(\\mathbf{v}_1,  ..., \\mathbf{v}_N\\) for each \\(i \\in [1, N]\\):</p> \\[ \\text{out}_i = \\sum_{j=1}^{N}a(\\mathbf{x}_j, \\mathbf{x}_i)\\mathbf{v}_j \\] <p>where \\(a(\\mathbf{x}_j, \\mathbf{x}_i)\\) is the attention that the \\(i^{th}\\) output pays to the \\(j^{th}\\) input. A weighted sum means that for each \\(i\\), the sum \\(\\sum_{j=1}^{N}a(\\mathbf{x}_j, \\mathbf{x}_i) = 1\\) , and each of the weights \\(a(\\mathbf{x}_j, \\mathbf{x}_i)\\) is non-negative.</p> <p>So what is the attention function? It's two inputs - the first is \\(\\mathbf{x}_m\\), i.e. the \\(m^{th}\\) input vector, whose corresponding value vector \\(\\mathbf{v}_m\\) is what the result of the attention computation will be multiplied with. The second is \\(\\mathbf{x}_i\\), the \\(i^{th}\\) input vector, where \\(i\\) is also the position of the output vector we are trying to compute. We will need to compute 2 linear transformations first - the first being</p> \\[ \\mathbf{q}_i = \\mathbf{A}_q\\mathbf{x}_i + \\mathbf{b}_q \\] <p>which is called the query value, computed on the output's \\(i^{th}\\) position. The second linear transformation is </p> \\[ \\mathbf{k}_m = \\mathbf{A}_k\\mathbf{x}_m + \\mathbf{b}_k \\] <p>which is called the key value, computed on the \\(m^{th}\\) input vector. </p> <p>The attention function \\(a(\\mathbf{x}_j, \\mathbf{x}_i)\\) will then be defined as:</p> \\[ a(\\mathbf{x}_j, \\mathbf{x}_i) := \\text{softmax}_m(\\mathbf{k}^T_{\\circ}\\mathbf{q}_i) \\\\ = \\frac{\\text{exp}(\\mathbf{k}^T_j\\mathbf{q}_i)}{\\sum_{j'=1}^{N}\\text{exp}(\\mathbf{k}_{j'}^T\\mathbf{q}_i)} \\] <p>The following diagram from the UDL book is instructive to visualise the 2 different stages of calculating the </p> <p></p> <p>All this seems pretty unintuitive - what is the correct \"mental model\" to have for the key, query, and value vectors we have just introduced? I'm going to modify the insight Eugene Yan's blog here has on this - </p> <ol> <li>Say you are in a library, and you want some knowledge. To get knowledge, you basically want the answers to N queries you have. </li> <li>Now in the library, there are a lot of books. Each book has a spine with its title, which you can think of as its key, and each book has some content, which is it's value. </li> <li>Now, for each query you have, you look at the spine of each book to decide how much attention should give to the contents of that book for that particular query, and you do it for all books present in the library.</li> </ol> <p>Keys and values tend to be computed on the same input matrices, but query values are often computed on other input matrices (it is called self-attention when all these operations are on the same inputs). As an example of these operations being applied to different inputs, you have to look no further than the \"Attention is all you need\" paper, particularly this diagram: </p> <p></p> <p>In the decoder, at the point in the model where the decoder takes into account the encoder outputs as well, the key and values will be computed as:</p> \\[ K = A_kX_{\\text{enc}} + B_k \\] \\[ V = A_vX_{\\text{enc}} + B_v \\] <p>while the queries will be on the decoder representation of the outputs:</p> \\[ Q = A_qX_{\\text{dec}} + B_q \\] <p>where \\(X_{\\text{enc}}, X_{\\text{dec}}\\) are the matrices as illustrated in the diagram above - \\(X_{\\text{enc}}\\) being the encoder  output, and \\(X_{\\text{dec}}\\) being the decoder representation of the output embedding at that layer (assume column first ordering for this series of equations). </p> <p>This is sometimes called cross-attention, and if you inspect the Huggingface code, you will see this term cropping up. Let's focus just on self-attention again.</p> <p>What does this all look like in code? You can look at the notebooks provided by the author of the UDL book for a clearer understanding, and I'm going to reproduce something very similar to the book here.</p> <p>For our toy example, let's assume the output of the embeddings are 3 input vectors \\(\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3\\) , each with dimension \\(D = 4\\).  </p> <pre><code>import numpy as np\n# Set seed so we get the same random numbers\nnp.random.seed(3)\n# Number of inputs\nN = 3\n# Number of dimensions of each input\nD = 4\n\nall_x = []\n# Create elements x_n and append to list\nfor n in range(N):\n  all_x.append(np.random.normal(size=(D,1)))  # &lt;- doesn't really matter for this toy example\n\nprint(all_x)\n</code></pre> <p>Now, let's initialise the weights for \\(A_q, A_k, A_v\\) and biases \\(b_q, b_k, b_v\\): </p> <pre><code># all the weights are of dimension DxD \nA_q = np.random.normal(size=(D,D))\nA_k = np.random.normal(size=(D,D))\nA_v = np.random.normal(size=(D,D))\n\n# all the biases are of dimension Dx1\nb_q = np.random.normal(size=(D,1))\nb_k = np.random.normal(size=(D,1))\nb_v = np.random.normal(size=(D,1))\n</code></pre> <p>We can now compute the keys, queries, and values for each of inputs:</p> <pre><code># Make three lists to store queries, keys, and values\nall_queries = []\nall_keys = []\nall_values = []\n# For every input\nfor x in all_x:\n  query = A_q @ x + b_q\n  key = A_k @ x + b_k \n  value = A_v @ x + b_v\n\n  all_queries.append(query)\n  all_keys.append(key)\n  all_values.append(value)\n</code></pre> <p>Side note regarding multiplying numpy arrays: the <code>@</code> operator is a standard matrix multiplication operation, and requires 2 matrices of dimensions \\(A \\times B\\),  and \\(B \\times C\\), and will result in a \\(A\\times C\\) matrix. The <code>*</code> operator can only be used if your matrix pair \\(M_1, M_2\\) are either vectors or square matrices, and is often not what you want. </p> <p>We then need the softmax function: <pre><code>def softmax(items_in: list):\n  e_x = np.exp(items_in - np.max(items_in))\n  return e_x / e_x.sum()\n</code></pre> Subtracting the maximum value in the exponent is a common technique to make sure the values in the numerator (<code>e_x</code> in the snippet above) do not become too large, and you have to start worrying about things like overflow etc. in the underlying programming language. Functionally they don't change the softmax function, since</p> \\[ \\frac{e^{x_i}}{\\sum^{N}_{i=1}{e^{x_i}}} = \\frac{(\\frac{1}{e^C})e^{x_i}}{(\\frac{1}{e^C})\\sum^{N}_{i=1}{e^{x_i}}} = \\frac{e^{x_i - C}}{\\sum^{N}_{i=1}{e^{x_i - C}}} \\] <p>For a pretty interesting discussion about why we use softmax, and the interpretation of this non-linear function, feel free to look at Pinecone's handy guide here.  </p> <p>We are now in a position to compute each of the outputs. As a reminder, we are going to have \\(N\\) output vectors \\(\\text{out}_1, \\text{out}_2, \\text{out}_3, ... \\text{out}_N\\), each of which is a weighted average of the value vectors. </p> <pre><code># Make three lists to store queries, keys, and values\nall_queries = []\nall_keys = []\nall_values = []\n# For every input\nfor x in all_x:\n  query = A_q @ x + b_q\n  key = A_k @ x + b_k \n  value = A_v @ x + b_v\n\n  all_queries.append(query)\n  all_keys.append(key)\n  all_values.append(value)\n\n#\u00a0compute the outs\nall_outs = []\n\nfor i in range(N):\n    all_kj_qi = [] # &lt;-- will be a 1 x N vector\n    q_i = all_queries[i]\n    for key_j in all_keys:\n        dot_product = np.dot(key_j.T, q_i).squeeze()\n        all_kj_qi.append(dot_product)\n\n    attention = softmax(all_kj_qi) # &lt;-- 1 x N vector that sums to 1\n    out_i = sum(attention[i] * all_values[i] for i in range(N))\n    all_outs.append(out_i)\n</code></pre> <p>And that is it! You have basically implemented the self-attention mechanism from scratch using just (mostly) raw python loops! If you want to involve more matrix operations, you can do it the following way: </p> <pre><code>def softmax_cols(data_in):\n    # Exponentiate all of the values\n    _data_in = data_in - np.max(data_in, axis=1, keepdims=True)\n    exp_values = np.exp(_data_in)\n    # Sum over rows\n    denom = np.sum(exp_values, axis=1, keepdims=True)\n    # Compute softmax\n    softmax = exp_values / denom\n    # return the answer\n    return softmax\n\ndef self_attention(X, A_v, A_q, A_k, b_v, b_q, b_k):\n    \"\"\"Self attention in a vectorized manner\n\n    Assumption here is that each column of X is a data point. In literature, each data point is usually a row, and not a column. Doesn't change the main thrust of this function\n\n    Args:\n        X: X is a DxN matrix, where D is the dimension of the input vectors, and N is the number of input vectors\n        A_v: A_v is a DxD matrix\n        A_q: A_q is a DxD matrix\n        A_k: A_k is a DxD matrix\n        b_v: b_v is a Dx1 vector\n        b_q: b_q is a Dx1 vector\n        b_k: b_k is a Dx1 vector\n\n    Returns:\n        a DxN matrix, where each column is the output of the self attention mechanism\n    \"\"\"\n    # 1. Compute queries, keys, and values\n    Q = X @ A_q.T + b_q.T\n    K = X @ A_k.T + b_k.T\n    V = X @ A_v.T + b_v.T\n    # 2. Compute dot products\n    dot_pdts = Q @ K.T\n    # 3. Apply softmax to calculate attentions\n    attention = softmax_cols(dot_pdts)\n    print(attention.shape) # &lt;-- This will now be a NxN matrix!\n    # 4. Weight values by attentions\n    out = attention @ V\n\n    return out\n\nX = np.array(all_x).squeeze()\nout = self_attention(X, A_v, A_q, A_k, b_v, b_q, b_k)\n</code></pre> <p>This function presents the famous self-attention equation:</p> \\[ \\text{attentionOutput}(V, Q, K) = V\\text{softmax}(QK^T) \\] <p>more naturally. </p> <p>If you inspect the values of the \\(N\\times N\\) <code>attention</code> matrix, you'll notice the extreme values - some values are very close to 1, and many values are nearly 0. This is because there is a large variance in the values of \\(QK^T\\) - they become either too big a positive value, or too big a negative value. </p> <p>We ideally want to scale the values in the attention such that the variance in the input values to the softmax function is reduced to avoid the vanishing gradient problem. A hand-wavy justification for this is the following:</p> <p>Weights in a neural network are updated using backpropagation (you can read up the details of this method elsewhere). Let the \\(i^{th}\\) output of the softmax function be defined as \\(p_i\\) where</p> \\[ p_i = \\frac{e^{x_i}}{\\sum^{N}_{i=1}{e^{x_i}}} \\] <p>Then, when we look at the gradients of this function, we can see that the partial derivative of \\(p_i\\) with respect to \\(x_i\\) is -</p> \\[ \\frac{\\partial p_i}{\\partial x_i} = \\frac{\\partial}{x_i} \\left( \\frac{e^{x_i}}{e^{x_i}+ C} \\right) = \\frac{(e^{x_i} + C)e^{x_i} - e^{x_i}e^{x_i}}{(e^{x_i} + C)^2} = (\\frac{e^{x_i}}{e^{x_i}+ C})(\\frac{C}{e^{x_i}+ C}) = p_i(1 - p_i) \\] <p>where we use \\(C\\) in that hand-wavy way non-mathematicians use to express terms that can be treated as a \"constant\" in a partial derivative. Then for \\(j\\neq i\\), </p> \\[ \\frac{\\partial p_i}{\\partial x_j} = \\frac{\\partial}{x_i}\\left(\\frac{C'}{e^{x_j}+ C}\\right) = \\frac{-C'e^{x_j}}{(e^{x_j}+ C)^2} = -p_jp_i \\] <p>The last characteristic you need to remember about the softmax is the following: </p> \\[ \\sum_{i=1}^N p_i = 1 \\] <p>So if any \\(p_i\\) is very close to one, the partial derivatives will be close to 0 because the other term (either \\(1 - p_i\\) or \\(p_j\\)) is going to be very close to 0. Naturally, the partial derivatives will also be close to 0 if any of the \\(p_i\\)'s is close to 0. </p> <p>Scaling the inputs the softmax function is typically done by dividing the \\(\\mathbf{Q}\\mathbf{K^T}\\) result with \\(\\sqrt{D_k}\\) , i.e. the dimension of the keys (and the dimension of the queries). Why this particular constant? This is explained in the paper. As the dimensions of the keys and the queries increase, it is likely that the final result of \\(\\mathbf{Q}\\mathbf{K^T}\\) increases in value. Intuitively, this is because if \\(q\\) and \\(k\\) are independent random variables with 0 mean and 1 variance, the dot product \\(q \\cdot k = \\sum_{i=1}^{d_k} q_ik_i\\) will have mean 0 and variance \\(d_k\\).  This is easy to derive from first principles, remembering that \\(\\text{Var}[x]) = \\mathbb{E}[x^2] - (\\mathbb{E}[x])^2\\) . The mean calculation of \\(\\mathbf{k}^T \\cdot \\mathbf{q}\\) is the following (with \\(D\\) being the dimension of the key and the query)</p> \\[ \\mathbb{E}[\\mathbf{k}^T \\cdot \\mathbf{q}] = \\mathbb{E}\\left[ \\sum_{i=1}^{D} k_iq_i \\right] = \\sum_{i=1}^D \\mathbb{E}[q_ik_i] \\] <p>and since \\(k_i, q_i\\) are independent, you have </p> \\[ \\sum_{i=1}^D \\mathbb{E}[q_ik_i] = \\sum_{i=1}^D \\mathbb{E}[q_i]\\mathbb{E}[k_i] = 0 \\] <p>As for variance we need to consider the following:  </p> \\[ \\text{Var}[\\mathbf{k}^T \\cdot \\mathbf{q}] = \\mathbb{E}\\left[(\\mathbf{k}^T \\cdot \\mathbf{q})^2\\right] - \\left(\\mathbb{E}[\\mathbf{k}^T \\cdot \\mathbf{q}] \\right)^2 \\] <p>We only have to consider the first term of the right hand side, because as we've just established, \\(\\mathbb{E}[\\mathbf{k}^T \\cdot \\mathbf{q}]=0\\). Given that each of these variables have variance 1, this means that:</p> \\[ \\text{Var}(k_i) = 1 =&gt; \\mathbb{E}[k_i^2] - (\\mathbb{E}[k_i])^2 = 1 \\] <p>and since \\(\\mathbb{E}[k_i]=0\\), we know \\(\\mathbb{E}[k_i^2] = 1\\). The same holds for \\(q_i\\).</p> <p>So, it easily follows:</p> \\[ \\text{Var}[\\mathbf{k}^T \\cdot \\mathbf{q}] = \\mathbb{E}\\left[(\\mathbf{k}^T \\cdot \\mathbf{q})^2\\right] = \\mathbb{E}\\left[ \\left( \\sum_{i=1}^Dk_iq_i \\right) \\left( \\sum_{j=1}^Dk_jq_j \\right) \\right] =\\left[ \\sum_{i=1}^D \\sum_{j=1}^D  \\mathbb{E}(q_iq_jk_ik_j)\\right] \\] <p>and since the \\(k\\)'s and the \\(q\\)'s are independent:</p> \\[ \\left[ \\sum_{i=1}^D \\sum_{j=1}^D  \\mathbb{E}(q_iq_jk_ik_j)\\right] = \\left[ \\sum_{i=1}^D \\sum_{j=1}^D  \\mathbb{E}(q_iq_j)\\mathbb{E}(k_ik_j)\\right] \\] <p>now, for cases where \\(i \\neq j\\), you get \\(\\mathbb{E}(q_iq_j) = \\mathbb{E}(q_i)\\mathbb{E}(q_j) = 0\\), and so you only have to care about the cases where \\(i = j\\). This then simplifies everything to: </p> \\[  \\left[ \\sum_{i=1}^D \\sum_{j=1}^D  \\mathbb{E}(q_iq_j)\\mathbb{E}(k_ik_j)\\right] = \\sum_{i=1}^D \\mathbb{E}(q_i^2) \\mathbb{E}(k_i^2) =\\sum_{i=1}^D 1= D \\] <p>So to summarise this last part - the reason we scale everything by \\(\\sqrt{D_k}\\) is because under the assumption that \\(k\\)'s and \\(q\\)'s are independent variables with 0 mean and unit variance, this is the scaling factor we need to keep the variance of </p> \\[ \\mathbf{k}^T \\cdot \\mathbf{q} \\] <p>to 1, and the mean to be 0. </p> <p>The self attention block is the basic unit of the transformer, and its details are often not appreciated. Hopefully, this post has given you a better intuition for it by decomposing every step into its most basic form.  </p>"},{"location":"blog/2025/01/10/some-notes-from-implementing-model-agnostic-meta-learning/","title":"Some notes from implementing model agnostic meta learning","text":"<p>I noticed something interesting about PyTorch while implementing the MAML paper. </p>"},{"location":"blog/2025/01/10/some-notes-from-implementing-model-agnostic-meta-learning/#maml-pseudocode","title":"MAML Pseudocode","text":"<p>The general algorithm presented in the paper can be summarised by the following pseudocode:</p> <pre><code>Initialize \u03b8\n\nfor _ in range(num_outer_steps):\n    Sample a batch of tasks {T_i} ~ p(T), where P(T) is your distribution of tasks (such as the omniglot dataset)\n\n    all_losses = []\n    for each task T_i in {T_i}:\n        # Split data into support set D_i^support and query set D_i^query\n\n        for _ in range(num_inner_steps)\n        # 1) Compute adapted parameters using one or more gradient steps on D_i^support:\n            \u03b8_i' = \u03b8 - \u03b1 * \u2207_\u03b8 L_Ti(\u03b8, D_i^support)\n\n        # 2) Evaluate loss on the query set D_i^query using the adapted parameters \u03b8_i':\n        L_Ti_query(\u03b8_i') = L_Ti(\u03b8_i', D_i^query)\n        update_all_losses(all_losses, L_Ti_query(\u03b8_i'))\n\n    # 3) Compute average query loss across all tasks\n    L_Ti_query_avg(\u03b8_i') = compute_avg(all_losses)\n\n    # 4) Compute the meta-gradient:\n    \u03b8 \u2190 \u03b8 - \u03b2 * \u2207_\u03b8 \u03a3_i [ L_Ti_query_avg(\u03b8_i') ]\n</code></pre> <p>Refer to the paper for the motivation for doing training and inference using this method. The big idea here is that instead of training the model to predict something specific, you show the model examples (i.e. the support set) and then from there you teach it to infer the correct answer on the query set. </p> <p>So during training, your training set would consist of a support set and a query set, and the model's task is to predict the correct answer for the query set, given the support set. At test time, you'd pass in a support set as well - or your context, to use the today's LLM obsessed terminology - and ask the model to predict the answer for the query set. This is basically in-context learning. </p> <p>A couple of things to remember: </p> <ol> <li>Inner Loop (Adaptation): For each task, you temporarily update the model parameters by one (or a few) gradient steps using the support split.</li> <li>Outer Loop (Meta-Update): You then compute how well the adapted parameters perform on the corresponding query split. This performance drives the gradient update on the original (meta) parameters \\(\\theta\\), i.e. loss from the query set is what is used to do the gradient updates on the parameters.</li> <li>Backprop Through Adaptation: The outer update must differentiate through the inner loop (the adaptation step). This is how the support set helps in training - when the loss from the query set is used to do updates of the parameters, it implicitly includes the losses from the adaptation step. This is explained in the last sections.</li> </ol>"},{"location":"blog/2025/01/10/some-notes-from-implementing-model-agnostic-meta-learning/#non-optimized-pytorch-code-for-using-maml-on-any-generic-nnmodule","title":"Non-optimized pytorch code for using MAML on any generic nn.Module","text":"<p>How would you implement this in Pytorch? The author of the original paper (Professor Chelsea Finn) used ConvNets as an example, and did so in TensorFlow V1 which is not readable for mere mortals.</p> <p>The concept though is simple enough, and can be re-written in PyTorch for any generic <code>nn.Module</code> subclass</p> <p>Below is how I'd implement MAML at a high level using PyTorch. Some syntactical liberties have been taken, and this code won't necessarily work if you copy paste it. Even if it did, it is probably going to be slow, but you should be able to work around it by optimizing individual components. </p> <pre><code>import copy\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\ndef init_model_with_params(base_model: nn.Module, params: dict[str, torch.Tensor]):\n    \"\"\"\n    Create a new copy of 'base_model' and load the given 'params' \n    (list of Tensors) into it as its parameters.\n\n    Args:\n        base_model (nn.Module): The model whose architecture we want to replicate.\n        params (List[Tensor]): A dict of parameter tensors in the same order \n            as base_model.named_parameters().\n\n    Returns:\n        nn.Module: A new model instance whose parameters match 'params'.\n    \"\"\"\n    # 1) Deepcopy the structure of base_model to get a fresh model.\n    #    This copy initially has the same weights as base_model.\n    new_model = copy.deepcopy(base_model)\n\n    # 2) Iterate through new_model's parameters and replace with those from 'params'.\n    #    We'll zip over the named_parameters of the new model and the list of adapted params.\n    for (_, p), (_, adapted_p) in zip(new_model.named_parameters(), params.items()):\n        p.data.copy_(adapted_p.data)\n\n    return new_model\n\ndef forward_with_params(base_model: nn.Module, params, x):\n    \"\"\"\n    Generic forward pass using a new set of 'params'.\n    Instantiates a new model, loads 'params' into it, and computes the forward pass.\n    \"\"\"\n    # Rebuild a new model whose parameters match 'params'\n    model_copy = init_model_with_params(base_model, params)\n    # Forward pass\n    return model_copy(x)\n\ndef clone_params(model: nn.Module):\n    \"\"\"Return a dict of parameter clones for the current model parameters.\"\"\"\n    return dict(model.named_parameters())\n\ndef inner_loop(\n    base_model,\n    params,\n    x_train,\n    y_train,\n    loss_fn,\n    lr,\n    num_inner_steps,\n    train: bool,\n):\n    \"\"\"\n    Perform the MAML inner loop adaptation step.\n    \"\"\"\n    # Forward with current 'params'\n    cloned_params = clone_params(model)\n    for _ in range(num_inner_steps):\n        y_pred = forward_with_params(base_model, params, x_train)\n        train_loss = loss_fn(y_pred, y_train)\n\n        # Compute gradients wrt 'params'\n        grads = torch.autograd.grad(train_loss, params, create_graph=train)\n\n        # Gradient descent update\n        # can make lr a dict to pass in different learning rates for different parameters\n        cloned_params = {\n            k: v - lr * grad\n            for (k, v), grad in zip(cloned_params.items(), grads)\n        }\n\n    return cloned_params\n\n# Some random model architecture. Could be anything really\nclass SomeModel(nn.Module):\n\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))\n\ndef main():\n    # 2) Hyperparameters\n    meta_lr = 1e-3\n    inner_lr = 0.1\n    outer_optimizer = optim.Adam(model.parameters(), lr=meta_lr)\n    loss_fn = nn.CrossEntropyLoss()\n\n    # Just an example loop with random data\n    num_meta_updates = 100\n    meta_batch_size = 4\n    num_inner_steps = 5\n\n    for meta_iter in range(num_meta_updates):\n        outer_optimizer.zero_grad()\n\n        meta_loss = 0.0\n        for task_i in range(meta_batch_size):\n            # Sample support and query data for the task\n            x_train = torch.randn(32, 784)\n            y_train = torch.randint(0, 10, (32,))\n            x_val   = torch.randn(32, 784)\n            y_val   = torch.randint(0, 10, (32,))\n\n            # Clone the base model's params\n            fast_weights = clone_params(model)\n\n            # --- Inner Loop: Adaptation ---\n            fast_weights = inner_loop(model, fast_weights, x_train, y_train, loss_fn, inner_lr, num_inner_steps)\n\n            # --- Compute the loss using updated (fast) params on the validation set ---\n            val_pred = forward_with_params(model, fast_weights, x_val)\n            val_loss = loss_fn(val_pred, y_val)\n\n            meta_loss += val_loss\n\n        meta_loss = meta_loss / meta_batch_size\n\n        # Outer loop update (meta update)\n        meta_loss.backward()\n        outer_optimizer.step()\n\n        if meta_iter % 10 == 0:\n            print(f\"[Iteration {meta_iter}] Meta Loss: {meta_loss.item():.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"blog/2025/01/10/some-notes-from-implementing-model-agnostic-meta-learning/#pytorch-shenanigans","title":"PyTorch shenanigans","text":"<p>The code above has a number of features of PyTorch that you may not be familiar with if you're just starting out with PyTorch. I will try and point out the most \"interesting\" features. </p>"},{"location":"blog/2025/01/10/some-notes-from-implementing-model-agnostic-meta-learning/#torchautogradgrad","title":"torch.autograd.grad","text":"<p>Why do we use <code>torch.autograd.grad</code>? Usually, in most PyTorch training loops, we use <code>loss_tensor.backward()</code>, and then do <code>optimizer.step</code>. Let's look at what the <code>.backwards</code> method does</p> <p>When using <code>.backwards</code>, gradients get accumulated when you do multiple backwards over a particular param: <pre><code>x1 = torch.tensor(3., requires_grad=True)\nx2 = torch.tensor(4., requires_grad=True)\nx3 = torch.tensor(5., requires_grad=True)\n\ny1 = 3 * (x1**2) * (x2**2) * (x3 ** 2)\ny2 = 3 * (x1**2) * (x2**2) * (x3 ** 2)\n\nprint(x1.grad)\ny1.backward()\nprint(x1.grad)\ny2.backward()\nprint(x1.grad)\n\n\"\"\"\nNone\ntensor(7200.)\ntensor(14400.)\n\"\"\"\n</code></pre></p> <p>Here you have 2 variables \\(y_1\\) and \\(y_2\\) both of which depend on \\(x_1\\). When you do <code>.backward</code> on each, the gradient on \\(x_1\\) accumulates - first being <code>7200</code> from the first <code>.backward</code>, and then having another <code>7200</code> added from the second <code>.backward</code> for a final value of <code>14400</code>. </p> <p>Now, when you use <code>torch.autograd.grad</code>, the gradients of the tensor do not accumulate:</p> <pre><code>x1 = torch.tensor(3., requires_grad=True)\nx2 = torch.tensor(4., requires_grad=True)\nx3 = torch.tensor(5., requires_grad=True)\n\ny1 = 3 * (x1**2) * (x2**2) * (x3 ** 2)\ny2 = 3 * (x1**2) * (x2**2) * (x3 ** 2)\n\nprint(x1.grad)\nfirst_grads = torch.autograd.grad(y1, [x1, x2, x3])\nprint(x1.grad)\ny2.backward()\nprint(x1.grad)\n\n\"\"\"\nNone\nNone\ntensor(7200.)\n\"\"\"\n</code></pre>"},{"location":"blog/2025/01/10/some-notes-from-implementing-model-agnostic-meta-learning/#run-time-comparisons-of-2-different-ways-of-doing-vanilla-sgd","title":"Run time comparisons of 2 different ways of doing vanilla SGD","text":"<p>Another thing to notice in the MAML code is the fact that in the <code>inner_loop</code>, we are simply doing gradient descent on the inner parameters. In <code>pytorch</code>, there are classes such as <code>torch.optim.SGD</code> that do this for us, and that we use in regular training loops for standard deep learning model training. It is possible to rewrite the entire code of the <code>inner_loop</code> to use these built in classes of optimizers, but is there a cost? </p> <p>Beyond issues of readability - using <code>torch.optim.SGD</code> does the updates of the model parameters implicitly, and is therefore something I do not like - is there a hit we are taking performance-wise when we write the <code>for</code> loops ourselves in the <code>inner_loop</code>?</p> <p>To test this, let's look at the code snippet below, which is basically comparing the runtime of a method that does gradient descent using a python for-loop \"by hand\" in <code>calculate_using_grad</code>, and one that uses <code>torch.optim.SGD</code> in <code>calculate_using_optim</code>. To prove that the final results are the same, there is also a method <code>compare_models_close</code>. </p> <pre><code>import torch\nimport time\nimport torch.nn as nn\nfrom copy import deepcopy\nimport torch.nn.functional as F\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nRANDOM_IMAGE_PATH = \"path/to/some/png_img.png\"\n\ndef load_image(file_path):\n    \"\"\"\n    Returns a Tensor containing image data\n            shape (1, 28, 28)\n    \"\"\"\n    x = imageio.imread(file_path)\n    x = torch.tensor(x, dtype=torch.float32).reshape([1, 28, 28]) / x.max()\n    return 1 - x\n\n\ndef compare_models_close(\n    model1: nn.Module,\n    model2: nn.Module,\n    rtol=1e-3,\n):\n    params1 = model1.state_dict()\n    params2 = model2.state_dict()\n\n    if params1.keys() != params2.keys():\n        print(\"params different!\")\n        print(f\"model1: {params1.keys()}\")\n        print(f\"model2: {params2.keys()}\")\n        return False\n\n    for param_name in params1:\n        if not torch.allclose(\n            params1[param_name],\n            params2[param_name],\n            rtol=rtol,\n        ):\n            print(f\"param_name: {param_name} not equal!\")\n            print(f\"model1: {params1[param_name]}\")\n            print(f\"model2: {params2[param_name]}\")\n            return False\n\n    return True\n\n\nclass OmniglotCNN(nn.Module):\n    def __init__(\n        self,\n        num_classes: int,\n        in_channels: int = 1,\n        num_hidden_channels: int = 32,\n        kernel_size: int = 3,\n        num_conv_layers: int = 4,\n    ):\n        super().__init__()\n        layers = []\n        cur_in_channels = in_channels\n        for _ in range(num_conv_layers):\n            conv = nn.Conv2d(\n                cur_in_channels,\n                num_hidden_channels,\n                kernel_size=kernel_size,\n                padding=\"same\",\n            )\n            nn.init.xavier_uniform_(conv.weight)\n            layers.append(conv)\n            layers.append(nn.BatchNorm2d(num_features=num_hidden_channels))\n            layers.append(nn.ReLU())\n            cur_in_channels = num_hidden_channels\n\n        self.layers = nn.Sequential(*layers)\n        self.linear = nn.Linear(num_hidden_channels, num_classes)\n\n    def forward(self, x: torch.Tensor):\n        x = self.layers(x)\n        x = x.mean(dim=[2, 3])\n        return self.linear(x)\n\n\ndef get_image(\n    image_path: Path = RANDOM_IMAGE_PATH,\n) -&gt; torch.Tensor:\n    return load_image(image_path)\n\n\ndef calculate_using_optim(\n    model: nn.Module,\n    img: torch.Tensor,\n    target: torch.Tensor,\n    num_steps: int = 10,\n    lr: float = 0.4,\n) -&gt; nn.Module:\n    inner_optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n    for _ in range(num_steps):\n        inner_optimizer.zero_grad()\n        out = model(img.unsqueeze(0))\n        loss = F.cross_entropy(out, target.unsqueeze(0))\n        loss.backward()\n        inner_optimizer.step()\n\n    return model\n\n\ndef calculate_using_grad(\n    model: nn.Module,\n    img: torch.Tensor,\n    target: torch.Tensor,\n    num_steps: int = 10,\n    lr: float = 0.4,\n) -&gt; nn.Module:\n    for _ in range(num_steps):\n        out = model(img.unsqueeze(0))\n        loss = F.cross_entropy(out, target.unsqueeze(0))\n        gradients = torch.autograd.grad(loss, model.parameters())\n\n        with torch.no_grad():\n            for param, grad in zip(model.parameters(), gradients):\n                param.data -= grad * lr\n\n    return model\n\n\ndef main():\n    torch.manual_seed(123)\n    num_classes = 5\n    model = OmniglotCNN(num_classes=num_classes)\n    model_copy = deepcopy(model)\n    lr = 0.4\n    img = get_image()\n    target = torch.zeros(5, dtype=float)\n    target[0] = 1.0\n    num_experiments = 100\n\n    start = time.time()\n    for _ in tqdm(range(num_experiments)):\n        calculate_using_optim(\n            model=model,\n            img=img,\n            target=target,\n            lr=lr,\n        )\n    end = time.time()\n    duration = end - start\n    print(f\"model using optim: {duration}\")\n    start = time.time()\n    for _ in tqdm(range(num_experiments)):\n        calculate_using_grad(\n            model=model_copy,\n            img=img,\n            target=target,\n            lr=lr,\n        )\n    end = time.time()\n    duration = end - start\n    print(f\"model using grad: {duration}\")\n    assert compare_models_close(model, model_copy)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>The output would be the following: <pre><code>model using optim: 3.6533949375152586\nmodel using grad: 1.864612102508545\n</code></pre></p> <p>i.e. writing your own <code>for</code> loop is faster than using the in-built <code>torch.optim.SGD</code>! </p> <p>This was counterintuitive to me because one would assume that the in-built PyTorch class like would do gradient descent faster than a method that uses raw Python for-loops in its implementation. </p> <p>But this makes sense the moment you look at how <code>torch.optim.SGD</code> is implemented - for example, there is a lot of movement of data to/from a <code>buffer</code>, and a lot of cloning of tensors. I will not cover the details of how <code>torch.optim.SGD</code> is implemented, and would recommend interested readers to go take a look yourself to satisfy yourself that it is indeed quite slow.</p>"},{"location":"blog/2025/01/10/some-notes-from-implementing-model-agnostic-meta-learning/#clones","title":"Clones","text":"<p>You will notice that in the <code>inner_loop</code>, the adapted parameters get initialised by cloning the parameters of the meta model. What does this do to the gradients? To investigate this let's take a look at the following code:</p> <pre><code>x = torch.tensor(3., requires_grad=True)\nsgd_optim = torch.optim.SGD(params=[x], lr=0.4)\nsgd_optim.zero_grad()\nx_copy = torch.clone(x)\nx_copy = x_copy - 2.\ny = 2*(x_copy**4)\n# define loss arbitrarily, just say 1-y for now\nloss = 1. - y\nloss.backward()\nsgd_optim.step()\nprint(x)\nprint(x_copy)\n\n\"\"\"\ntensor(6.2000, requires_grad=True)\ntensor(1., grad_fn=&lt;SubBackward0&gt;)\n\"\"\"\n</code></pre> <ol> <li>First, x = torch.tensor(3., requires_grad=True) creates our starting tensor with value 3.0. We turn on gradient tracking because we'll need to optimize this value.</li> <li><code>sgd_optim = torch.optim.SGD(params=[x], lr=0.4)</code> sets up the SGD optimizer with a learning rate of 0.4. This optimizer will only update the tensor x as it in the only tensor \"registered\" to the optimizer</li> <li>sgd_optim.zero_grad() clears any existing gradients, giving us a clean slate.</li> <li>Now comes the interesting part. x_copy = torch.clone(x) creates a copy of x with value 3.0. Then x_copy = x_copy - 2. subtracts 2 from x_copy, making x_copy = 1.0. This subtraction maintains the computational graph connection to x.</li> <li>y = 2(x_copy*4) computes y = 2 * (1.0^4) = 2 * 1 = 2</li> <li>loss = 1. - y computes loss = 1 - 2 = -1</li> <li>When loss.backward() computes gradients, it flows like this:<ol> <li>d(loss)/dy = -1</li> <li>d(y)/d(x_copy) = 8 * (x_copy^3) = 8 * 1 = 8</li> <li>d(x_copy)/dx = 1 (from the clone and subtract operations)</li> <li>Therefore, d(loss)/dx = -1 * 8 * 1 = -8</li> </ol> </li> <li>sgd_optim.step() updates x using the gradient and learning rate:<ol> <li>new_x = x - lr * gradient</li> <li>new_x = 3.0 - 0.4 * (-8) = 3.0 + 3.2 = 6.2</li> </ol> </li> </ol> <p>This is why the print statements will show: 1. x = 6.2 (the updated value after optimization) 2. x_copy = 1.0 (the original copied value minus 2)</p> <p>The key insight here is that even though we modified x_copy with subtraction, the computational graph still maintains the connection back to x. However, the gradient that flows back to x is different because x_copy now represents a different value (1.0 instead of 3.0) when computing y. This leads to a smaller gradient (-8 instead of -216 if you did not have the <code>x_copy = x_copy - 2.</code> line) and thus a smaller update to x. This example nicely illustrates how PyTorch's autograd system can handle multiple operations while maintaining gradient flow through the computational graph. </p> <p>Let's look at a more complicated example: <pre><code>x = torch.tensor(3., requires_grad=True)\nlr = 0.4\nouter_optim = torch.optim.SGD(params=[x], lr=lr)\n# simulate inner loop\nx_inner = torch.clone(x)\n# define y_inner arbitrarily\ny_inner = 2*(x_inner**4) # 2 * (3^4) = 162\n# define inner loss arbitrarily\ninner_loss = 2. - y_inner # 2 - 162 = -160\ngradients = torch.autograd.grad(inner_loss, x_inner) # diff of 2 - 2x^4 =&gt; -8x^3 @ (x = 3) =&gt; -216\nprint(f\"num of inner grads: {len(gradients)}\")\n# arbitrary update - NOT what happens in MAML\nx_inner = 2 * x_inner - lr * gradients[0] # 6 - 0.4*(-216) = 92.4\n\n# now simulate the outer loop gradient update\n# define y_outer arbitrarily\ny_outer = 3 * (x_inner ** 2)  # 3 * (92.4 ^ 2) = 25613.28\n# define outer loss arbitrarily\nouter_loss = 1. - y_outer # 1 - 25613.28 = -25612.28\nprint(f\"outer_loss: {outer_loss}\")\nouter_loss.backward()\nouter_optim.step()\n\nprint(f\"x_inner: {x_inner}, x_inner.grad: {x_inner.grad}\")\nprint(f\"x: {x}, x.grad: {x.grad}\")\n</code></pre></p> <p>The final print statements will have the following output:</p> <pre><code>num of inner grads: 1\nouter_loss: -25612.28125\nx_inner: 92.4000015258789, x_inner.grad: None\nx: 446.52001953125, x.grad: -1108.800048828125\n</code></pre> <p>To see why the final values of <code>x</code> and <code>x.grad</code> are what they are, let's work out what <code>x.grad</code> will be, and then we'll see what <code>x</code> is.</p> <p>The final <code>outer_loss.backward()</code> will basically accumulate the gradient on <code>x</code> such that <code>x.grad</code> will be <code>d(outer_loss)/d(x)</code>. Using the chain rule, you basically have </p> <p><pre><code>d(outer_loss)/d(x) = [d(outer_loss) / d(x_inner)] * [d(x_inner) / d(x)]\n</code></pre> The first term <code>[d(outer_loss) / d(x_inner)]</code> will basically be <code>d(1 - 3*x_inner^2)/d(x_inner) = -6*x_inner</code> @ <code>x_inner = 92.4</code>, so it will be <code>-554.4</code>.</p> <p>The second term is 2. This comes from the line </p> <p><pre><code>x_inner = 2 * x_inner - lr * gradients[0]\n</code></pre> where we overwrite the value of <code>x_inner</code> (originally just a clone of x) with a new update. We ignore the contribution of <code>lr * gradients[0]</code> to the <code>[d(x_inner) / d(x)]</code> derivative! The reason for this is PyTorch has not stored the computation graph from <code>gradients[0]</code> to <code>x</code>. </p> <p>The key reason for this is that  <code>create_graph=False</code> by default in the inner gradient update. This is not what the MAML algorithm is! What happens if you do set that parameter to <code>true</code>?</p>"},{"location":"blog/2025/01/10/some-notes-from-implementing-model-agnostic-meta-learning/#create_graphtrue","title":"create_graph=True","text":"<p>You will also notice that when doing <code>torch.autograd.grad</code> in the MAML algorithm in the <code>inner_loop</code>, we set <code>create_graph=True</code>. How does this affect anything?</p> <p>Let's use the exact same code as the more complicated case above, with the only difference being setting <code>create_graph = True</code> in the <code>torch.autograd.grad</code> line:</p> <pre><code>x = torch.tensor(3., requires_grad=True)\nlr = 0.4\nouter_optim = torch.optim.SGD(params=[x], lr=lr)\n# simulate inner loop\nx_inner = torch.clone(x)\n# define y_inner arbitrarily\ny_inner = 2*(x_inner**4) # 2 * (3^4) = 162\n# define inner loss arbitrarily\ninner_loss = 2. - y_inner # 2 - 162 = -160\ngradients = torch.autograd.grad(inner_loss, x_inner, create_graph=True) # diff of 2 - 2x^4 =&gt; -8x^3 @ (x = 3) =&gt; -216\nprint(f\"num of inner grads: {len(gradients)}\")\n# arbitrary update - NOT what happens in MAML\nx_inner = 2 * x_inner - lr * gradients[0] # 6 - 0.4*(-216) = 92.4\n\n# now simulate the outer loop gradient update\n# define y_outer arbitrarily\ny_outer = 3 * (x_inner ** 2)  # 3 * (92.4 ^ 2) = 25613.28\n# define outer loss arbitrarily\nouter_loss = 1. - y_outer # 1 - 25613.28 = -25612.28\nprint(f\"outer_loss: {outer_loss}\")\nouter_loss.backward()\nouter_optim.step()\n\nprint(f\"x_inner: {x_inner}, x_inner.grad: {x_inner.grad}\")\nprint(f\"x: {x}, x.grad: {x.grad}\")\n</code></pre> <p>The final output is: <pre><code>num of inner grads: 1\nouter_loss: -25612.28125\nx_inner: 92.4000015258789, x_inner.grad: None\nx: 19606.5859375, x.grad: -49008.96484375\n</code></pre></p> <p>You'll notice that while x_inner is the exact same as the previous output, the values for <code>x.grad</code> and <code>x</code> are considerably different! How did we get to <code>x.grad = -49008.96484375</code>?</p> <p>The machinations are the exact same as that above, where <code>x.grad = d(outer_loss)/d(x)</code>, and </p> <p><pre><code>d(outer_loss)/d(x) = [d(outer_loss) / d(x_inner)] * [d(x_inner) / d(x)]\n</code></pre> The difference now when <code>create_graph=True</code> is that PyTorch keeps the graph pointing from <code>x_inner</code> to <code>x</code>! So <code>[d(x_inner) / d(x)]</code> is no longer 1. </p> <p>What is it instead? You'll notice that we have this line:</p> <p><pre><code>x_inner = 2 * x_inner - lr * grad[0]\n</code></pre> where we are overwriting the value of <code>x_inner</code>. The original <code>x_inner</code> was the clone of <code>x</code>, and <code>grad[0]</code> was <code>-8x^3</code> (basically <code>d(inner_loss)/d(x_inner) = 2 * d(inner_loss)/d(x)</code> since when <code>grad[0]</code> was computed, <code>d(x_inner) / d(x) = 1</code> since they are still just clones of each other until we overwrite <code>x_inner</code>). So what you basically have is </p> <p><pre><code>x_inner = 2x - lr * (-8x^3) = 2x + 8*lr*x^3\n==&gt; \nd(x_inner)/d(x) = 2 + 24*lr*x^2\n</code></pre> and this is all done when x = 3, so </p> <pre><code>d(x_inner)/d(x) = 2 + 24*lr*x^2\n= 2 + 24*0.4*(3^2)\n= 88.4\n</code></pre> <p>So now, you get</p> <pre><code>d(outer_loss)/d(x) = [d(outer_loss) / d(x_inner)] * [d(x_inner) / d(x)]\n\n= -554.4           *    88.4\n   ^\n   |\nCalculation from\nprevious section\n\n= -49008.96\n</code></pre> <p>which is the value of <code>x.grad</code>!</p> <p>The value of <code>x</code> is as per usual <code>x = 3 - 0.4 * (-49008.96) = 19606.584</code></p> <p>This is what the MAML update is about. This is what we mean by  <pre><code>The outer update must differentiate through the inner loop (the adaptation step).\n</code></pre> right at the top of the article!</p>"},{"location":"blog/2025/01/10/some-notes-from-implementing-model-agnostic-meta-learning/#visualisation-using-torchviz","title":"Visualisation using torchviz","text":"<p>Using the torchviz library, you can very easily visualize the computational graphs that PyTorch creates and how they differ when you do <code>create_graph=True</code>.</p> <p>The code to visualise our above examples are as simple as the following lines:</p> <pre><code>from torchviz import make_dot\n\n# after defining outer_loss:\ndot_ = make_dot(outer_loss, params={'x': x})\ndot_.render(\"graph1\", format=\"png\")\n</code></pre> <p>Graph for when <code>create_graph=False</code></p> <p></p> <p>Graph for when <code>create_graph=True</code></p> <p></p> <p>You can see that there is an extra branch coming out of the <code>CloneBackward0</code> node, representing PyTorch keeping track of the computation graph from the inner gradients to the original tensor <code>x</code>. </p>"},{"location":"blog/2024/01/04/manacher-algo/","title":"Manacher's Algorithm","text":"<p>Super annoying algorithm, but it has uses in bioinfomatics. </p> <p>Here's the task: Given a string\u00a0<code>s</code>, return\u00a0the longest palindromic substring \u00a0in\u00a0<code>s</code>.</p> <p>Example inputs and outputs:</p> <p>Input: s = \"babad\" Output: \"bab\" Explanation: \"aba\" is also a valid answer.</p> <p>Input: s = \"cbbd\" Output: \"bb\"</p> <p>To build some intuition for how this algorithm works, let's see an example of a brute force implementation. In this implementation, given a string <code>_str</code>, we want to see for any <code>i</code>, what is the maximum length of a palindrome centred around the character <code>_str[i]</code>. Something simple would be something like this: </p> <pre><code>def manacher_brute_force(_str: str) -&gt; list:\n  _str_len = len(_str)\n  out = [0 for _ in range(_str_len + 2)]\n  # make sure first and last chars are different and do not\n  #\u00a0happen in _str itself\n  _str = f\"${_str}^\"\n\n  for i in range(1, _str_len + 1):\n\n    while _str[i - out[i]] == _str[i + out[i]]:\n      out[i] += 1\n  return out\n\nif __name__ == \"__main__\":\n  print(manacher_brute_force(\"abbcbba\"))\n</code></pre> <p>the output will be something like <code>[0, 1, 1, 1, 4, 1, 1, 1, 0]</code></p> <p>But in this approach, when we sweep from left to right, we are not using the work we have already done to the left of <code>i</code>. This is where Manacher comes in. </p>","tags":["algorithms"]},{"location":"blog/2024/01/04/manacher-algo/#what-dont-we-have-to-search","title":"What don't we have to search?","text":"<p>We can exploit the nature of a palindrome - assume we have a palindrome of length <code>l</code> centred around index <code>i</code>, and say we take 2 indexes <code>i'</code> and <code>i''</code> that are distance <code>d</code> left and right to <code>i</code> respectively s.t. <code>d &lt; l</code>, then we basically know that any palindrome that is centred around <code>i'</code> will also likely be centred around <code>i''</code>!</p> <p>From this simple observation, we can already amend the inner <code>while</code> loop so that we are not searching all of the characters to the left and right of a particular index <code>i''</code> of an index that is to the right of an index <code>i</code> we have already done work for.  </p> <p>The rest of the complexity comes from the need to handle the case where the borders of the inner palindrome reaches the border of the outer palindrome. All you have to do there is make sure you always check whenever you go beyond the borders of the longest current palindrome. </p> <pre><code>def manacher_odd(_str: str) -&gt; list:\n    _str_len = len(_str)\n    out = [0 for _ in range(_str_len + 2)]\n    # make sure first and last chars are different and do not\n    #\u00a0happen in _str itself\n    _str = f\"${_str}^\"\n    l, r = 1, 1\n    for i in range(1, _str_len + 1):\n        dist_to_border = r - i\n        inner_palindrome_len = min(dist_to_border, out[l + dist_to_border])\n        out[i] = max(0, inner_palindrome_len)\n        while _str[i - out[i]] == _str[i + out[i]]:\n          out[i] += 1\n        if i + out[i] &gt; r:\n            l = i - out[i]\n            r = i + out[i]\n    return out\n</code></pre> <p>Code is a translation of the cpp code here</p> <p>NOTE:</p> <p>The above algorithm is only for odd length. In practice, you can make any string odd length by doing something like </p> <pre><code>_str = \"\u00a3\".join(_str)\n_str = f\"#{_str}^\"\n</code></pre> <p>the <code>.join</code> adds <code>n-1</code> characters to the string, so the total number of characters will be odd, since <code>even + odd = odd</code></p>","tags":["algorithms"]},{"location":"blog/2024/01/04/manacher-algo/#where-does-the-time-saving-come-from-that-makes-it-linear","title":"Where does the time saving come from that makes it linear?","text":"<p>In the brute force method, consider the number of times a character at index <code>i</code> is compared to some other character. You will quickly realise it is <code>O(n)</code>, and that is where the overall \\(O(n^2)\\) complexity for the brute force method comes from. </p> <p>But with Manacher's algorithm, the while loop is no longer independent of the outer for loop. The outer loop is keeping track of the <code>centre</code> of palindromes and is always increasing. We only do additional comparison operations when <code>r</code> variable (i.e. the rightmost boundary, the <code>centre + radius</code> value) increases - and this quantity never decreases in value! Therefore, the total number of operations in the outer and in the inner loop adds to <code>n</code>. </p>","tags":["algorithms"]},{"location":"blog/2024/01/04/manacher-algo/#final","title":"Final:","text":"<pre><code>def manacher(s: str) -&gt; str:\n        if len(s) &lt;= 1:\n            return s\n\n        s = f\"#{'#'.join(s)}#\"\n        s_len = len(s)\n        out = [0 for _ in range(s_len)]\n        max_radius = 1\n        max_str = s[1]\n\n        l, r = 1, 1\n        for i in range(1, s_len - 1):\n            dist_to_edge = r - i\n            allowable_dist = min(dist_to_edge, out[l + dist_to_edge])\n            out[i] = max(0, allowable_dist)\n            while i - out[i]&gt;= 0 and i + out[i] &lt; s_len and s[i - out[i]] == s[i + out[i]]:\n                out[i] += 1\n            if i + out[i] &gt; r:\n                r = i + out[i]\n                l = i - out[i]\n            if out[i] &gt; max_radius:\n                max_radius = out[i]\n                max_str = s[l + 1 : r].replace(\"#\", \"\") # if you want s[l: r + 1] you need to offset in the while loop like so: \n                # while i - out[i] - 1&gt;= 0 and i + out[i] + 1 &lt; s_len and s[i - out[i] - 1] == s[i + out[i] + 1]\n        return max_str\n</code></pre> <p>or if you want to use the original <code>manacher_odd</code> kind of notation:</p> <pre><code>    def manacher(s: str) -&gt; str:\n        if len(s) &lt;= 1:\n            return s\n\n        s = f\"\u00a3#{'#'.join(s)}#^\"\n        s_len = len(s)\n        out = [0 for _ in range(s_len)]\n        max_radius = 1\n        max_str = s[1]\n\n        l, r = 1, 1\n        for i in range(1, s_len - 1):\n            dist_to_edge = r - i\n            allowable_dist = min(dist_to_edge, out[l + dist_to_edge])\n            out[i] = max(0, allowable_dist)\n            while s[i - out[i]] == s[i + out[i]]:\n                out[i] += 1\n            if i + out[i] &gt; r:\n                r = i + out[i]\n                l = i - out[i]\n            if out[i] &gt; max_radius:\n                max_radius = out[i]\n                max_str = s[l + 1 : r].replace(\"#\", \"\")\n        return max_str\n</code></pre>","tags":["algorithms"]},{"location":"blog/2024/08/09/various-observations-on-multiprocessing-in-python/","title":"Various observations on multiprocessing in Python","text":"<p>Here are a number of observations about various \"interesting\" behaviours in Python I have collected over the years:</p>"},{"location":"blog/2024/08/09/various-observations-on-multiprocessing-in-python/#forking-vs-spawning","title":"Forking vs Spawning","text":"<p>You'll get a different output for the following lines of code on a MacOS vs on a Linux</p> <pre><code>import pickle\nimport multiprocessing\n\nclass Container:\n    def __init__(self):\n        self.data = list(range(100))\n\n    def __getstate__(self):\n        print(\"Pickling Container\")\n        return self.__dict__\n\n    def __setstate__(self, state):\n        print(\"Unpickling Container\")\n        self.__dict__ = state\n\nclass ProcessingEg:\n    def __init__(self):\n        self.container = Container()\n\n    def __getstate__(self):\n        print(\"Pickling ProcessingEg\")\n        return self.__dict__\n\n    def __setstate__(self, state):\n        print(\"Unpickling ProcessingEg\")\n        self.__dict__ = state\n\n    def method(self):\n        print(f\"Method called, container data: {self.container.data[:5]}...\")\n\ndef worker(func):\n    print(\"Worker process started\")\n    func()\n\nif __name__ == \"__main__\":\n    pe = ProcessingEg()\n\n    print(\"Pickling process starting\")\n    pickled_method = pickle.dumps(pe.method)\n    print(f\"Pickled size: {len(pickled_method)} bytes\")\n\n    print(\"\\nUnpickling process starting\")\n    unpickled_method = pickle.loads(pickled_method)\n    print(f\"Unpickled type: {type(unpickled_method)}\")\n\n    print(\"\\nStarting multiprocessing\")\n    p = multiprocessing.Process(target=worker, args=(unpickled_method,))\n    p.start()\n    p.join()\n</code></pre> <p>in macos, output is: <pre><code>Pickling process starting\nPickling ProcessingEg\nPickling Container\nPickled size: 329 bytes\n\nUnpickling process starting\nUnpickling Container\nUnpickling ProcessingEg\nUnpickled type: &lt;class 'method'&gt;\n\nStarting multiprocessing\nPickling ProcessingEg\nPickling Container\nUnpickling Container\nUnpickling ProcessingEg\nWorker process started\nMethod called, container data: [0, 1, 2, 3, 4]...\n</code></pre></p> <p>while on linux, this is <pre><code>Pickling process starting\nPickling ProcessingEg\nPickling Container\nPickled size: 329 bytes\n\nUnpickling process starting\nUnpickling Container\nUnpickling ProcessingEg\nUnpickled type: &lt;class 'method'&gt;\n\nStarting multiprocessing\nWorker process started\nMethod called, container data: [0, 1, 2, 3, 4]...\n</code></pre></p> <p>In Python's multiprocessing:</p> <ol> <li>macOS uses spawn by default (since Python 3.8)</li> <li>Linux/Unix uses fork by default</li> </ol> <p>The difference in output comes from this difference - how Python\u2019s multiprocessing module creates new processes on different operating systems.</p>"},{"location":"blog/2024/08/09/various-observations-on-multiprocessing-in-python/#macos-spawn","title":"macOS (spawn)","text":"<p>The default start method is spawn. The spawn start method does not copy the parent\u2019s memory. </p> <p>Instead, it starts a fresh Python interpreter process and then passes necessary objects from the parent to the child by pickling them and sending them over some form of inter-process communication (like named pipes, message queues etc.).</p> <p>This means that every object the child process needs\u2014 functions, classes, instances \u2014 must be pickled in the parent and unpickled in the child. As a result, there are multiple print statements from the <code>__getstate__</code> (pickling - when <code>dumps</code> is called) and <code>__setstate__</code> (unpickling - when <code>loads</code> is called) methods.</p>"},{"location":"blog/2024/08/09/various-observations-on-multiprocessing-in-python/#linux-fork","title":"Linux (fork)","text":"<p>When a new process is created using fork, it duplicates the parent process's memory space, including all Python objects. Because the child process starts as a near-identical copy of the parent, objects do not need to be re-serialized (pickled) and then deserialized (unpickled) to be available in the child. This results in fewer calls to <code>__getstate__</code> and <code>__setstate__</code>.</p>"},{"location":"blog/2024/08/09/various-observations-on-multiprocessing-in-python/#difference-in-output","title":"Difference in output","text":"<p>Taking a look at the print statements again:</p> <pre><code>Starting multiprocessing\n\n# &lt;--The following lines appear in spawn, but not in fork--&gt;\nPickling ProcessingEg\nPickling Container\nUnpickling Container\nUnpickling ProcessingEg\n# &lt;----&gt;\n\nWorker process started\n</code></pre> <p>The additional print statements that happen when <code>spawn</code> gets used in macOS regarding pickling/unpickling basically comes from the additional calls to <code>__getstate__</code> and <code>__setstate__</code> that come from <code>pickle.dumps</code> and <code>pickle.loads</code> calling them respectively.</p>"},{"location":"blog/2024/08/09/various-observations-on-multiprocessing-in-python/#different-ways-of-deleting-a-key-from-a-dictionary","title":"Different ways of deleting a key from a dictionary","text":"<p>Look at the difference in the time it takes 2 different methods to remove an element from a dictionary in Python:</p> <p><pre><code>import time\n\nnum = 10**6\ntest_dict = {i: i for i in range(num)}\nstart = time.time()\nfor i in range(num):\n    test_dict.pop(i)\nend = time.time()\npop_time = end - start\nprint(pop_time)\ntest_dict = {i: i for i in range(num)}\nstart = time.time()\nfor i in range(num):\n    del test_dict[i]\nend = time.time()\ndel_time = end - start\nprint(del_time)\n</code></pre> The output is </p> <p><pre><code>0.06211280822753906\n0.060539960861206055\n</code></pre> And the ratio is <pre><code>((pop_time - del_time) / pop_time) * 100\n# 2.5322432058959006\n</code></pre> Let's do a slightly more rigorous statistical analysis of this difference. Here's the code for it:</p> <pre><code>import time\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm\n\nnum = 10**6\nnum_experiments = 10**3\npop_times = []\nfor _ in tqdm(range(num_experiments)):\n    test_dict = {i: i for i in range(num)}\n    start = time.time()\n    for i in range(num):\n        test_dict.pop(i)\n    end = time.time()\n    pop_time = end - start\n    pop_times.append(pop_time)\n\ndel_times = []\nfor _ in tqdm(range(num_experiments)):\n    test_dict = {i: i for i in range(num)}\n    start = time.time()\n    for i in range(num):\n\n        del test_dict[i]\n\n    end = time.time()\n    del_time = end - start\n    del_times.append(del_time)\n\n# First, create a figure and axis objects explicitly\nplt.figure(figsize=(12, 8))\nax = plt.gca()\n\npop_mean = np.mean(pop_times)\npop_var = np.var(pop_times)\ndel_mean = np.mean(del_times)\ndel_var = np.var(del_times)\n\n# Plot individual points\n# scatter\n# plt.scatter(range(len(pop_times)), pop_times, alpha=0.5, label='pop()', s=5)\n# plt.scatter(range(len(del_times)), del_times, alpha=0.5, label='del', s=5)\n# histogram\nplt.hist(pop_times, bins=100, alpha=0.5, label='pop()', density=True)\nplt.hist(del_times, bins=100, alpha=0.5, label='del', density=True)\n\n# Add horizontal lines for means\n# scatter\n# plt.axhline(pop_mean, color='blue', linestyle='dashed', linewidth=2, label=f'pop() mean: {pop_mean:.6f}')\n# plt.axhline(del_mean, color='orange', linestyle='dashed', linewidth=2, label=f'del mean: {del_mean:.6f}')\n# histogram\n# Add vertical lines for means\nplt.axvline(pop_mean, color='blue', linestyle='dashed', linewidth=2, label=f'pop() mean: {pop_mean:.6f}')\nplt.axvline(del_mean, color='orange', linestyle='dashed', linewidth=2, label=f'del mean: {del_mean:.6f}')\n\n# Place legend outside to the right\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n\n# Add text annotations for variances below the legend\nplt.text(1.05, 0.6, f'pop() variance: {pop_var:.6f}', transform=ax.transAxes)\nplt.text(1.05, 0.55, f'del variance: {del_var:.6f}', transform=ax.transAxes)\n\nplt.title('Distribution of Dictionary Deletion Times')\n# scatter\n# plt.xlabel('Experiment Number')\n# plt.ylabel('Time (seconds)')\n# histogram\nplt.xlabel('Time (seconds)')\nplt.ylabel('Density')\nplt.grid(True, alpha=0.3)\n\n# Adjust layout to make room for legend and text\nplt.tight_layout()\nplt.subplots_adjust(right=0.85)  # This makes room for the legend and text on the right\n\nplt.show()\n</code></pre> <p></p> <p>Seems statistically significant - let's just confirm:</p> <p><pre><code>from scipy import stats\n\nt_stat, p_value = stats.ttest_rel(pop_times, del_times)\nprint(p_value) # &lt;- 1.1523452235190812e-37\n</code></pre> i.e. there is a probability of 1.1523452235190812e-37 of this deviation happening at random. So we can conclude that this roughly ~2.5% difference is actually statistically significant!</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/algorithms/","title":"algorithms","text":""}]}