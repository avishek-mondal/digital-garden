{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"blog/","title":"Hello!","text":"<p>I post random things that are tangentially related to data structures and algorithms, deep learning, and other machine learning related topics.</p> <p>If you want more information about who I am, feel free to look at my website.</p>"},{"location":"blog/2024/01/05/self-attention/","title":"How does self attention work?","text":"<p>Self-attention is the cornerstone of a transformer block. We are going to go through the intuition behind the following steps:</p> <ol> <li>Tokenization</li> <li>Embedding</li> <li>The self attention mechanism</li> </ol> <p>by using raw, non-vectorised Python code as much as possible.</p> <p>The original transformer paper here is actually easy to read for a technical paper, but difficult to wrap your head around if this is the first time you are dipping your toes into neural networks. Other resources, like \"The Illustrated Transformer\" don't really have code. One resource that does have code is the \"Understanding Deep Learning\" (UDL) book, that I will be leaning on heavily for this post.</p> <p>Before we get to the famous equation -</p> \\[ \\text{selfAttention}[X] = V[X]\\text{softmax}(\\frac{K^TQ}{\\sqrt{D_q}}) \\] <p>let's see what the steps often are before a matrix/tensor gets to the self-attention mechanism. </p> <p>Many images are from the UDL book, and a lot of the code is covered in the notebooks  published alongside the book. </p> <p>Before diving into architectural details, we have to remind ourselves what the problem it is that transformers had set out to solve: translation </p>"},{"location":"blog/2024/01/05/self-attention/#brief-introduction-to-the-problem-of-translation","title":"Brief introduction to the problem of translation","text":"<p>Translation is the \"flagship\" natural language processing task. The general inputs and outputs of the problem are incredibly simple to describe - given a sequence of words in one language, return a sequence of words in another language automatically. </p> <pre><code>Input: I am a student.\n\nOutput: \u0986\u09ae\u09bf \u098f\u0995\u099c\u09a8 \u099b\u09be\u09a4\u09cd\u09b0\u0964\n</code></pre> <p>Before deep learning methods were used to tackle this problem, companies like Google used statistical, phrase-based systems that did not work that well, and did not scale to many languages.</p> <p>Nowadays, all dominant translation systems use neural networks to do their translation. </p>"},{"location":"blog/2024/01/05/self-attention/#how-do-you-train-a-neural-machine-translation-system","title":"How do you train a  neural machine translation system?","text":"<p>There are many approaches to neural machine translation, but what I'll cover here is sequence to sequence way the \"Attention is all you need\" paper did it.</p> <p>We'll walk through the following broad sections: 1. Tokenization 2. Embedding 3. Self attention</p>"},{"location":"blog/2024/01/05/self-attention/#tokenization-brief-remarks","title":"Tokenization - brief remarks","text":"<p>To use any neural based system, inputs and outputs must be vectors of floats.  The process of converting words/sequence of words into vectors of floats is called tokenization. Tokenization won't be covered in detail here, and there are several resources that go over the different techniques (my personal favourite is the series of lectures from Stanford). The main thing to remember for tokenization is the input and the output - (example is from HuggingFace): </p> <pre><code>from transformers import BertTokenizer\n\ninp = \"I love gpus!\"\n\ntokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n\ninp_str = \"I have a new GPU!\"\ntokenizer.tokenize(inp_str)\n\n# ['i', 'have', 'a', 'new', 'gp', '##u', '!'] =&gt; len= 7\n\n# Huggingface's tokenizers' .encode() method tends to add a special start and end tokens\n\ntokenizer.encode(inp_str)\n# each number here represents the index of each token in the tokenizer's entire vocabulary\n# [101, 1045, 2031, 1037, 2047, 14246, 2226, 999, 102] =&gt; len= 9 (7+2)\n\ntokenizer.decode([101, 1045, 2031, 1037, 2047, 14246, 2226, 999, 102])\n\n# '[CLS] i have a new gpu! [SEP]'\n\"\"\"\nCLS -&gt; Special CLASSIFY token usually added to the beginning of a sequence\nIn HF tokenizers, this is usually 101\n\nSEP -&gt; separator token, usually added at the end of a sequence\nIn HF tokenizers, this is usually 102\n\"\"\"\n</code></pre> <p>There are several popular tokenization schemes: the original Attention paper doesn't actually describe what tokenizing scheme it used, but it is commonly believed they used byte pair encoding (BPE). The GPT family of models also use some variation of BPE, see for example tiktoken, which does BPE at a byte level. Doing BPE at a byte level (as opposed to the more classic Unicode word/character level tokenization) means you don't really have the \"out of vocabulary\" problem when you move from one modality to another.</p> <p>Other popular tokenization methods are WordPiece (which is used in the BERT paper), and SentencePiece which can be used to tokenize multiple languages including non-Latin languages (which Google's T5 uses).</p> <p>Depending on your use case, you can also implement your own tokenization scheme! Before the transformers paper for example, a briefly popular paper was the ByteNet paper that did precisely this and implemented a character-level tokenization scheme. </p>"},{"location":"blog/2024/01/05/self-attention/#embedding","title":"Embedding","text":"<p>What happens after tokenziation? In the Huggingface transformer's library, after tokenisation, you get a sequence of \\(N\\) tokens, each of which is an integer that represents the index in the tokeniser's vocabulary. </p> <p>What we then want to do is convert each of these integers into a vector \\(\\in \\mathbb{R}^D\\) where \\(D\\) is some embedding dimension that is smaller than the size of the vocabulary in the tokenizer. This means that the sequence of integers would be converted into a sequence of vectors, i.e. a matrix.</p> <p>The way this is done in most architectures is through a simple look up table. If the size of the tokenizer's vocabulary is \\(N_{\\text{vocab}}\\) all we need is a \\(N_{\\text{vocab}} \\times D\\) matrix, where row \\(i\\) corresponds to the \\(D\\) dimensional representation of token index \\(i\\). This \\(N_{\\text{vocab}} \\times D\\) matrix is learnable. </p> <p>Let's see how it happens in code:</p> <p>Let's have a look at the inputs and outputs of <code>torch</code>'s <code>nn.Embedding</code> : <pre><code>import torch.nn as nn\nimport torch\nembedding_dim = 3\nvocab_size = 5\nembedding_layer = nn.Embedding(vocab_size, embedding_dim)\n\n# toy example - how would a sequence of ones [1., 1., ...] be embedded?\nout = embedding_layer(torch.ones((6,), dtype=int))\nout\n</code></pre> prints the following: <pre><code>tensor([\n        [-0.8257, 0.0528, 1.3637],\n        [-0.8257, 0.0528, 1.3637],\n        [-0.8257, 0.0528, 1.3637],\n        [-0.8257, 0.0528, 1.3637],\n        [-0.8257, 0.0528, 1.3637],\n        [-0.8257, 0.0528, 1.3637]],\n        grad_fn=&lt;EmbeddingBackward0&gt;)\n</code></pre> a 6x1 matrix. If you want to dig further into the nature of the embedding layer, you can see the following:</p> <pre><code>embedding_layer.weight\n\ntensor([\n    [ 0.2668, -1.0410, -1.5245],\n    [-0.8257, 0.0528, 1.3637],\n    [-1.7534, -0.4505, -1.0951],\n    [-0.6984, -1.7775, -1.3832],\n    [ 2.5235, -0.7539, -2.1454]], requires_grad=True)\n</code></pre> <p>The output of  <code>embedding_layer(torch.ones((6,), dtype=int))</code> is basically the 2nd row (index of 1) <code>[-0.8257, 0.0528, 1.3637]</code> repeated 6 times. So all the embedding layer \\(N_{\\text{vocab}} \\times D\\) does is act as a look-up matrix.</p> <p>In pure python, the code for the embedding layer will be something as simple as:</p> <pre><code>import random \n\nclass PurePythonEmbeddingLayer:\n    def __init__(self, vocab_size: int, embedding_dim: int):\n        self.weight = [[random.random() for _ in range(embedding_dim)] for _ in range(vocab_size)]  # &lt;- the trainable lookup!\n\n    def forward(self, inputs: list[int]):\n        out = []\n        for inp in inputs:\n            out.append(self.weight[inp])\n        return out\n    ...\n</code></pre> <p>From this simple building block, you can make things as complicated as you want. The <code>BertEmbeddings</code> class in the <code>transformers</code> library shows you that for transformers, you will need to encode the position information of the text as well among other things. </p>"},{"location":"blog/2024/01/05/self-attention/#positional-embeddingencoding","title":"Positional Embedding/Encoding","text":"<p>The original transformers paper also embeds the positional information into the input sequence to the self attention layers. What  does positional embeddings do? Imagine if we didn't have the positional embeddings. When the self-attention mechanism is presented in the next section, you'll realise that each token embedding is treated independently of its position in a sentence. We need a way to \"inject\" the fact that a token's position is important before the embeddings enter as input into the self attention mechanism, where they will be further projected into many more different kind of vectors in different spaces. </p> <p>A motivating example would be something like processing a sentence - <code>a person ate the fish</code>. This has a completely different meaning to another ordering of the words - <code>a fish ate the person</code> - the positions of the words <code>fish</code> and <code>person</code> change the meaning of the phrases, and we need a mechanism to preserve that.</p> <p>The way the original transformers paper did this was by using sinusoidal encodings. The scheme they use is something like </p> \\[ \\text{PE}_{(pos, 2i)} = sin(\\frac{pos}{10000^{\\frac{2i}{d_\\text{model}}}}) \\] <p>for tokens in the even indexes of the hidden dimension, and</p> \\[ \\text{PE}_{(pos, 2i + 1)} = cos(\\frac{pos}{10000^{\\frac{2i}{d_\\text{model}}}}) \\] <p>for tokens in the odd index of the hidden dimension. </p> <p>These encodings would then be added to the learned embeddings. </p> <p>In pure python code, this would look something like this</p> <pre><code>def positional_encoding(seq_len: int, d_model: int) -&gt; list[list[float]]:\n    \"\"\"Given a sequence length, and a model embedding size, return a matrix\n    of seq_len x d_model encoding the positions\n\n    Args:\n        seq_len: number of tokens\n        d_model: dimensions of the input to the embedding vector\n\n    Returns:\n        a matrix of seq_len x d_model\n    \"\"\"\n    def _get_angle(pos, d_i):\n        denom = 10_000 ** ((2 * (d_i // 2)) / d_model)\n        return pos / denom\n\n    out = []\n    for pos in range(seq_len):\n        row = []\n        for d_i in range(d_model):\n            _fn = math.sin if d_i % 2 == 0 else math.cos\n            angle = _get_angle(pos, d_i)\n            row.append(_fn(angle))\n        out.append(row)\n\n    return out\n</code></pre> <p>When you plot it for <code>seq_len = 10</code> and <code>d_model = 64</code> (just toy values), you will get something like </p> <p> You can look at Jay Alamar's code to see how to do this in a vectorised manner. </p> <p>In the old tensor2tensor library, the way positional encoding was done was actually by not interleaving the sines and cosines, but instead by doing something like this instead:</p> <pre><code>def positional_encoding_concat(seq_len: int, d_model: int) -&gt; list[list[float]]:\n    \"\"\"Given a sequence length, and a model embedding size, return a matrix\n    of seq_len x d_model encoding the positions.\n\n    The difference this time is that you don't interleave the sines and the cosines\n    but follow the tensor2tensor style of just concatenating sines and cosines\n\n    Args:\n        seq_len: number of tokens\n        d_model: dimensions of the input to the embedding vector\n\n    Returns:\n        a matrix of seq_len x d_model\n    \"\"\"\n    def _get_angle(pos, d_i):\n        denom = 10_000 ** (d_i / (d_model/2))\n        return pos / denom\n\n    out = []\n    for pos in range(seq_len):\n        row = []\n        for d_i in range(d_model//2):\n            angle = _get_angle(pos, d_i)\n            row.append(math.sin(angle))\n        for d_i in range(d_model//2):\n            angle = _get_angle(pos, d_i)\n            row.append(math.cos(angle))\n        out.append(row)\n\n    return out\n</code></pre> <p>for which you'd get the plot  The main difference being in 1. the <code>_get_angle</code> method 2. the for loop where the values are being appended</p> <p>According to the Tensorflow tutorial on transformers, these 2 are functionally equivalent. You can see why - the main aim of positional encodings is to make sure that embeddings that are near each other have a \"similar\" positional encoding, while those far away have different positional encodings. The second implementation is easier to do in a vectorised manner.</p> <p>The advantage of positional encodings that use such a sinusoidal pattern is that it can generalise to sequence lengths not seen before during training, because it is a deterministic function with respect to the token's position in the input.</p> <p>So the final embedding layer that includes positional encoding will look something like: </p> <pre><code>import random \n\nclass PurePythonEmbeddingLayer:\n    def __init__(self, vocab_size: int, embedding_dim: int):\n        self.weight = [[random.random() for _ in range(embedding_dim)] for _ in range(vocab_size)]  # &lt;- the trainable lookup!\n        self.embedding_dim = embedding_dim\n\n    def forward(self, inputs: list[int]):\n        out = []\n        positional_encodings = positional_encoding_concat(seq_len=len(inputs), d_model=self.embedding_dim)\n        for i, inp in enumerate(inputs):\n            token_embedding = self.weight[inp]\n            positional_encoding = positional_encodings[i]\n            out.append([_t + _p for _t, _p in zip(token_embedding, positional_encoding)])\n        return out\n    ...\n</code></pre> <p>Why don't we just make the model learn how to embed position instead of hardcoding it?  The answer to that is, you can! The BERT paper, which basically takes the original transformer paper and scales it up immensely, does precisely this. The <code>positional_embedding</code> is just another learnable parameter in the BERT encoder, as opposed to the deterministic vector you see in the original transformer paper. This increases the number of parameters a model has. The disadvantage of a scheme like this is that at inference time, you cannot have sequence lengths that are longer than what the model has been trained at </p> <p>Other more sophisticated embedding schemes include rotary positional embeddings (RoPE). RoPE is what the later versions of the GPT class of models use. This will not be discussed in greater detail here. </p> <p>Now that you have the embeddings ready, we are finally in a position to explain how the self attention mechanisms works. </p>"},{"location":"blog/2024/01/05/self-attention/#self-attention","title":"Self-attention","text":""},{"location":"blog/2024/01/05/self-attention/#why-self-attention-why-not-just-use-fully-connected-layers","title":"Why self-attention? Why not just use fully connected layers?","text":"<p>The UDL book makes a very compelling argument, that will be reproduced in part here. </p> <p>Here's an example of an actual review from Amazon:</p> <pre>\nin 1989, I managed a crummy bicycle shop, \"Full Cycle\" in Boulder, Colorado. The Samples had just recorded this album and they played most nights, at \"Tulagi's\" - a bar on 13th street. They told me they had been so broke and hungry, that they lived on the free samples at the local supermarkets - thus, the name. i used to fix their bikes for free, and even feed them, but they won't remember. That Sean Kelly is a gifted songwriter and singer.\n</pre> <p>There are immediately 3 observations you can make about using just fully connected layers:</p> <ol> <li>This is 83 word review, and roughly 110 tokens (remember - everything is in terms of token length!) worth of text. Imagine now that each token is represented by a vector of size 1024 (the embedding dimension \\(\\textbf{D}\\)). If you were to have a model consisting only of fully connected layers, each layer would contain \\(N^2D^2\\)  parameters, and with \\(k\\) such layers, you'd have \\(kN^2D^2\\) such parameters. This is a lot of parameters.</li> <li>Each review on Amazon can have a variable number of inputs. How would you take that into account when designing a model consisting only of fully connected layers? You could technically just fix the upper bound of N to some arbitrarily large value, and fill the positions with no text with some arbitrary \"pad\" tokens, but this feels like an inefficient way of learning representations. </li> <li>Language is inherently ambiguous, and it is unclear from syntax alone what the semantic meaning of each token is. <ol> <li>In the example above, the word <code>That</code> in the last sentence doesn't specifically refer to any object or subject. </li> <li>This means that we want an architecture where there must be connections between word representations and that the strength of these connections should depend on the words themselves. </li> <li>Technically you could have fully connected layers that do the same thing. In such a case, you'd hope the superfluous weights go to zero, and all weights that connect neurons that belong to the same group of tokens to other such groups would all be the same during the learning process. But this is an incredibly inefficient way of learning. This is probably easier to see in the diagram below. <ol> <li>In the top part, you want all the lines of the same colour to be the same weight.</li> <li>In the second part, you see that it is also technically possible to do this with fully connected layers, but this adds a lot of parameters for training without adding any real value, because we know the constraints we would like them to fulfill. </li> </ol> </li> </ol> </li> </ol> <p></p>"},{"location":"blog/2024/01/05/self-attention/#self-attention-block","title":"Self attention block","text":"<p>We can now turn our attention to what the self attention block does. We will proceed from the outputs of the embedding layer as input to this block</p> <p>If we were to use a standard neural network, each \\(D\\times1\\) input would have a linear transformation applied to it, followed by a non-linear activation function (softmax, ReLU etc.) applied to it. We can represent the mapping of a standard neural network on an input \\(\\mathbf{x}\\) as </p> \\[ f(\\mathbf{x}) = \\text{someNonLinearFunction}(\\mathbf{Ax + b}) \\] <p>How is a self attention block different? The simplest way to think of a self attention block is in 2 steps.</p> <p>Assume an input of \\(N\\) vectors \\(x_1, x_2, .... x_N\\), each of dimension \\(D\\). </p> <p>The first step in an a self-attention block is the following - for each of the \\(N\\) vectors, a corresponding value is computed using the standard way - </p> \\[ \\mathbf{v}_m = \\mathbf{A}_v\\mathbf{x}_m + \\mathbf{b}_v \\] <p>where \\(\\mathbf{A} \\in \\mathbb{R}^{D\\times D}, \\mathbf{b} \\in \\mathbb{R}^{D}\\) , are shared across all input vectors. You can see this in the description section of the original transformers paper. </p> <p>The second step is then computing a weighted sum across this set of values \\(\\mathbf{v}_1,  ..., \\mathbf{v}_N\\) for each \\(i \\in [1, N]\\):</p> <p>$$ \\text{out}i = \\sum{m=1}^{N}a(\\mathbf{x}_m, \\mathbf{x}_i)\\mathbf{v}_m $$ where \\(a(\\mathbf{x}_m, \\mathbf{x}_i)\\) is the attention that the \\(i^{th}\\) output pays to the \\(m^{th}\\) input. A weighted sum means that for each \\(i\\), the sum \\(\\sum_{m=1}^{N}a(\\mathbf{x}_m, \\mathbf{x}_i) = 1\\) , and each of the weights \\(a(\\mathbf{x}_m, \\mathbf{x}_i)\\) is non-negative.</p> <p>So what is the attention function? It's two inputs - the first is \\(\\mathbf{x}_m\\), i.e. the \\(m^{th}\\) input vector, whose corresponding value vector \\(\\mathbf{v}_m\\) is what the result of the attention computation will be multiplied with. The second is \\(\\mathbf{x}_i\\), the \\(i^{th}\\) input vector, where \\(i\\) is also the position of the output vector we are trying to compute. We will need to compute 2 linear transformations first - the first being</p> \\[ \\mathbf{q}_i = \\mathbf{A}_q\\mathbf{x}_i + \\mathbf{b}_q \\] <p>which is called the query value, computed on the output's \\(i^{th}\\) position. The second linear transformation is </p> \\[ \\mathbf{k}_m = \\mathbf{A}_k\\mathbf{x}_m + \\mathbf{b}_k \\] <p>which is called the key value, computed on the \\(m^{th}\\) input vector. </p> <p>The attention function \\(a(\\mathbf{x}_m, \\mathbf{x}_i)\\) will then be defined as:</p> \\[ a(\\mathbf{x}_m, \\mathbf{x}_i) \\coloneqq \\text{softmax}_m(\\mathbf{k}^T_{\\circ}\\mathbf{q}_i) \\\\ = \\frac{\\text{exp}(\\mathbf{k}^T_m\\mathbf{q}_i)}{\\sum_{j=1}^{N}\\text{exp}(\\mathbf{k}_j^T\\mathbf{q}_i)} \\] <p>The following diagram from the UDL book is instructive to visualise the 2 different stages of calculating the </p> <p></p> <p>All this seems pretty unintuitive - what is the correct \"mental model\" to have for the key, query, and value vectors we have just introduced? I'm going to modify the insight Eugene Yan's blog here has on this -  1. Say you are in a library, and you want some knowledge. To get knowledge, you basically want the answers to N queries you have.  2. Now in the library, there are a lot of books. Each book has a spine with its title, which you can think of as its key, and each book has some content, which is it's value.  3. Now, for each query you have, you look at the spine of each book to decide how much attention should give to the contents of that book for that particular query, and you do it for all books present in the library.</p> <p>What does this all look like in code? You can look at the notebooks provided by the author of the UDL book for a clearer understanding, and I'm going to reproduce something very similar to the book here.</p> <p>For our toy example, let's assume the output of the embeddings are 3 input vectors \\(\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3\\) , each with dimension \\(D = 4\\).  </p> <pre><code>import numpy as np\n# Set seed so we get the same random numbers\nnp.random.seed(3)\n# Number of inputs\nN = 3\n# Number of dimensions of each input\nD = 4\n\nall_x = []\n# Create elements x_n and append to list\nfor n in range(N):\n  all_x.append(np.random.normal(size=(D,1)))  # &lt;- doesn't really matter for this toy example\n\nprint(all_x)\n</code></pre> <p>Now, let's initialise the weights for \\(A_q, A_k, A_v\\) and biases \\(b_q, b_k, b_v\\): </p> <pre><code># all the weights are of dimension DxD \nA_q = np.random.normal(size=(D,D))\nA_k = np.random.normal(size=(D,D))\nA_v = np.random.normal(size=(D,D))\n\n# all the biases are of dimension Dx1\nb_q = np.random.normal(size=(D,1))\nb_k = np.random.normal(size=(D,1))\nb_v = np.random.normal(size=(D,1))\n</code></pre> <p>We can now compute the keys, queries, and values for each of inputs:</p> <pre><code># Make three lists to store queries, keys, and values\nall_queries = []\nall_keys = []\nall_values = []\n# For every input\nfor x in all_x:\n  query = omega_q @ x + beta_q\n  key = omega_k @ x + beta_k \n  value = omega_v @ x + beta_v\n\n  all_queries.append(query)\n  all_keys.append(key)\n  all_values.append(value)\n</code></pre> <p>Side note regarding multiplying numpy arrays - the <code>@</code> operator is a standard matrix multiplication operation, and requires 2 matrices of dimensions \\(A \\times B\\),  and \\(B \\times C\\), and will result in a \\(A\\times C\\) matrix. If you use a <code>*</code> operator instead, you can only do so if your matrix pair \\(M_1, M_2\\) are either vectors or square matrices. </p> <p>We then need the softmax function: <pre><code>def softmax(items_in: list):\n  e_x = np.exp(items_in - np.max(items_in))\n  return e_x / e_x.sum()\n</code></pre> Subtracting the maximum value in the exponent is a common technique to make sure the values do not explode. Functionally they don't change the softmax function, since</p> \\[ \\frac{e^{x_i}}{\\sum^{N}_{i=1}{e^{x_i}}} = \\frac{(\\frac{1}{e^C})e^{x_i}}{(\\frac{1}{e^C})\\sum^{N}_{i=1}{e^{x_i}}} = \\frac{e^{x_i - C}}{\\sum^{N}_{i=1}{e^{x_i - C}}} \\] <p>For a pretty interesting discussion about why we use softmax, and the interpretation of this non-linear function, feel free to look at Pinecone's handy guide here.  </p> <p>We are now in a position to compute each of the outputs. As a reminder, we are going to have \\(N\\) output vectors \\(\\text{out}_1, \\text{out}_2, \\text{out}_3, ... \\text{out}_N\\), each of which is a weighted average of the value vectors. </p> <pre><code># Make three lists to store queries, keys, and values\nall_queries = []\nall_keys = []\nall_values = []\n# For every input\nfor x in all_x:\n  query = omega_q @ x + beta_q\n  key = omega_k @ x + beta_k \n  value = omega_v @ x + beta_v\n\n  all_queries.append(query)\n  all_keys.append(key)\n  all_values.append(value)\n\n#\u00a0compute the outs\nall_outs = []\n\nfor i in range(N):\n    all_kj_qi = [] # &lt;-- will be a 1 x N vector\n    q_i = all_queries[i]\n    for key_j in all_keys:\n        dot_product = np.dot(key_j, q_i).squeeze()\n        all_kj_qi.append(dot_product)\n\n    attention = softmax(all_kj_qi) # &lt;-- 1 x N vector that sums to 1\n    out_i = sum(attention[i] * all_values[i] for i in range(N))\n    all_outs.append(out_i)\n</code></pre> <p>And that is it! You have basically implemented the self-attention mechanism from scratch using just (mostly) raw python loops! If you want to involve more matrix operations, you can do it the following way: </p> <pre><code>def softmax_cols(data_in):\n    # Exponentiate all of the values\n    _data_in = data_in - np.max(data_in, axis=0)\n    exp_values = np.exp(_data_in)\n    # Sum over columns\n    denom = np.sum(exp_values, axis=0)\n    # Compute softmax\n    softmax = exp_values / denom\n    # return the answer\n    return softmax\n\ndef self_attention(X, A_v, A_q, A_k, b_v, b_q, b_k):\n    # 1. Compute queries, keys, and values\n    V = b_v + A_v @ X\n    Q = b_q + A_q @ X\n    K = b_k + A_k @ X\n    # 2. Compute dot products\n    dot_pdts = K.T @ Q\n    # 3. Apply softmax to calculate attentions\n    attention = softmax_cols(dot_pdts)\n    print(attention.shape) # &lt;-- This will now be a NxN matrix!\n    # 4. Weight values by attentions\n    out = V @ attention\n\n    return out\n</code></pre> <p>This function presents the famous self-attention equation:</p> \\[ \\text{attentionOutput}(V, Q, K) = V\\text{softmax}(K^TQ) \\] <p>more naturally. </p> <p>If you inspect the values of the \\(N\\times N\\) <code>attention</code> matrix, you'll notice the extreme values - some values are very close to 1, and many values are nearly 0. This is because the values of \\(K^TQ\\) become either too big a positive value, or too big a negative value. </p> <p>We ideally want to scale the values in the attention such that the variance in the input values to the softmax function is reduced to avoid the vanishing gradient problem. A hand-wavy justification for this is the following:</p> <p>Weights in a neural network are updated using backpropagation (you can read up the details of this method elsewhere). Let the \\(i^{th}\\) output of the softmax function be defined as \\(p_i\\) where</p> \\[ p_i = \\frac{e^{x_i}}{\\sum^{N}_{i=1}{e^{x_i}}} \\] <p>Then, when we look at the gradients of this function, we can see that the partial derivative of \\(p_i\\) with respect to \\(x_i\\) is -</p> \\[ \\frac{\\partial p_i}{\\partial x_i} = \\frac{\\partial}{x_i} \\left( \\frac{e^{x_i}}{e^{x_i}+ C} \\right) = \\frac{(e^{x_i} + C)e^{x_i} - e^{x_i}e^{x_i}}{(e^{x_i} + C)^2} = (\\frac{e^{x_i}}{e^{x_i}+ C})(\\frac{C}{e^{x_i}+ C}) = p_i(1 - p_i) \\] <p>where we use \\(C\\) in that hand-wavy way non-mathematicians use to express terms that can be treated as a \"constant\" in a partial derivative. Then for \\(j\\neq i\\), </p> \\[ \\frac{\\partial p_i}{\\partial x_j} = \\frac{\\partial}{x_i}\\left(\\frac{C'}{e^{x_j}+ C}\\right) = \\frac{-C'e^{x_j}}{(e^{x_j}+ C)^2} = -p_jp_i \\] <p>The last characteristic you need to remember about the softmax is the following: </p> \\[ \\sum_{i=1}^N p_i = 1 \\] <p>So if any \\(p_i\\) is very close to one, the partial derivatives will be close to 0 because the other term (either \\(1 - p_i\\) or \\(p_j\\)) is going to be very close to 0. Naturally, the partial derivatives will also be close to 0 if any of the \\(p_i\\)'s is close to 0. </p> <p>Scaling the inputs the softmax function is typically done by dividing the \\(\\mathbf{K^T}\\mathbf{Q}\\) result with \\(\\sqrt{D_k}\\) , i.e. the dimension of the keys (and the dimension of the queries). Why this particular constant? This is explained in the paper. As the dimensions of the keys and the queries increase, it is likely that the final result of \\(\\mathbf{K^T}\\mathbf{Q}\\) increases in value. Intuitively, this is because if \\(q\\) and \\(k\\) are independent random variables with 0 mean and 1 variance, the dot product \\(q \\cdot k = \\sum_{i=1}^{d_k} q_ik_i\\) will have mean 0 and variance \\(d_k\\).  This is easy to derive from first principles, remembering that \\(\\text{Var}[x]) = \\mathbb{E}[x^2] - (\\mathbb{E}[x])^2\\) . The mean calculation of \\(\\mathbf{k}^T \\cdot \\mathbf{q}\\) is the following (with \\(D\\) being the dimension of the key and the query)</p> \\[ \\mathbb{E}[\\mathbf{k}^T \\cdot \\mathbf{q}] = \\mathbb{E}\\left[ \\sum_{i=1}^{D} k_iq_i \\right] = \\sum_{i=1}^D \\mathbb{E}[q_ik_i] \\] <p>and since \\(k_i, q_i\\) are independent, you have </p> \\[ \\sum_{i=1}^D \\mathbb{E}[q_ik_i] = \\sum_{i=1}^D \\mathbb{E}[q_i]\\mathbb{E}[k_i] = 0 \\] <p>As for variance we need to consider the following:  </p> \\[ \\text{Var}[\\mathbf{k}^T \\cdot \\mathbf{q}] = \\mathbb{E}\\left[(\\mathbf{k}^T \\cdot \\mathbf{q})^2\\right] - \\left(\\mathbb{E}[\\mathbf{k}^T \\cdot \\mathbf{q}] \\right)^2 \\] <p>We only have to consider the first term of the right hand side, because as we've just established, \\(\\mathbb{E}[\\mathbf{k}^T \\cdot \\mathbf{q}]=0\\). Given that each of these variables have variance 1, this means that:</p> \\[ \\text{Var}(k_i) = 1 =&gt; \\mathbb{E}[k_i^2] - (\\mathbb{E}[k_i])^2 = 1 \\] <p>and since \\(\\mathbb{E}[k_i]=0\\), we know \\(\\mathbb{E}[k_i^2] = 1\\). The same holds for \\(q_i\\).</p> <p>So, it easily follows:</p> \\[ \\text{Var}[\\mathbf{k}^T \\cdot \\mathbf{q}] = \\mathbb{E}\\left[(\\mathbf{k}^T \\cdot \\mathbf{q})^2\\right] = \\mathbb{E}\\left[ \\left( \\sum_{i=1}^Dk_iq_i \\right) \\left( \\sum_{j=1}^Dk_jq_j \\right) \\right] =\\left[ \\sum_{i=1}^D \\sum_{j=1}^D  \\mathbb{E}(q_iq_jk_ik_j)\\right] \\] <p>and since the \\(k\\)'s and the \\(q\\)'s are independent:</p> \\[ \\left[ \\sum_{i=1}^D \\sum_{j=1}^D  \\mathbb{E}(q_iq_jk_ik_j)\\right] = \\left[ \\sum_{i=1}^D \\sum_{j=1}^D  \\mathbb{E}(q_iq_j)\\mathbb{E}(k_ik_j)\\right] \\] <p>now, for cases where \\(i \\neq j\\), you get \\(\\mathbb{E}(q_iq_j) = \\mathbb{E}(q_i)\\mathbb{E}(q_j) = 0\\), and so you only have to care about the cases where \\(i = j\\). This then simplifies everything to: </p> \\[  \\left[ \\sum_{i=1}^D \\sum_{j=1}^D  \\mathbb{E}(q_iq_j)\\mathbb{E}(k_ik_j)\\right] = \\sum_{i=1}^D \\mathbb{E}(q_i^2) \\mathbb{E}(k_i^2) =\\sum_{i=1}^D 1= D \\] <p>So to summarise this last part - the reason we scale everything by \\(\\sqrt{D_k}\\) is because under the assumption that \\(k\\)'s and \\(q\\)'s are independent variables with 0 mean and unit variance, this is the scaling factor we need to keep the variance of </p> <p>$$ \\mathbf{k}^T \\cdot \\mathbf{q} $$ to 1, and the mean to be 0. </p> <p>The self attention block is the basic unit of the transformer, and its details are often not appreciated. Hopefully, this post has given you a better intuition for it by decomposing every step into its most basic form.  </p>"},{"location":"blog/2024/01/04/manacher-algo/","title":"Manacher's Algorithm","text":"<p>Super annoying algorithm, but it has uses in bioinfomatics. </p> <p>Here's the task: Given a string\u00a0<code>s</code>, return\u00a0the longest palindromic substring \u00a0in\u00a0<code>s</code>.</p> <p>Example inputs and outputs:</p> <p>Input: s = \"babad\" Output: \"bab\" Explanation: \"aba\" is also a valid answer.</p> <p>Input: s = \"cbbd\" Output: \"bb\"</p> <p>To build some intuition for how this algorithm works, let's see an example of a brute force implementation. In this implementation, given a string <code>_str</code>, we want to see for any <code>i</code>, what is the maximum length of a palindrome centred around the character <code>_str[i]</code>. Something simple would be something like this: </p> <pre><code>def manacher_brute_force(_str: str) -&gt; list:\n  _str_len = len(_str)\n  out = [0 for _ in range(_str_len + 2)]\n  # make sure first and last chars are different and do not\n  #\u00a0happen in _str itself\n  _str = f\"${_str}^\"\n\n  for i in range(1, _str_len + 1):\n\n    while _str[i - out[i]] == _str[i + out[i]]:\n      out[i] += 1\n  return out\n\nif __name__ == \"__main__\":\n  print(manacher_brute_force(\"abbcbba\"))\n</code></pre> <p>the output will be something like <code>[0, 1, 1, 1, 4, 1, 1, 1, 0]</code></p> <p>But in this approach, when we sweep from left to right, we are not using the work we have already done to the left of <code>i</code>. This is where Manacher comes in. </p>","tags":["algorithms"]},{"location":"blog/2024/01/04/manacher-algo/#what-dont-we-have-to-search","title":"What don't we have to search?","text":"<p>We can exploit the nature of a palindrome - assume we have a palindrome of length <code>l</code> centred around index <code>i</code>, and say we take 2 indexes <code>i'</code> and <code>i''</code> that are distance <code>d</code> left and right to <code>i</code> respectively s.t. <code>d &lt; l</code>, then we basically know that any palindrome that is centred around <code>i'</code> will also likely be centred around <code>i''</code>!</p> <p>From this simple observation, we can already amend the inner <code>while</code> loop so that we are not searching all of the characters to the left and right of a particular index <code>i''</code> of an index that is to the right of an index <code>i</code> we have already done work for.  </p> <p>The rest of the complexity comes from the need to handle the case where the borders of the inner palindrome reaches the border of the outer palindrome. All you have to do there is make sure you always check whenever you go beyond the borders of the longest current palindrome. </p> <pre><code>def manacher_odd(_str: str) -&gt; list:\n    _str_len = len(_str)\n    out = [0 for _ in range(_str_len + 2)]\n    # make sure first and last chars are different and do not\n    #\u00a0happen in _str itself\n    _str = f\"${_str}^\"\n    l, r = 1, 1\n    for i in range(1, _str_len + 1):\n        dist_to_border = r - i\n        inner_palindrome_len = min(dist_to_border, out[l + dist_to_border])\n        out[i] = max(0, inner_palindrome_len)\n        while _str[i - out[i]] == _str[i + out[i]]:\n          out[i] += 1\n        if i + out[i] &gt; r:\n            l = i - out[i]\n            r = i + out[i]\n    return out\n</code></pre> <p>Code is a translation of the cpp code here</p> <p>NOTE:</p> <p>The above algorithm is only for odd length. In practice, you can make any string odd length by doing something like </p> <pre><code>_str = \"\u00a3\".join(_str)\n_str = f\"#{_str}^\"\n</code></pre> <p>the <code>.join</code> adds <code>n-1</code> characters to the string, so the total number of characters will be odd, since <code>even + odd = odd</code></p>","tags":["algorithms"]},{"location":"blog/2024/01/04/manacher-algo/#where-does-the-time-saving-come-from-that-makes-it-linear","title":"Where does the time saving come from that makes it linear?","text":"<p>In the brute force method, consider the number of times a character at index <code>i</code> is compared to some other character. You will quickly realise it is <code>O(n)</code>, and that is where the overall \\(O(n^2)\\) complexity for the brute force method comes from. </p> <p>But with Manacher's algorithm, the while loop is no longer independent of the outer for loop. The outer loop is keeping track of the <code>centre</code> of palindromes and is always increasing. We only do additional comparison operations when <code>r</code> variable (i.e. the rightmost boundary, the <code>centre + radius</code> value) increases - and this quantity never decreases in value! Therefore, the total number of operations in the outer and in the inner loop adds to <code>n</code>. </p>","tags":["algorithms"]},{"location":"blog/2024/01/04/manacher-algo/#final","title":"Final:","text":"<pre><code>def manacher(s: str) -&gt; str:\n        if len(s) &lt;= 1:\n            return s\n\n        s = f\"#{'#'.join(s)}#\"\n        s_len = len(s)\n        out = [0 for _ in range(s_len)]\n        max_radius = 1\n        max_str = s[1]\n\n        l, r = 1, 1\n        for i in range(1, s_len - 1):\n            dist_to_edge = r - i\n            allowable_dist = min(dist_to_edge, out[l + dist_to_edge])\n            out[i] = max(0, allowable_dist)\n            while i - out[i]&gt;= 0 and i + out[i] &lt; s_len and s[i - out[i]] == s[i + out[i]]:\n                out[i] += 1\n            if i + out[i] &gt; r:\n                r = i + out[i]\n                l = i - out[i]\n            if out[i] &gt; max_radius:\n                max_radius = out[i]\n                max_str = s[l + 1 : r].replace(\"#\", \"\") # if you want s[l: r + 1] you need to offset in the while loop like so: \n                # while i - out[i] - 1&gt;= 0 and i + out[i] + 1 &lt; s_len and s[i - out[i] - 1] == s[i + out[i] + 1]\n        return max_str\n</code></pre> <p>or if you want to use the original <code>manacher_odd</code> kind of notation:</p> <pre><code>    def manacher(s: str) -&gt; str:\n        if len(s) &lt;= 1:\n            return s\n\n        s = f\"\u00a3#{'#'.join(s)}#^\"\n        s_len = len(s)\n        out = [0 for _ in range(s_len)]\n        max_radius = 1\n        max_str = s[1]\n\n        l, r = 1, 1\n        for i in range(1, s_len - 1):\n            dist_to_edge = r - i\n            allowable_dist = min(dist_to_edge, out[l + dist_to_edge])\n            out[i] = max(0, allowable_dist)\n            while s[i - out[i]] == s[i + out[i]]:\n                out[i] += 1\n            if i + out[i] &gt; r:\n                r = i + out[i]\n                l = i - out[i]\n            if out[i] &gt; max_radius:\n                max_radius = out[i]\n                max_str = s[l + 1 : r].replace(\"#\", \"\")\n        return max_str\n</code></pre>","tags":["algorithms"]},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/algorithms/","title":"algorithms","text":""}]}