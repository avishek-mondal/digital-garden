{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"blog/","title":"Hello!","text":"<p>I post random things that are tangentially related to data structures and algorithms, deep learning, and other machine learning related topics.</p> <p>If you want more information about who I am, feel free to look at my website.</p>"},{"location":"blog/2024/08/05/lipschitz-constant-of-a-linear-transformation/","title":"Lipschitz constant of a linear transformation","text":"<p>DISCLAIMER: If you spot an error, please feel free to email me. </p> <p>The \"Understanding Deep Learning\" book has recently come out (you can look at it here, this post refers to the 2023-05-08 version), and is a great resource. In the appendices, it contains several statements which can be non-obvious to those of us who have been out of touch with linear algebra in our day jobs. </p> <p>Here is one such statement: </p> <p>\"The Lipschitz constant of a linear transformation $$ f[z] = Az + b $$ is equal to the maximum eigenvalue of the matrix A.\"</p> <p>This is not an obvious result at all. Let us try and break this down step by step.</p>"},{"location":"blog/2024/08/05/lipschitz-constant-of-a-linear-transformation/#definitions","title":"Definitions","text":""},{"location":"blog/2024/08/05/lipschitz-constant-of-a-linear-transformation/#lipschitz-constant","title":"Lipschitz constant","text":"<p>A function \\(\\(f[z]\\)\\) is Lipschitz continuous if for all $$ z1,z2 $$:</p> \\[ |f[z1] \u2212 f[z2]| \u2264 \u03b2|z1 \u2212 z2|, \\] <p>The value $$ \\beta $$ is called the Lipschitz constant. Both definitions are taken from the UDL book in the link above.</p>"},{"location":"blog/2024/08/05/lipschitz-constant-of-a-linear-transformation/#vector-norm","title":"Vector norm","text":"<p>The following definitions are lifted from this very useful resource here (Definition 3)</p> <p>Let \\(\\(\\mathcal{V}\\)\\) be a vector space over a field $$ \\mathbb{R} $$. A function $$ \\lvert{.}\\rvert : \\mathcal{V} \u2192 \\mathbb{R}$$ is called a (vector) norm if</p> <ol> <li>$$ \\lvert x \\rvert \u2265 0 $$ for all $$ x \u2208 \\mathcal{V} $$ , with equality iff x = 0, [positivity]</li> <li>$$ \\lvert \u03bbx \\rvert = \\lvert\u03bb \\rvert \\lvert x \\rvert $$ for all \u03bb \u2208 $$ \\mathbb{K} $$ and x \u2208 V , [homogeneity] (reminder $$ \\mathbb{K} \u2208 {\\mathbb{R}, \\mathbb{C} }$$, i.e. either real or complex)</li> <li>$$ \\lvert x + y \\rvert \u2264 \\lvert x \\rvert + \\lvert y \\rvert $$for all x, y \u2208 V . [triangle inequality]</li> </ol> <p>An example of such a norm would be an expression as follows for $$ p \\geq 1 $$: </p> \\[ |x|_p = [|x_1|^p + |x_2|^p + |x_3|^p + ... ]^{\\frac{1}{p}} \\]"},{"location":"blog/2024/08/05/lipschitz-constant-of-a-linear-transformation/#matrix-norm","title":"Matrix norm","text":"<p>Also from the above resource (Definition 7)</p> <p>A function  $$ || .||: \\mathcal{M} \\rightarrow \\mathbb{C} $$ is a matrix norm if:</p> <ol> <li> <p>$$||A|| \\geq 0 $$ for all  $$ A \\in \\mathcal{M}$$ with equality iff x = 0 [positivity]</p> </li> <li> <p>$$ ||\\lambda A|| = |\\lambda| ||A|| $$ for all \\(\\(\\lambda \\in \\mathbb{C}\\)\\) and $$ A \\in \\mathcal{M}$$ [homogeneity]</p> </li> <li> <p>$$||A + B|| \\leq ||A|| + ||B|| $$ for all \\(\\(A, B \\in \\mathcal{M}\\)\\)  [triangle inequality]</p> </li> <li> <p>$$ ||AB|| \\leq ||A|| ||B|| $$ for all $$A, B \\in \\mathcal{M} $$ [submultiplicativity]</p> </li> </ol> <p>The last property, submultiplicativity, deserves some more attention. It uses a more general definition of matrix norm which is </p> \\[ ||A|| = \\text{sup}\\{|Ax|: x \\in \\mathcal{V}, |x| \\leq 1 \\} \\\\  = \\text{sup}\\{\\frac{|Ax|}{|x|}: x \\in \\mathcal{V}, x \\neq 0 \\} \\] <p>(lots of resources use this definition, for clarification of max vs sup, look here).</p> <p>An intermediate result we can now prove is the following (or you could also just look here):</p> \\[ |Ax| \\leq ||A|| |x|, \\, \\forall x \\in \\mathbb{R}^n \\] <p>The proof in the top answer of the above link from Stackoverflow is copied below for ease of reference: </p> <p>By definition $$ \u2016\ud835\udc34\u2016= \\text{sup} {|Ax|, x \u2208 \\mathbb{R}^n, |x| \u2264 1 } $$ and hence for any  $$ x \\in \\mathbb{R}^n $$ such that  $$ |x| \\leq 1 $$ we must have by the definition of supremum that  $$ |Ax| \\leq ||A|| $$ </p> <p>The $$ x = 0 $$ case is trivial.</p> <p>For the $$ x \\neq 0 $$ case: </p> <p>Let  $$ y = \\frac{x}{|x|} $$. It follows that $$ |y| = 1 $$, and hence $$ |Ay| \\leq ||A|| $$ from above.</p> <p>Now we have </p> \\[ |Ay| = | A \\frac{x}{|x|} | = \\frac{|Ax|}{|x|} \\] <p>Therefore, since  $$|x|&gt;0 $$, we have $$ |Ax|=|Ay||x|\u2264 ||A|| |x| $$</p>"},{"location":"blog/2024/08/05/lipschitz-constant-of-a-linear-transformation/#tying-it-all-together","title":"Tying it all together","text":"<p>Now, we can look at the Lipschitz constant of a linear transformation $$ f[z] = Az + b $$.</p> <p>$$ |f[z1] \u2212 f[z2]| = |Az_1 - Az_2| \\</p> <p>= |A(z_1 - z_2)| \\</p> <p>\\leq ||A|| |z_1 - z_2| $$</p> <p>The last step is using the same intermediate property we proved when showing the proof for submultiplicativity.</p> <p>This is the general case - the Lipschitz constant of a linear transform will be the matrix norm of the matrix $$ A $$.</p> <p>We need one more step to prove the claim in the book, and I think that involves deviating away from the general case.</p> <p>I am going to be following some of the material here. When the 2-norm is used, the induced matrix operator is the following (called the spectral norm):</p> \\[ ||A||_2 = \\text{max}_{x \\neq 0} \\{ \\frac{|Ax|_2}{|x|_2} \\} \\] <p>It is a known result for the 2-norm that  $$ |y|_2^2 = y^Ty $$</p> <p>So let us take the square of the induced matrix operator.</p> \\[ ||A||_2^2 = \\text{max}_{x \\neq 0} \\{ \\frac{|Ax|_2^2}{|x|_2^2} \\} \\\\ = \\text{max}_{x \\neq 0} \\{ \\frac{x^TA^TAx}{x^Tx} \\} \\] <p>We will need to use concepts from singular value decomposition (SVD). This resource might help. The \"trick\" is to write A as</p> <p>$$ A = U\\Sigma V^T $$ and then write $$ A^TA = (U\\Sigma V<sup>T)</sup>T (U\\Sigma V^T) = V \\Sigma^T \\Sigma V $$, where $$ \\Sigma $$ is a diagonal matrix with the eigenvalues of A in the diagonal.</p> <p>Let $$ \\lambda_1 $$ be the largest eigenvalue of A. Then, the maximum value of the ratio of $$ \\frac{x<sup>TA</sup>TAx}{x^Tx} $$ can be shown to be $$ \\lambda_1^2 $$.</p> <p>That was the final step - showing  $$ ||A||_2^2 = \\lambda_1^2 $$, i.e. $$ ||A||_2 = \\lambda_1 $$</p> <p>So therefore, </p> <p>$$ |f[z1] \u2212 f[z2]|_2 = |Az_1 - Az_2|_2 \\</p> <p>= |A(z_1 - z_2)|_2 \\</p> <p>\\leq ||A||_2 |z_1 - z_2|_2 \\ = \\lambda_1 |z_1 - z_2| $$</p> <p>Other resources:  1. I found this from Drexel University to also be quite helpful</p>"},{"location":"blog/2024/06/13/masking-in-transformers/","title":"Masking in transformers","text":"<p>Masking is pretty fundamental to how transformers work. The main purpose of masking is to make sure the model doesn't \"attend to\" some tokens. Feel free to read  my previous blog post about Self attention.</p> <p>What are the kind of tokens we may not want a model to \"attend to\"? They're usually:</p> <ol> <li>Padding tokens - These tokens are added in some architectures (such as BERT) such that all input token sequences are of the same length, and you do not want them to affect your final output.  </li> <li>Future tokens - when training some decoder architectures, you want the \"weighting\" by the attention mechanism placed on the value vectors corresponding to tokens further along in the sequence in the training data to be zero. Because if it is non-zero your model is basically \"cheating\" during training by looking at what it is supposed to predict, and will not perform as well during inference time.</li> </ol> <p>With this in mind, let's see how masking tends to be implemented in practice. </p>"},{"location":"blog/2024/06/13/masking-in-transformers/#masking-the-basics","title":"Masking - the basics","text":"<p>The main way to make sure the tokens you want to mask are not being attended to is by manipulating the inputs to the softmax function. As a reminder, the softmax function is defined as the following - given an array of inputs \\([x_0, x_1, ... x_i, ..., x_N]\\) the softmax function returns an array of \\([p_0, p_1, ... p_i, ... p_N]\\) where </p> \\[ p_i = \\frac{e^{x_i}}{\\sum^{N}_{i=1}{e^{x_i}}} \\] <p>and \\(\\sum_ip_i = 1\\). </p> <p>This softmax function gets applied on the output of the \\(k^Tq\\) operation in the attention. The main \"trick\" when it comes to masking in a vectorized way is basically then overwriting this value with a large negative value. </p> <p>To see this in action, let us see how you'd make sure future tokens aren't being attended to in pure Python for loops. </p> <p>First, let's set up the toy example:</p> <pre><code>import numpy as np\n\ndef softmax(items_in: list):\n  e_x = np.exp(items_in - np.max(items_in))\n  return e_x / e_x.sum()\n\n\n# Set seed so we get the same random numbers\nnp.random.seed(3)\n# Number of inputs\nN = 4\n# Number of dimensions of each input\nD = 3\n\nall_x = []\n# Create elements x_n and append to list\nfor n in range(N):\n  all_x.append(np.random.normal(size=(D,1)))\n\nprint(\"all_x: \")\nprint(all_x)\nA_q = np.random.normal(size=(D,D))\nA_k = np.random.normal(size=(D,D))\nA_v = np.random.normal(size=(D,D))\n\n# all the biases are of dimension Dx1\nb_q = np.random.normal(size=(D,1))\nb_k = np.random.normal(size=(D,1))\nb_v = np.random.normal(size=(D,1))\n\nall_queries = []\nall_keys = []\nall_values = []\n# For every input\nfor x in all_x:\n  query = A_q @ x + b_q\n  key = A_k @ x + b_k \n  value = A_v @ x + b_v\n\n  all_queries.append(query)\n  all_keys.append(key)\n  all_values.append(value)\n</code></pre> <p>In this above example, the columns of <code>all_x</code> are the <code>D</code> dimensioned data points, and there are <code>N=3</code> of them. This can be thought of as the output of the embedding layer in a standard transformer block.</p> <p>Assume this is a causal mask we are trying to apply, i.e. for the <code>i_th</code> input, there should only be non-zero attention weights for values with an index <code>&lt;=i</code>. If that was confusing, let's illustrate this in code:</p> <pre><code>all_outs = []\n\nfor i in range(N):\n    print(\"==\"*10)\n    print(f\"i: {i}\")\n    all_kj_qi = []\n    q_i = all_queries[i]\n    for j in range(i + 1):  # &lt;- no future tokens will be attended to. If you want to attent to all tokens, this line would change to range(N) instead of range(i + 1) \n        key_j = all_keys[j]\n        dot_product = np.dot(key_j.T, q_i).squeeze()\n        all_kj_qi.append(dot_product)\n\n    print(\"before softmax:\")\n    print(all_kj_qi)\n    attention = softmax(all_kj_qi)\n    print(f\"attentions: {attention}\")\n    out_i = sum(attention[i] * all_values[i] for i in range(len(attention)))\n    all_outs.append(out_i)\n    print(\"==\"*10)\n</code></pre> <p>The output would be:  <pre><code>====================\ni: 0\nbefore softmax:\n[array(8.0985502)]\nattentions: [1.]\n====================\n====================\ni: 1\nbefore softmax:\n[array(1.33534917), array(1.04085729)]\nattentions: [0.57309546 0.42690454]\n====================\n====================\ni: 2\nbefore softmax:\n[array(2.95774823), array(0.4307023), array(2.98146278)]\nattentions: [0.47530942 0.0379747  0.48671588]\n====================\n====================\ni: 3\nbefore softmax:\n[array(7.39220366), array(0.99822195), array(6.05577164), array(5.18771535)]\nattentions: [0.72739962 0.00121591 0.19114723 0.08023724]\n====================\n</code></pre></p> <p>This basically means the following - </p> \\[ out_0 = 1.0 \\times v_0 \\] \\[ out_1 = 0.573 \\times v_0 + 0.427 \\times v_1 \\] \\[ out_2 = 0.475 \\times v_0 + 0.038 \\times v_1 + 0.487 \\times v_2 \\] <p>and so forth. As you can see, \\(out_i\\) only depends on \\(v_j\\) if \\(j&lt;=i\\). </p> <p>So far, so good, but these pure Python <code>for</code> loops are slow, and you want to do this in a vectorized way. How would you do that?</p> <p>As mentioned previously, you do this by \"forcing\" the pre-softmax co-efficients to be large negative values at the positions you want to mask, so that the post-softmax coefficients at these positions are 0. This is best illustrated in this snippet of code: </p> <pre><code>all_outs = []\n\nfor i in range(N):\n    print(\"==\"*10)\n    print(f\"i: {i}\")\n    all_kj_qi = [] # &lt;-- will be a 1 x N vector\n    q_i = all_queries[i]\n    for j in range(N):\n        key_j = all_keys[j]\n        dot_product = np.dot(key_j.T, q_i).squeeze()\n        all_kj_qi.append(dot_product)\n\n    print(\"before adding:\")\n    print(all_kj_qi)\n    to_add = np.array([0.] * (i + 1) + [-np.inf] * (N - i - 1))\n    all_kj_qi += to_add\n    print(\"after adding:\")\n    print(all_kj_qi)\n    attention = softmax(all_kj_qi) # &lt;-- 1 x N vector that sums to 1\n    print(f\"attentions: {attention}\")\n    out_i = sum(attention[i] * all_values[i] for i in range(N))\n    all_outs.append(out_i)\n    print(\"==\"*10)\n</code></pre> <p>resulting in the output: </p> <pre><code>====================\ni: 0\nbefore adding:\n[array(8.0985502), array(-1.41964676), array(0.40887655), array(-4.51894638)]\nafter adding:\n[8.0985502      -inf      -inf      -inf]\nattentions: [1. 0. 0. 0.]\n====================\n====================\ni: 1\nbefore adding:\n[array(1.33534917), array(1.04085729), array(3.15550058), array(4.36428589)]\nafter adding:\n[1.33534917 1.04085729       -inf       -inf]\nattentions: [0.57309546 0.42690454 0.         0.        ]\n====================\n====================\ni: 2\nbefore adding:\n[array(2.95774823), array(0.4307023), array(2.98146278), array(2.65666126)]\nafter adding:\n[2.95774823 0.4307023  2.98146278       -inf]\nattentions: [0.47530942 0.0379747  0.48671588 0.        ]\n====================\n====================\ni: 3\nbefore adding:\n[array(7.39220366), array(0.99822195), array(6.05577164), array(5.18771535)]\nafter adding:\n[7.39220366 0.99822195 6.05577164 5.18771535]\nattentions: [0.72739962 0.00121591 0.19114723 0.08023724]\n====================\n</code></pre> <p>You can see that the attention values are the same as the previous attention values! This second snippet is much simpler to write in a vectorized manner. First let's set everything up as matrices instead of vectors:</p> <pre><code>X = np.array(all_x).squeeze()\nQ = X @ A_q.T + b_q.T  # &lt;- N x D matrix\nK = X @ A_k.T + b_k.T  # &lt;- N x D matrix\nV = X @ A_v.T + b_v.T  # &lt;- N x D matrix\n\n# show that our set up above using pure python for loops, and this matrix set up are equivalent\nassert (Q == np.array(all_queries).squeeze()).all()\n\ndef softmax_cols(data_in):\n    # Exponentiate all of the values\n    # keepdims=True IS VERY IMPORTANT\n    _data_in = data_in - np.max(data_in, axis=1, keepdims=True)\n    exp_values = np.exp(_data_in)\n    # Sum over columns\n    denom = np.sum(exp_values, axis=1, keepdims=True)\n    # Compute softmax\n    softmax = exp_values / denom\n    # return the answer\n    return softmax\n</code></pre> <p>Now, to look at what the pre-softmax values are, you can inspect what <code>Q @ K.T</code> is: </p> <pre><code>&gt;&gt; Q@K.T\narray([[ 8.0985502 , -1.41964676,  0.40887655, -4.51894638],\n       [ 1.33534917,  1.04085729,  3.15550058,  4.36428589],\n       [ 2.95774823,  0.4307023 ,  2.98146278,  2.65666126],\n       [ 7.39220366,  0.99822195,  6.05577164,  5.18771535]])\n</code></pre> <p>note that each row is basically the array that was printed out as part of the <code>before adding:</code> part in the previous output snippet! For illustration:</p> <p></p> <p>And now you need the attention mask: </p> <pre><code>&gt;&gt; to_add = np.triu(-np.inf * np.ones((N, N)), k=1)\n&gt;&gt; to_add\n\narray([[  0., -inf, -inf, -inf],\n       [  0.,   0., -inf, -inf],\n       [  0.,   0.,   0., -inf],\n       [  0.,   0.,   0.,   0.]])\n</code></pre> <p>Then you'd add this mask to the \\(QK^T\\) value.  <pre><code>&gt;&gt;pre_softmax = Q@K.T + to_add\n&gt;&gt;pre_softmax\narray([[8.0985502 ,       -inf,       -inf,       -inf],\n       [1.33534917, 1.04085729,       -inf,       -inf],\n       [2.95774823, 0.4307023 , 2.98146278,       -inf],\n       [7.39220366, 0.99822195, 6.05577164, 5.18771535]])\n</code></pre></p> <p>Once done, just put it all through the softmax function! <pre><code>&gt;&gt;softmax_cols(pre_softmax)\narray([[1.        , 0.        , 0.        , 0.        ],\n       [0.57309546, 0.42690454, 0.        , 0.        ],\n       [0.47530942, 0.0379747 , 0.48671588, 0.        ],\n       [0.72739962, 0.00121591, 0.19114723, 0.08023724]])\n</code></pre></p> <p>This is your attention matrix! It is the same output we get as when we used the pure Python loops.</p> <p>As a recap, in a vectorized way, all you need is just the following lines of code:</p> <pre><code>Q = X @ A_q.T + b_q.T  # &lt;- N x D matrix\nK = X @ A_k.T + b_k.T  # &lt;- N x D matrix\nV = X @ A_v.T + b_v.T  # &lt;- N x D matrix\n\nto_add = np.triu(-np.inf * np.ones((N, N)), k=1)\npre_softmax = Q@K.T + to_add\nattention = softmax_cols(pre_softmax)\nout = attention @ V\n\nassert (out == np.array(all_outs).squeeze()).all()  # &lt;- shows equivalence\n</code></pre>"},{"location":"blog/2024/06/13/masking-in-transformers/#masking-in-the-hf-library","title":"Masking in the HF library","text":"<p>Now that you know the basics of how masking is supposed to work, let's take a look at how the Huggingface library implements this, specifically in the BERT modules. </p> <p>Let's set up this toy example: </p> <pre><code>from transformers import BertTokenizerFast\ntokenizer = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-uncased\")\n\nwords = [\n    [\"word1*word2|word3\"],\n    [\"word1*\", \"word2|\", \"word3\"],\n]\ntokenizer_out = tokenizer(\n    text=words,\n    is_split_into_words=True,\n    return_offsets_mapping=True,\n    return_overflowing_tokens=True,\n    truncation=\"longest_first\",\n    padding=\"max_length\",\n    return_tensors=\"pt\",\n    max_length=512,\n    stride=0,\n    return_length=True,\n)\n# Input IDs and attention mask\ninput_ids = tokenizer_out[\"input_ids\"]\nattention_mask = tokenizer_out[\"attention_mask\"]\n</code></pre> <p>If you look at what input_ids and the attention_mask are:  <pre><code>input_ids\ntensor([[ 101, 2773, 2487, 1008, 2773, 2475, 1064, 2773, 2509,  102,    0, 0, ...],\n        [ 101, 2773, 2487, 1008, 2773, 2475, 1064, 2773, 2509,  102,    0, 0, ...]])\n\nattention_mask\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...]])\n</code></pre></p> <p>The <code>1</code>s in the <code>attention_mask</code> are used to indicate tokens that should be attended to, while the <code>0</code>s are used to indicate tokens that you don't want the model to attend to.</p> <p>Where does this attention mask get computed in the <code>BertTokenizerFast</code>? You have to chase down the <code>return_offsets_mapping</code>, and you will go all the way to the <code>_batch_encode_plus</code> method of the <code>PreTrainedTokenizerFast</code> class, which calls <code>self._tokenzier.encode_batch</code>. As part of the <code>fast</code> implementation of tokenizers, this method is implemented in Rust for performance reasons!</p> <p>You can still follow the logic of where the attention mask is computed in the non-fast implementation called <code>BertTokenizer</code>. To do so, you'll have to chase down the functions in the <code>forward</code> method all the way till you reach the <code>_pad</code> method of the <code>PreTrainedTokenizerBase</code> class, where you can see how Huggingface deals with the various padding strategies available. In the snippet above, we chose <code>max_length</code>, the relevant parts of the code are: </p> <pre><code># Initialize attention mask if not present.\n    if return_attention_mask and \"attention_mask\" not in encoded_inputs:\n        encoded_inputs[\"attention_mask\"] = [1] * len(required_input)\n    difference = max_length - len(required_input)\n    encoded_inputs[\"attention_mask\"] = encoded_inputs[\"attention_mask\"] + [0] * difference\n</code></pre> <p>We next turn our attention to the <code>forward</code> method of the <code>BertModel</code> to see what is going on. You can go through the forward pass with -</p> <pre><code>model = BertModel.from_pretrained(model_name)\n\n# Inference with the mask\nmodel.eval()\noutputs = model(input_ids, attention_mask=attention_mask)\n</code></pre> <p>Once it enters the <code>forward</code> method, you'll see that there's a method called <code>get_extended_attention_mask</code> that is part of the <code>BertModel</code> (actually its inheritance is a bit more complicated - it is defined in the <code>ModuleUtilsMixin</code> and the chain of inheritance is something like <code>ModuleUtilsMixin</code> -&gt; <code>PreTrainedModel</code> -&gt; <code>BertPreTrainedModel</code> -&gt; <code>BertModel</code>, where the <code>-&gt;</code> is shorthand for \"is inherited by\").</p> <p>The conversion of this array of 0s and 1s to the form of <code>to_add</code> is done here in this line:</p> <pre><code>extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(dtype).min\n</code></pre> <p>where the 1s in the <code>attention_mask</code> get converted to 0s and the <code>0</code>s get converted into a large negative value (like the <code>-np.inf</code> in the code snippets I provided). </p> <p>This extended mask then gets passed trough <code>BertEncoder</code>, to <code>BertAttention</code> to <code>BertSelfAttention</code> where it is finally used in the <code>forward</code> method: </p> <pre><code>class BertSelfAttention(nn.Module):\n    ...\n    def forward(\n        self,\n        ...,\n        attention_mask,\n        ...,\n    ):\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        ...\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n        ...\n        return outputs\n</code></pre> <p>The <code>...</code> is there to get rid of all the \"noise\" that clutters our understanding of how masking is being used in the underlying torch modules in the HF library on Bert. </p> <p>As you can see, the structure is very similar to the code snippets that I wrote from scratch above!</p> <p>Hopefully, you now have more clarity about how this often overlooked (but important) part of any transformer block functions. </p>"},{"location":"blog/2024/07/02/rope-embeddings/","title":"RoPE embeddings","text":"<p>Rotary positional embeddings (RoPE) is yet another tool people use to improve the performance of BERT based models.</p> <p>For a description of what embeddings are, and how they get used in attention/transformers, feel free to take a look at a previous post - Self attention#Embedding - for more information. The main point of an embedding module is to serve as a lookup table, mapping various kinds of integer ids - token ids, position ids etc. - into vectors that will then get used in the attention mechanism.</p> <p>While token embeddings are one part of the puzzle, it is also important to inject the notion of a token's position in a sequence in the inputs to the attention mechanism. This injection is the subject of various methods in positional embeddings/encodings. </p> <p>Here we'll go through the following - </p> <ol> <li> <p>Quickly recap the following:</p> <ol> <li>The encoding scheme of proposed in the original transformers paper</li> <li>The embedding schema in the original BERT model<ol> <li>How it is implemented in HuggingFace</li> </ol> </li> </ol> </li> <li> <p>Rotary positional embeddings (RoPE)</p> <ol> <li>Brief description behind wanting to use relative positional information</li> <li>How it is implemented in code</li> </ol> </li> </ol>"},{"location":"blog/2024/07/02/rope-embeddings/#sinusoidal-embeddings-from-transformers-paper","title":"Sinusoidal embeddings from transformers paper","text":"<p>As a recap the positional encoding in the original transformers paper was:</p> \\[ \\text{PE}_{(pos, 2i)} = \\text{sin}(\\text{pos} \\times \\theta_{i}) \\] \\[ \\text{PE}_{(pos, 2i + 1)} = \\text{cos}(\\text{pos} \\times \\theta_{i}) \\] <p>where </p> \\[ \\theta_{i} = 10,000^{\\frac{-2i}{d}} \\] <p>and \\(d\\) is the dimension of the vector. </p> <p>To visualise these quantities, let's plot some of them and how they vary. Let's start with \\(\\theta_i\\) and how it varies with \\(i \\in [0, d]\\), with \\(d=64\\).   </p> Plotting code <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n\ndim = 64\n\nindices = np.arange(0, dim // 2)\nbase = 10_000\npower_term = np.power(base, -2 * indices / dim)\n\nplt.figure(figsize=(15,10))\nplt.ylabel(r'$\\mathrm{\\theta_i = 10000^{-\\frac{2i}{d}}}$')\nplt.xlabel(\"i\")\nplt.plot(power_term)\nplt.show()\n</code></pre> <p></p> <p>Now, let's see how \\(\\sin(m\\theta_i)\\) varies for different values of \\(m\\), restricting ourselves to \\(m=100\\). Recall that \\(m\\) here represents the positions of tokens. </p> Plotting code  <pre><code>fig, axs = plt.subplots(2, 2, figsize=(20, 15))\nfig.suptitle(\"Sine term for different values of m\", fontsize=16)\n\nm_values = [1, 10, 20, 99]  # Different values of m for each subplot\n\nfor i, ax in enumerate(axs.flat):\n    m = m_values[i]\n    ax.plot(np.sin(m * power_term))\n    ax.set_title(f\"m = {m}\")\n    ax.set_xlabel(\"i\")\n    ax.set_ylabel(r'$\\mathrm{\\sin(m \\times 10000^{-\\frac{2i}{d}})}$')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p> Each subplot here represents the positional encoding for a data input. For each subplot, the x-axis represents the \\(i^{th}\\) position of the \\(d\\) dimensional vector in position \\(m\\). I show these plots because it is not intuitive what the shape of a sinusoid of a exponential term would be. </p> <p>With this in mind, let us see a 2D plot of this encoding scheme, using  <code>seq_len = 10</code> and <code>dim=64</code>. </p> Plotting code <pre><code>def sinusoidal_embeddings(positions: np.array, dim: int, base=10000):\n    \"\"\"Interleaved sinusoidal position embeddings.\n\n    Like in the original transformer paper - interleaved sin and cos.\n    \"\"\"\n    indices = np.arange(0, dim // 2)\n    power_term = np.power(base, -2 * indices / dim)\n    angle = np.einsum(\"...,d-&gt;...d\", positions, power_term)\n    embeddings = np.stack([np.sin(angle), np.cos(angle)], axis=-1)\n    embeddings = embeddings.reshape(list(embeddings.shape[:-2]) + [-1])\n    return embeddings\n\nseq_len = 10\npositions = np.arange(seq_len)\nembeddings = sinusoidal_embeddings(positions=positions, dim=dim)\n\nplt.figure(figsize=(12,8))\nplt.pcolormesh(embeddings, cmap='viridis')\nplt.xlabel('Embedding Dimensions')\nplt.xlim((0, dim))\nplt.ylim((0, seq_len))\nplt.ylabel('Token position')\nplt.colorbar()\nplt.show()\n</code></pre> <p></p> <p>Each row represents the encoding that will be added to the embedding of a token.</p> <p>But even this plot is not useful in visualising how this \"helps\" in injecting position information into the inputs. One visualisation that may help is by looking at the Euclidean distance of each of the vectors to a vector at a particular position index. The plot below can provide some intution (here <code>seq_len</code> is 100 again):</p> Plotting Code <pre><code>seq_len = 10\npositions = np.arange(seq_len)\nembeddings = sinusoidal_embeddings(positions=positions, dim=dim)\n\nfig, axs = plt.subplots(2, 2, figsize=(20, 15))\nfig.suptitle(\"Euclidean of all vectors to a vector at a particular position\", fontsize=16)\n\nvector_idxes = [0, 10, 20, 75]  # different position for each subplot\n\nfor i, ax in enumerate(axs.flat):\n    idx = vector_idxes[i]\n    distances = np.linalg.norm(embeddings - embeddings[idx], axis=1)\n    ax.plot(np.arange(distances.shape[0]), distances, marker='o')\n    ax.set_title(f'Distance of Vectors to the {idx}th Vector')\n    ax.set_xlabel('Vector Index')\n    ax.set_ylabel('Euclidean Distance')\n    ax.grid(True)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>Here you can see that these graphs have a \"V\" shape - for a given vector, vectors that are close to it positionally generally have a smaller Euclidean distance than those that are further away positionally. </p> <p>You can also look at the \"frequency\" of the sinusoids at each \\(i\\) as \\(m\\) varies:</p> Plotting Code <pre><code>fig, axs = plt.subplots(2, 2, figsize=(20, 15))\nfig.suptitle(\"Frequency variation for each i as m varies\", fontsize=16)\n\ndim_idxes = [1, 10, 20, 30]  # Different values of m for each subplot\n\nfor i, ax in enumerate(axs.flat):\n    dim_idx = dim_idxes[i]\n    ax.plot(embeddings[:, dim_idx])\n    ax.set_title(f'i = {dim_idx}')\n    ax.set_xlabel('m')\n    ax.set_ylabel(r'$\\mathrm{\\theta_i = 10000^{-\\frac{2i}{d}}}$')\n    ax.grid(True)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>This makes sense because the frequency of the sinusoid is \\(\\theta_i\\). \\(\\theta_i\\) has a higher value for when \\(i\\) is smaller, and a smaller value when \\(i\\) is larger. </p>"},{"location":"blog/2024/07/02/rope-embeddings/#what-are-the-limitations-of-of-the-sinusoidal-scheme-proposed-in-the-original-transformers-paper","title":"What are the limitations of of the sinusoidal scheme proposed in the original transformers paper?","text":"<p>The most obvious issues are as listed: 1. The main problem with the scheme presented above is that everything is fixed. None of the components - the variation in the frequency range, the sinusoidal function itself etc. - gets updated during training.  2. The different sinusoids, while expressive, may not be the correct kind of expressive for all sequences and may not be capturing the complex positional relationships in the data. 3. For longer sequence lengths, you can see that the difference blurs for vectors that are further away. </p>"},{"location":"blog/2024/07/02/rope-embeddings/#bert-embeddings","title":"BERT embeddings","text":"<p>The BERT paper uses learned positional embeddings (though the paper doesn't explicitly state it). The main idea is this - instead of using predefined sinusoids like the original transformers paper used, the BERT paper defers the learning of how best to deal with a token's position to the model itself, so that the model learns this in conjunction with the other weights. </p> <p>Yet another embedding layer is defined, and instead of serving as a look up table for token ids to \\(d\\) dimensional vectors, this embedding layer serves as a look up table for a token's position in the sequence to a \\(d\\) dimensional vector.</p> <p>That is it, there's nothing more complicated. Let's quickly look at how the HuggingFace library does it.</p>"},{"location":"blog/2024/07/02/rope-embeddings/#bert-positional-embeddings-in-the-huggingface-library","title":"BERT positional embeddings in the HuggingFace library","text":"<p>Let's start with the <code>BertEmbeddings</code> class to see what happens. In the <code>BertEmbeddings</code> class, the <code>self.position_ids</code> is basically a 1 dimensional tensor that goes from <code>0</code> to <code>max_tokens - 1</code>. In the default case, <code>max_tokens</code> is 512, so <code>self.position_ids</code> is a \\(1 \\times 512\\) <code>tensor([[ 0, 1, 2, 3, ... 511]])</code> (in code, this happens here). Remember that for BERT, the number of input tokens is fixed, and shorter token sequences are padded.  In the <code>forward</code> method of <code>BertEmbeddings</code>, you can see that this tensor is then passed through a <code>torch.nn.Embedding</code> module, which is nothing more than a look up table casting ints to vectors. The result is then added to the token embeddings.</p> <pre><code>class BertEmbeddings(nn.Module):\n    def __init__(self, config):\n        ...\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n        ...\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n\n        # same as self.position_ids = torch.arange(config.max_position_embeddings).expand((1, -1))\n        self.register_buffer(\n            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n        )\n\n        ...\n\n    def forward(self, input_ids, ...):\n        ...\n        inputs_embeds = self.word_embeddings(input_ids)\n        ...\n        position_embeddings = self.position_embeddings(position_ids)\n\n        embeddings += position_embeddings  # &lt;- main line here\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n</code></pre> <p>That is basically how embedding is done in <code>BertEmbeddings</code>.  As an aside, let's also quickly look at how the output of this module is used. Looking at the <code>forward</code> method of the <code>BertModel</code> in Huggingface, you'll see that these embedding outputs become the <code>hidden_state</code>:</p> <p>In <code>BertModel</code>:</p> <pre><code>def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.Tensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -&gt; Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n        ...\n        encoder_outputs = self.encoder(\n                embedding_output,  # &lt;- this is \"hidden_state\" for the encoder\n                attention_mask=extended_attention_mask,\n                head_mask=head_mask,\n                encoder_hidden_states=encoder_hidden_states,\n                encoder_attention_mask=encoder_extended_attention_mask,\n                past_key_values=past_key_values,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n        )\n</code></pre> <p>and then you go into the <code>BertEncoder</code>:  <pre><code>def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = False,\n        output_hidden_states: Optional[bool] = False,\n        return_dict: Optional[bool] = True,\n    ) -&gt; Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n        ...\n        layer_outputs = layer_module(\n                hidden_states,  # &lt;- this contains the embeddings\n                attention_mask,\n                layer_head_mask,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                past_key_value,\n                output_attentions,\n        )\n</code></pre></p> <p>and now go all the way into the <code>BertSelfAttention</code> and investigate it line by line:</p> <pre><code>class BertSelfAttention(nn.Module):\n    def __init__(self, config, position_embedding_type=None):\n        super().__init__()\n        ...\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n\n        def forward(\n                self,\n                hidden_states: torch.Tensor,  # &lt;- this contains the embeddings, used as input to the attention mechanism\n                attention_mask: Optional[torch.FloatTensor] = None,\n                head_mask: Optional[torch.FloatTensor] = None,\n                encoder_hidden_states: Optional[torch.FloatTensor] = None,\n                encoder_attention_mask: Optional[torch.FloatTensor] = None,\n                past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n                output_attentions: Optional[bool] = False,\n        ) -&gt; Tuple[torch.Tensor]:\n                mixed_query_layer = self.transpose_for_scores(self.query(hidden_states))\n                ...\n                key_layer = self.transpose_for_scores(self.key(hidden_states))\n                value_layer = self.transpose_for_scores(self.value(hidden_states))\n</code></pre>"},{"location":"blog/2024/07/02/rope-embeddings/#rope-embeddings","title":"RoPE embeddings","text":""},{"location":"blog/2024/07/02/rope-embeddings/#background","title":"Background","text":"<p>In the embedding/encoding schemes above, the model has to learn the absolute position embeddings. But this is often not very useful under the following circumstances:</p> <ol> <li>Often times when training on some datasets, shorter unrelated token sequences are stacked together and trained on together. Here absolute positional embeddings make no sense because the actual first token of a sentence would have some arbitrary positional information based on what came before it.</li> <li>Other times, long token sequences are broken up into shorter sequences before passing through the attention layers. In this scenario absolute positions don't correspond to the actual positions of the tokens in the original sequences.</li> <li>In the standard attention mechanism, the dot product no longer cares about the positions of tokens. This means the attention mechanism is only ever indirectly dependent on the position of tokens because the positional embeddings are added to the token embeddings before the matrix multiplication.</li> </ol> <p></p> <p>For such cases, we want an embedding scheme that efficiently does some kind of relative positional embedding, and also explicitly \"injects\" this knowledge into the attention mechanism. What does relative positional embedding mean? We basically want a function \\(f(\\mathbf{x}, l)\\) that takes in as arguments an input token sequence \\(\\mathbf{x}\\) and its position \\(l\\) such that the dot product between 2 vectors \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) in positions \\(l_1\\) and \\(l_2\\) is only sensitive to \\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\), and the relative position \\(l_1 - l_2\\). </p> <p>There are many ways to do this, but the scheme that most of the field has settled on was first proposed in the RoFormer paper, which we talk about in the next section.</p>"},{"location":"blog/2024/07/02/rope-embeddings/#what-are-rope-embeddings","title":"What are RoPE embeddings?","text":"<p>The RoFormer paper, and the EleutherAI blogpost that the paper mentions, explain the intuition behind rotational positional embeddings in detail. But both contains a lot of detail that obfuscated the essence (the \"Visual Intuition\" section of the blogpost continues to confuse me) for me.</p> <p>If you have found yourself on the same boat, below is hopefully a simpler and more practical explanation of it.</p> <p>The easiest way to to preserve the notion of some kind of \"relative\" distance between two token embeddings in the pre-softmax attention step, is to make use of the angle between them in their \\(d\\) dimensional space. This is because the dot product of two vectors is proportional to the cosine of the angle between them, and the pre-softmax matrix multiplication step is nothing but a series of dot products. </p> <p>A cunning way to do so would be to multiply the token embedding in the \\(m^{th}\\) position in the sequence by the following rotational matrix:</p> \\[ \\mathbf{R}_{d, m} = \\tiny{\\begin{bmatrix} \\cos(m\\theta_1) &amp; - \\sin(m\\theta_1) &amp; 0 &amp; 0 &amp; ...  &amp; 0 &amp; 0 \\\\ \\sin(m\\theta_1) &amp; \\cos(m\\theta_1) &amp; 0 &amp; 0 &amp; ...  &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\cos(m\\theta_2) &amp; - \\sin(m\\theta_2) &amp; ...  &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sin(m\\theta_2) &amp; \\cos(m\\theta_2) &amp; ...  &amp; 0 &amp; 0 \\\\ ... \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; ...  &amp; \\cos(m\\theta_{d/2}) &amp; - \\sin(m\\theta_{d/2}) \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; ...  &amp; \\sin(m\\theta_{d/2}) &amp; \\cos(m\\theta_{d/2}) \\\\ \\end{bmatrix}} \\] <p>where </p> \\[ \\theta_i = 10,000 ^{\\frac{-2i}{d}} \\] <p>i.e. the original definition of the angle as in the transformer paper.</p> <p>So now, your key values become: </p> \\[ \\mathbf{k}'_{m} = \\mathbf{R}_{d, m}\\mathbf{k}_{m} \\] <p>and your query values become </p> \\[ \\mathbf{q}'_{n} = \\mathbf{R}_{d, n}\\mathbf{q}_{n} \\] <p>These are now inputs to your attention mechanism. To see why multiplying your keys and queries with this rotational matrix is cunning, let's see what happens when you take the dot product of a key vector in position \\(m\\) and a query vector in position \\(n\\) (as is usual in the attention mechanism):</p> \\[ \\mathbf{k'}^{T}_{m}\\mathbf{q'}_{n} = \\mathbf{k}^T_{m}\\mathbf{R}^T_{d, m}\\mathbf{R}_{d, n}\\mathbf{q}_{n} \\] <p>The quantity \\(\\mathbf{R}^T_{d, m}\\mathbf{R}_{d, n}\\) is nothing but another rotation matrix! You can see the derivation pretty simply:</p> \\[ \\mathbf{R}^T_{d, m}\\mathbf{R}_{d, n} =  \\] \\[ \\tiny{ \\begin{bmatrix} \\cos(m\\theta_1) &amp;  \\sin(m\\theta_1) &amp; 0 &amp; 0 &amp; ...  &amp; 0 &amp; 0 \\\\ - \\sin(m\\theta_1) &amp; \\cos(m\\theta_1) &amp; 0 &amp; 0 &amp; ...  &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\cos(m\\theta_2) &amp;  \\sin(m\\theta_2) &amp; ...  &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -\\sin(m\\theta_2) &amp; \\cos(m\\theta_2) &amp; ...  &amp; 0 &amp; 0 \\\\ ... \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; ...  &amp; \\cos(m\\theta_{d/2}) &amp;  \\sin(m\\theta_{d/2}) \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; ...  &amp; -\\sin(m\\theta_{d/2}) &amp; \\cos(m\\theta_{d/2}) \\\\ \\end{bmatrix} \\begin{bmatrix} \\cos(n\\theta_1) &amp; - \\sin(n\\theta_1) &amp; 0 &amp; 0 &amp; ...  &amp; 0 &amp; 0 \\\\ \\sin(n\\theta_1) &amp; \\cos(n\\theta_1) &amp; 0 &amp; 0 &amp; ...  &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\cos(n\\theta_2) &amp; - \\sin(n\\theta_2) &amp; ...  &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sin(n\\theta_2) &amp; \\cos(n\\theta_2) &amp; ...  &amp; 0 &amp; 0 \\\\ ... \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; ...  &amp; \\cos(n\\theta_{d/2}) &amp; - \\sin(n\\theta_{d/2}) \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; ...  &amp; \\sin(n\\theta_{d/2}) &amp; \\cos(n\\theta_{d/2}) \\\\ \\end{bmatrix} } \\] \\[ = \\tiny{\\begin{bmatrix} \\cos(m\\theta_1)\\cos(n\\theta_1) + \\sin(m\\theta_1)\\sin(n\\theta_1) &amp;  -\\cos(m\\theta_1)\\sin(n\\theta_1) + \\sin(m\\theta_1)\\cos(n\\theta_1) &amp; 0 &amp; 0 &amp; ...  &amp; 0 &amp; 0 \\\\ - \\sin(m\\theta_1)\\cos(n\\theta_1) + \\cos(m\\theta_1)\\sin(n\\theta_1) &amp; \\cos(m\\theta_1)\\cos(n\\theta_1) + \\sin(m\\theta_1)\\sin(n\\theta_1) &amp; 0 &amp; 0 &amp; ...  &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; ... &amp;  ... &amp; ...  &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; ... &amp; ... &amp; ...  &amp; 0 &amp; 0 \\\\ ... \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; ...  &amp; \\cos(m\\theta_{d/2})\\cos(n\\theta_{d/2}) + \\sin(m\\theta_{d/2})\\sin(n\\theta_{d/2}) &amp;  -\\cos(m\\theta_{d/2})\\sin(n\\theta_{d/2}) + \\sin(m\\theta_{d/2})\\cos(n\\theta_{d/2})\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; ...  &amp; - \\sin(m\\theta_{d/2})\\cos(n\\theta_{d/2}) + \\cos(m\\theta_{d/2})\\sin(n\\theta_{d/2}) &amp; \\cos(m\\theta_{d/2})\\cos(n\\theta_{d/2}) + \\sin(m\\theta_{d/2})\\sin(n\\theta_{d/2})\\\\ \\end{bmatrix} } \\] \\[ = \\tiny{\\begin{bmatrix} \\cos(n-m)\\theta_1 &amp; - \\sin(n-m)\\theta_1 &amp; 0 &amp; 0 &amp; ...  &amp; 0 &amp; 0 \\\\ \\sin(n-m)\\theta_1 &amp; \\cos(n-m)\\theta_1 &amp; 0 &amp; 0 &amp; ...  &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\cos(n-m)\\theta_2 &amp; - \\sin(n-m)\\theta_2 &amp; ...  &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sin(n-m)\\theta_2 &amp; \\cos(n-m)\\theta_2 &amp; ...  &amp; 0 &amp; 0 \\\\ ... \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; ...  &amp; \\cos(n-m)\\theta_{d/2} &amp; - \\sin(n-m)\\theta_{d/2} \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; ...  &amp; \\sin(n-m)\\theta_{d/2} &amp; \\cos(n-m)\\theta_{d/2} \\\\ \\end{bmatrix} } \\] \\[ = \\mathbf{R}_{d, n-m} \\] <p>So we now basically have a way of \"injecting\" the relative distance between 2 tokens in a sequence into the attention mechanism!  All this theory still does not completely explain exactly how this leads to better training, as is common with all transformers behaviour. But the common consensus in the field has been that this injection, and corresponding multiplicative and sinusoidal relationship between relative position and the learned weights, makes it \"easier\" (more data efficient) for the model to learn sequences - a consensus that has been proved empirically over and over again.</p> <p>Now, let's see some basic plots of this. </p> <p>First, here's the code to generate \\(\\mathbf{R}_{d, m}\\) </p> <pre><code>def get_rot_matrix(dim: int, m: int):\n    # same as \n    # indices = np.arange(0, dim // 2)\n    # base = 10_000\n    # theta = np.power(base, -2 * indices / dim)\n    theta = 1.0 / (10000 ** (np.arange(0, dim, 2) / dim))\n    theta = m * theta\n    cos_theta = np.cos(theta)\n    sin_theta = np.sin(theta)\n    main_diagonal = np.repeat(cos_theta, 2)\n    rot = np.diag(main_diagonal)\n    off_diagonal = np.zeros(dim)\n    off_diagonal[::2] = sin_theta\n    rot += np.diag(-off_diagonal[:-1], k=1)\n    rot += np.diag(off_diagonal[:-1], k=-1)\n    return rot\n</code></pre> <p>To see how this affects the pre-softmax layer, let's take 2 identity vectors of dimension \\(d=64\\),</p> <pre><code>dim = 64\nk = np.ones((dim, 1))\nq = np.ones((dim, 1))\n</code></pre> <p>Without the rotation matrix, the value of \\(\\mathbf{k}_m^T\\mathbf{q}_n\\) has a constant value of 64 regardless of the value of \\(m\\) and \\(n\\). With the rotational matrix, lets's see how the value of \\(\\mathbf{k}^T_{m}\\mathbf{R}_{d, n-m}\\mathbf{q}_{n}\\) varies with \\(n-m\\) with the following plot:</p> Plotting code <pre><code>x = np.arange(0, 100)\nplt.plot(x, [(k.T@get_rot_matrix(dim=dim, m=_m)@q).item() for _m in x])\nplt.xlabel(r'Relative distance $\\mathrm{n - m}$')\nplt.ylabel(r'$\\mathbf{k}^T_{m}\\mathbf{R}_{d, n-m}\\mathbf{q}_{n}$')\nplt.axhline(y=64, color='r', linestyle=':')\nplt.text(x=70, y=62, s=\"y=64\", color='r', fontsize=12)\n</code></pre> <p></p> <p>This by itself doesn't actually show the mechanics of what will happen during training. During training, the vectors \\(\\mathbf{k}_m\\) and \\(\\mathbf{q}_n\\) are themselves the result of multiplying a weight vector \\(\\mathbf{W}_k\\) (for the key values), \\(\\mathbf{W}_q\\) (for the query values) to the token embedding at positions \\(m\\) and \\(n\\) respectively. This means that the pre-softmax value is actually </p> \\[ \\mathbf{k}^T_{m}\\mathbf{R}_{d, n-m}\\mathbf{q}_{n} = \\mathbf{x}_m^T\\mathbf{W}_k^T\\mathbf{R}_{d, n-m}\\mathbf{W}_q\\mathbf{x}_n \\] <p>Where \\(\\mathbf{W}_k, \\mathbf{W}_q\\) are learned weights. This means that the exact nature of the sinusoidal patterns we see in the plot as \\(n-m\\) increases can get very finely tuned during the training process.</p>"},{"location":"blog/2024/07/02/rope-embeddings/#implementation-in-practice","title":"Implementation in practice","text":"<p>So far, the code we have looked at is at a vector level. Now let's see how this would get implemented when you have to do it for an entire sequence of tokens. </p> <p>The query matrix \\(\\mathbf{Q} \\in \\mathbb{R}^{N \\times d}\\), where \\(N\\) is the number of tokens in the sequence and \\(d\\) is the dimension of each data point, can be represented as:</p> \\[ \\mathbf{Q} = \\left[ \\begin{array}{c} \\hphantom{-}\\whitetextemdash q_0^T\\whitetextemdash \\hphantom{-} \\\\ \\hphantom{-}\\whitetextemdash q_1^T\\whitetextemdash \\hphantom{-} \\\\ \\hphantom{-}\\whitetextemdash q_2^T\\whitetextemdash \\hphantom{-} \\\\ ...\\\\ \\hphantom{-}\\whitetextemdash q_{N-1}^T\\whitetextemdash \\hphantom{-} \\\\ \\end{array} \\right] \\] <p>where \\(q_i \\in \\mathbb{R}^d, q_i = \\mathbf{W}_q\\mathbf{x}_i\\) represents the query vector of the embedding of the token in the \\(i^{th}\\) position \\(\\mathbf{x}_i\\).</p> <p>Similarly for the key values:</p> \\[ \\mathbf{K} = \\left[ \\begin{array}{c} \\hphantom{-}\\whitetextemdash k_0^T \\whitetextemdash \\hphantom{-} \\\\ \\hphantom{-}\\whitetextemdash k_1^T \\whitetextemdash \\hphantom{-} \\\\ \\hphantom{-}\\whitetextemdash k_2^T \\whitetextemdash \\hphantom{-} \\\\ ...\\\\ \\hphantom{-}\\whitetextemdash k_{N-1}^T \\whitetextemdash \\hphantom{-} \\\\ \\end{array} \\right] \\] <p>Let's focus only on the query matrix for now. After applying the rotational matrix to each of the vectors, let's call the result \\(\\mathbf{Q'}\\):</p> \\[ \\mathbf{Q'} = \\left[ \\begin{array}{c} \\hphantom{-}\\whitetextemdash q_0^{,T}\\whitetextemdash \\hphantom{-} \\\\ \\hphantom{-}\\whitetextemdash q_1^{,T}\\whitetextemdash \\hphantom{-} \\\\ \\hphantom{-}\\whitetextemdash q_2^{,T}\\whitetextemdash \\hphantom{-} \\\\ ...\\\\ \\hphantom{-}\\whitetextemdash q_{N-1}^{,T}\\whitetextemdash \\hphantom{-} \\\\ \\end{array} \\right] \\] \\[ = \\left[ \\begin{array}{c} \\hphantom{-}\\whitetextemdash \\mathbf{R}^T_{d, 0}q_0^T\\whitetextemdash \\hphantom{-} \\\\ \\hphantom{-}\\whitetextemdash \\mathbf{R}^T_{d, 1}q_1^T\\whitetextemdash \\hphantom{-} \\\\ \\hphantom{-}\\whitetextemdash \\mathbf{R}^T_{d, 2}q_2^T\\whitetextemdash \\hphantom{-} \\\\ ...\\\\ \\hphantom{-}\\whitetextemdash \\mathbf{R}^T_{d, N-1}q_{N-1}^T\\whitetextemdash \\hphantom{-} \\\\ \\end{array} \\right] \\] <p>The \"naive\" way to get the rotary positional embeddings of the key/query matrix is simply - </p> <ol> <li>Multiplying each row \\(\\mathbf{q}_i\\) with the rotational matrix \\(\\mathbf{R}_{d, i}\\). </li> <li>Setting the result of the previous step as the \\(i^{th}\\) row of the resultant matrix.</li> </ol> <p>In naive python, the code would be like this:</p> <pre><code>np.random.seed(3)\nQ = np.random.randn(5, 4)\nQ_prime = np.zeros_like(Q)\nfor pos, q_i in enumerate(Q):\n    rot = get_rot_matrix(Q_prime.shape[-1], pos)\n    Q_prime[pos] = rot @ q_i.T\nQ_prime\n</code></pre> <p>and the result you get is <pre><code>array([[ 1.78862847,  0.43650985,  0.09649747, -1.8634927 ],\n       [ 0.1486459 , -0.42509122, -0.07646744, -0.62779673],\n       [ 0.45216792,  0.15874903, -1.33129326,  0.85816992],\n       [-1.11375321, -1.5680929 ,  0.06214963, -0.40299454],\n       [-0.81390684,  1.4235748 ,  1.02561261, -1.06090267]])\n</code></pre></p> <p>But there's a more cunning way of doing everything in a vectorised way and getting rid of the loop in the code snippet. Let's take a closer look at the multiplication of the rotational matrix with a query vector:</p> \\[ \\mathbf{R}_{d, m}\\mathbf{q}_m = \\tiny{\\begin{bmatrix} \\cos(m\\theta_1) &amp; - \\sin(m\\theta_1) &amp; 0 &amp; 0 &amp; ...  &amp; 0 &amp; 0 \\\\ \\sin(m\\theta_1) &amp; \\cos(m\\theta_1) &amp; 0 &amp; 0 &amp; ...  &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\cos(m\\theta_2) &amp; - \\sin(m\\theta_2) &amp; ...  &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sin(m\\theta_2) &amp; \\cos(m\\theta_2) &amp; ...  &amp; 0 &amp; 0 \\\\ ... \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; ...  &amp; \\cos(m\\theta_{d/2}) &amp; - \\sin(m\\theta_{d/2}) \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; ...  &amp; \\sin(m\\theta_{d/2}) &amp; \\cos(m\\theta_{d/2}) \\\\ \\end{bmatrix} \\begin{bmatrix} q^m_1 \\\\ q^m_2 \\\\ ... \\\\ q^m_{d-1} \\\\ q^m_d \\\\ \\end{bmatrix} } \\] <p>(let's drop the \\(m\\) superscript showing the sequence position on the individual elements of the \\(\\mathbf{q}\\) vector)</p> \\[ =\\tiny{ \\begin{bmatrix} \\cos({m\\theta_1})q_1 - \\sin({m\\theta_1})q_2 \\\\ \\sin({m\\theta_1})q_1 + \\cos({m\\theta_1})q_2 \\\\ \\cos({m\\theta_2})q_3 - \\sin({m\\theta_2})q_4 \\\\ \\sin({m\\theta_2})q_3 + \\cos({m\\theta_2})q_4 \\\\ ... \\\\ \\cos({m\\theta_{d/2}})q_{d-1} - \\sin({m\\theta_{d/2}})q_d \\\\ \\sin({m\\theta_{d/2}})q_{d-1} + \\cos({m\\theta_{d/2}})q_d \\\\ \\end{bmatrix} } \\] \\[ = \\tiny{ \\begin{bmatrix} \\cos({m\\theta_1}) \\\\ \\cos({m\\theta_1}) \\\\ \\cos({m\\theta_2}) \\\\ \\cos({m\\theta_2}) \\\\  ... \\\\ \\cos({m\\theta_{d/2}}) \\\\ \\cos({m\\theta_{d/2}}) \\end{bmatrix} \\odot \\begin{bmatrix} q_1 \\\\ q_2 \\\\ q_3 \\\\ q_4 \\\\ ... \\\\ q_{d-1} \\\\ q_d \\\\ \\end{bmatrix} } \\] \\[ + \\] \\[  \\tiny{ \\begin{bmatrix} \\sin({m\\theta_1}) \\\\ \\sin({m\\theta_1}) \\\\ \\sin({m\\theta_2}) \\\\ \\sin({m\\theta_2}) \\\\  ... \\\\ \\sin({m\\theta_{d/2}}) \\\\ \\sin({m\\theta_{d/2}}) \\end{bmatrix} \\odot \\begin{bmatrix} - q_2 \\\\ q_1 \\\\ - q_4 \\\\ q_3 \\\\ ... \\\\ - q_d \\\\ q_{d - 1} \\\\ \\end{bmatrix} } \\] <p>where \\(\\odot\\) is the element-wise multiplication/Hadamard product.</p> <p>So now, </p> \\[ \\mathbf{Q'} = \\left[ \\begin{array}{c} \\hphantom{-}\\whitetextemdash q_0^{,T}\\whitetextemdash \\hphantom{-} \\\\ \\hphantom{-}\\whitetextemdash q_1^{,T}\\whitetextemdash \\hphantom{-} \\\\ \\hphantom{-}\\whitetextemdash q_2^{,T}\\whitetextemdash \\hphantom{-} \\\\ ...\\\\ \\hphantom{-}\\whitetextemdash q_{N-1}^{,T}\\whitetextemdash \\hphantom{-} \\\\ \\end{array} \\right] \\] \\[ = \\tiny{ \\begin{bmatrix} \\cos({0\\theta_1}) &amp; \\cos({0\\theta_1}) &amp; \\cos({0\\theta_2}) &amp; \\cos({0\\theta_2}) &amp;  ... &amp; \\cos({0\\theta_{d/2}}) &amp; \\cos({0\\theta_{d/2}}) \\\\ \\cos({1\\theta_1}) &amp; \\cos({1\\theta_1}) &amp; \\cos({1\\theta_2}) &amp; \\cos({1\\theta_2}) &amp;  ... &amp; \\cos({1\\theta_{d/2}}) &amp; \\cos({1\\theta_{d/2}}) \\\\ . \\\\ .\\\\ . \\\\ \\cos({(N-1)\\theta_1}) &amp; \\cos({(N-1)\\theta_1}) &amp; \\cos({(N-1)\\theta_2}) &amp; \\cos({(N-1)\\theta_2}) &amp;  ... &amp; \\cos({(N-1)\\theta_{d/2}}) &amp; \\cos({(N-1)\\theta_{d/2}}) \\\\ \\end{bmatrix} \\odot  \\left[ \\begin{array}{c} \\hphantom{-}\\whitetextemdash q_0^{,T}\\whitetextemdash \\hphantom{-} \\\\ \\hphantom{-}\\whitetextemdash q_1^{,T}\\whitetextemdash \\hphantom{-} \\\\ \\hphantom{-}\\whitetextemdash q_2^{,T}\\whitetextemdash \\hphantom{-} \\\\ ...\\\\ \\hphantom{-}\\whitetextemdash q_{N-1}^{,T}\\whitetextemdash \\hphantom{-} \\\\ \\end{array} \\right] } \\] \\[ + \\] \\[ \\tiny{ \\begin{bmatrix} \\sin({0\\theta_1}) &amp; \\sin({0\\theta_1}) &amp; \\sin({0\\theta_2}) &amp; \\sin({0\\theta_2}) &amp;  ... &amp; \\sin({0\\theta_{d/2}}) &amp; \\sin({0\\theta_{d/2}}) \\\\ \\sin({1\\theta_1}) &amp; \\sin({1\\theta_1}) &amp; \\sin({1\\theta_2}) &amp; \\sin({1\\theta_2}) &amp;  ... &amp; \\sin({1\\theta_{d/2}}) &amp; \\sin({1\\theta_{d/2}}) \\\\ . \\\\ .\\\\ . \\\\ \\sin({(N-1)\\theta_1}) &amp; \\sin({(N-1)\\theta_1}) &amp; \\sin({(N-1)\\theta_2}) &amp; \\sin({(N-1)\\theta_2}) &amp;  ... &amp; \\sin({(N-1)\\theta_{d/2}}) &amp; \\sin({(N-1)\\theta_{d/2}}) \\\\ \\end{bmatrix} \\odot  \\left[ \\begin{array}{c} \\hphantom{-}\\whitetextemdash \\text{rearranged-q}_0^{,T}\\whitetextemdash \\hphantom{-} \\\\ \\hphantom{-}\\whitetextemdash \\text{rearranged-q}_1^{,T}\\whitetextemdash \\hphantom{-} \\\\ \\hphantom{-}\\whitetextemdash \\text{rearranged-q}_2^{,T}\\whitetextemdash \\hphantom{-} \\\\ ...\\\\ \\hphantom{-}\\whitetextemdash \\text{rearranged-q}_{N-1}^{,T}\\whitetextemdash \\hphantom{-} \\\\ \\end{array} \\right] } \\] <p>where \\(\\text{rearranged-q}_i\\) is </p> \\[ \\begin{bmatrix} - q^i_2 \\\\ q^i_1 \\\\ - q^i_4 \\\\ q^i_3 \\\\ ... \\\\ - q^i_d \\\\ q^i_{d - 1} \\\\ \\end{bmatrix} \\] <p>So now, you can see pretty quickly that the way to implement rotary positional embeddings in a vectorised way is going to be something similar to:</p> <pre><code>from einops import repeat, rearrange\nimport numpy as np\n\n\ndef rotate_every_two(x):\n    \"\"\"\n    Similar to what EleutherAI's implementation of the MeshTransformer is in JAX\n    \"\"\"\n    x1 = x[:, ::2]\n    x2 = x[:, 1::2]\n    x = np.stack((-x2, x1), axis=-1)\n    return rearrange(x, \"... d j -&gt; ... (d j)\")\n\n\ndef apply_rotary_pos_emb(x: np.array, seq_idx=1, dim_idx=-1) -&gt; np.array:\n    \"\"\"apply rotary positional embedding\n\n    x in an input array, and the assumption that the shape is\n    (batch, seq_len, ..., dim)\n\n    Args:\n        x: the input array\n        seq_idx: the index in x.shape that shows the sequence length.\n            By default we assume it is 1\n        dim_idx: the index in x.shape that shows the number of dimensions\n            By default we assume it is -1\n\n    Returns:\n        a numpy array with rotary positional embedding applied\n    \"\"\"\n    dim = x.shape[dim_idx]\n    inv_freq = 1.0 / (10000 ** (np.arange(0, dim, 2) / dim))\n\n    sinusoid_inp = np.einsum(\"i , j -&gt; i j\", np.arange(x.shape[seq_idx]), inv_freq)\n    sines = np.sin(sinusoid_inp)\n    cosines = np.cos(sinusoid_inp)\n\n    def _apply_repeat(_arr: np.array) -&gt; np.array:\n        return repeat(_arr, \"b n -&gt; b (n j)\", j=2)\n    sin = _apply_repeat(sines)\n    cos = _apply_repeat(cosines)\n    return (x * cos) + (rotate_every_two(x) * sin)\n\napply_rotary_pos_emb(Q, seq_idx=0)\n</code></pre> <p>and the result is something like</p> <pre><code>array([[ 1.78862847,  0.43650985,  0.09649747, -1.8634927 ],\n       [ 0.1486459 , -0.42509122, -0.07646744, -0.62779673],\n       [ 0.45216792,  0.15874903, -1.33129326,  0.85816992],\n       [-1.11375321, -1.5680929 ,  0.06214963, -0.40299454],\n       [-0.81390684,  1.4235748 ,  1.02561261, -1.06090267]])\n</code></pre> <p>which is the same result you get using the \"naive\" python approach! This approach is also how the HuggingFace repository and other repositories implement rotary positional embeddings. </p> <p>Hopefully that has been a useful discourse in rotary positional embeddings!</p>"},{"location":"blog/2024/01/05/self-attention/","title":"How does self attention work?","text":"<p>Self-attention is the cornerstone of a transformer block. We are going to go through the intuition behind the following steps:</p> <ol> <li>Tokenization</li> <li>Embedding</li> <li>The self attention mechanism</li> </ol> <p>by using raw, non-vectorised Python code as much as possible.</p> <p>The original transformer paper here is actually easy to read for a technical paper, but difficult to wrap your head around if this is the first time you are dipping your toes into neural networks. Other resources, like \"The Illustrated Transformer\" don't really have code. One resource that does have code is the \"Understanding Deep Learning\" (UDL) book, that I will be leaning on heavily for this post.</p> <p>Before we get to the famous equation -</p> \\[ \\text{selfAttention}[X] = V[X]\\text{softmax}(\\frac{QK^T}{\\sqrt{D_q}}) \\] <p>let's see what the steps often are before a matrix/tensor gets to the self-attention mechanism. </p> <p>Many images are from the UDL book, and a lot of the code is covered in the notebooks  published alongside the book. </p> <p>Before diving into architectural details, we have to remind ourselves what the problem it is that transformers had set out to solve: translation </p>"},{"location":"blog/2024/01/05/self-attention/#brief-introduction-to-the-problem-of-translation","title":"Brief introduction to the problem of translation","text":"<p>Translation is the \"flagship\" natural language processing task. The general inputs and outputs of the problem are incredibly simple to describe - given a sequence of words in one language, return a sequence of words in another language automatically. </p> <pre><code>Input: I am a student.\n\nOutput: \u0986\u09ae\u09bf \u098f\u0995\u099c\u09a8 \u099b\u09be\u09a4\u09cd\u09b0\u0964\n</code></pre> <p>Before deep learning methods were used to tackle this problem, companies like Google used statistical, phrase-based systems that did not work that well, and did not scale to many languages.</p> <p>Nowadays, all dominant translation systems use neural networks to do their translation. </p>"},{"location":"blog/2024/01/05/self-attention/#how-do-you-train-a-neural-machine-translation-system","title":"How do you train a  neural machine translation system?","text":"<p>There are many approaches to neural machine translation, but what I'll cover here is sequence to sequence way the \"Attention is all you need\" paper did it.</p> <p>We'll walk through the following broad sections: 1. Tokenization 2. Embedding 3. Self attention</p>"},{"location":"blog/2024/01/05/self-attention/#tokenization-brief-remarks","title":"Tokenization - brief remarks","text":"<p>To use any neural based system, inputs and outputs must be vectors of floats.  The process of converting words/sequence of words into vectors of floats is called tokenization. Tokenization won't be covered in detail here, and there are several resources that go over the different techniques (my personal favourite is the series of lectures from Stanford). The main thing to remember for tokenization is the input and the output - (example is from HuggingFace): </p> <pre><code>from transformers import BertTokenizer\n\ninp = \"I love gpus!\"\n\ntokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n\ninp_str = \"I have a new GPU!\"\ntokenizer.tokenize(inp_str)\n\n# ['i', 'have', 'a', 'new', 'gp', '##u', '!'] =&gt; len= 7\n\n# Huggingface's tokenizers' .encode() method tends to add a special start and end tokens\n\ntokenizer.encode(inp_str)\n# each number here represents the index of each token in the tokenizer's entire vocabulary\n# [101, 1045, 2031, 1037, 2047, 14246, 2226, 999, 102] =&gt; len= 9 (7+2)\n\ntokenizer.decode([101, 1045, 2031, 1037, 2047, 14246, 2226, 999, 102])\n\n# '[CLS] i have a new gpu! [SEP]'\n\"\"\"\nCLS -&gt; Special CLASSIFY token usually added to the beginning of a sequence\nIn HF tokenizers, this is usually 101\n\nSEP -&gt; separator token, usually added at the end of a sequence\nIn HF tokenizers, this is usually 102\n\"\"\"\n</code></pre> <p>There are several popular tokenization schemes: the original Attention paper doesn't actually describe what tokenizing scheme it used, but it is commonly believed they used byte pair encoding (BPE). The GPT family of models also use some variation of BPE, see for example tiktoken, which does BPE at a byte level. Doing BPE at a byte level (as opposed to the more classic Unicode word/character level tokenization) means you don't really have the \"out of vocabulary\" problem when you move from one modality to another.</p> <p>Other popular tokenization methods are WordPiece (which is used in the BERT paper), and SentencePiece which can be used to tokenize multiple languages including non-Latin languages (which Google's T5 uses).</p> <p>Depending on your use case, you can also implement your own tokenization scheme! Before the transformers paper for example, a briefly popular paper was the ByteNet paper that did precisely this and implemented a character-level tokenization scheme. </p>"},{"location":"blog/2024/01/05/self-attention/#embedding","title":"Embedding","text":"<p>What happens after tokenziation? In the Huggingface transformer's library, after tokenisation, you get a sequence of \\(N\\) tokens, each of which is an integer that represents the index in the tokeniser's vocabulary. </p> <p>What we then want to do is convert each of these integers into a vector \\(\\in \\mathbb{R}^D\\) where \\(D\\) is some embedding dimension that is smaller than the size of the vocabulary in the tokenizer. This means that the sequence of integers would be converted into a sequence of vectors, i.e. a matrix.</p> <p>The way this is done in most architectures is through a simple look up table. If the size of the tokenizer's vocabulary is \\(N_{\\text{vocab}}\\) all we need is a \\(N_{\\text{vocab}} \\times D\\) matrix, where row \\(i\\) corresponds to the \\(D\\) dimensional representation of token index \\(i\\). This \\(N_{\\text{vocab}} \\times D\\) matrix is learnable. </p> <p>Let's see how it happens in code:</p> <p>Let's have a look at the inputs and outputs of <code>torch</code>'s <code>nn.Embedding</code> : <pre><code>import torch.nn as nn\nimport torch\nembedding_dim = 3\nvocab_size = 5\nembedding_layer = nn.Embedding(vocab_size, embedding_dim)\n\n# toy example - how would a sequence of ones [1., 1., ...] be embedded?\nout = embedding_layer(torch.ones((6,), dtype=int))\nout\n</code></pre> prints the following: <pre><code>tensor([\n        [-0.8257, 0.0528, 1.3637],\n        [-0.8257, 0.0528, 1.3637],\n        [-0.8257, 0.0528, 1.3637],\n        [-0.8257, 0.0528, 1.3637],\n        [-0.8257, 0.0528, 1.3637],\n        [-0.8257, 0.0528, 1.3637]],\n        grad_fn=&lt;EmbeddingBackward0&gt;)\n</code></pre> a 6x3 matrix. If you want to dig further into the nature of the embedding layer, you can see the following:</p> <pre><code>embedding_layer.weight\n\ntensor([\n    [ 0.2668, -1.0410, -1.5245],\n    [-0.8257, 0.0528, 1.3637],\n    [-1.7534, -0.4505, -1.0951],\n    [-0.6984, -1.7775, -1.3832],\n    [ 2.5235, -0.7539, -2.1454]], requires_grad=True)\n</code></pre> <p>The output of  <code>embedding_layer(torch.ones((6,), dtype=int))</code> is basically the 2nd row (index of 1) <code>[-0.8257, 0.0528, 1.3637]</code> repeated 6 times. So all the embedding layer \\(N_{\\text{vocab}} \\times D\\) does is act as a look-up matrix.</p> <p>In pure python, the code for the embedding layer will be something as simple as:</p> <pre><code>import random \n\nclass PurePythonEmbeddingLayer:\n    def __init__(self, vocab_size: int, embedding_dim: int):\n        self.weight = [[random.random() for _ in range(embedding_dim)] for _ in range(vocab_size)]  # &lt;- the trainable lookup!\n\n    def forward(self, inputs: list[int]):\n        out = []\n        for inp in inputs:\n            out.append(self.weight[inp])\n        return out\n    ...\n</code></pre> <p>From this simple building block, you can make things as complicated as you want. The <code>BertEmbeddings</code> class in the <code>transformers</code> library shows you that for transformers, you will need to encode the position information of the text as well among other things. </p>"},{"location":"blog/2024/01/05/self-attention/#positional-embeddingencoding","title":"Positional Embedding/Encoding","text":"<p>The original transformers paper also embeds the positional information into the input sequence to the self attention layers. What  does positional embeddings do? Imagine if we didn't have the positional embeddings. When the self-attention mechanism is presented in the next section, you'll realise that each token embedding is treated independently of its position in a sentence. We need a way to \"inject\" the fact that a token's position is important before the embeddings enter as input into the self attention mechanism, where they will be further projected into many more different kind of vectors in different spaces. </p> <p>A motivating example would be something like processing a sentence - <code>a person ate the fish</code>. This has a completely different meaning to another ordering of the words - <code>a fish ate the person</code> - the positions of the words <code>fish</code> and <code>person</code> change the meaning of the phrases, and we need a mechanism to preserve that.</p> <p>The way the original transformers paper did this was by using sinusoidal encodings. The scheme they use is something like </p> \\[ \\text{PE}_{(pos, 2i)} = sin(\\frac{pos}{10000^{\\frac{2i}{d_\\text{model}}}}) \\] <p>for tokens in the even indexes of the hidden dimension, and</p> \\[ \\text{PE}_{(pos, 2i + 1)} = cos(\\frac{pos}{10000^{\\frac{2i}{d_\\text{model}}}}) \\] <p>for tokens in the odd index of the hidden dimension. </p> <p>These encodings would then be added to the learned embeddings. </p> <p>In pure python code, this would look something like this</p> <pre><code>def positional_encoding(seq_len: int, d_model: int) -&gt; list[list[float]]:\n    \"\"\"Given a sequence length, and a model embedding size, return a matrix\n    of seq_len x d_model encoding the positions\n\n    Args:\n        seq_len: number of tokens\n        d_model: dimensions of the input to the embedding vector\n\n    Returns:\n        a matrix of seq_len x d_model\n    \"\"\"\n    def _get_angle(pos, d_i):\n        denom = 10_000 ** ((2 * (d_i // 2)) / d_model)\n        return pos / denom\n\n    out = []\n    for pos in range(seq_len):\n        row = []\n        for d_i in range(d_model):\n            _fn = math.sin if d_i % 2 == 0 else math.cos\n            angle = _get_angle(pos, d_i)\n            row.append(_fn(angle))\n        out.append(row)\n\n    return out\n</code></pre> <p>When you plot it for <code>seq_len = 10</code> and <code>d_model = 64</code> (just toy values), you will get something like </p> <p> You can look at Jay Alamar's code to see how to do this in a vectorised manner, which I have reimplemented here in numpy: </p> <pre><code>def sinusoidal_encodings(pos, dim, base=10000):\n    \"\"\"Interleaved sinusoidal position embeddings.\n\n    Like in the original transformer paper - interleaved sin and cos.\n    \"\"\"\n    indices = np.arange(0, dim // 2)\n    power_term = np.power(base, -2 * indices / dim)\n    angle = np.einsum(\"...,d-&gt;...d\", pos, power_term)\n    encodings = np.stack([np.sin(angle), np.cos(angle)], axis=-1)\n    encodings = encodings.reshape(list(encodings.shape[:-2]) + [-1])\n    return encodings\n</code></pre> <p>In the old tensor2tensor library, the way positional encoding was done was actually by not interleaving the sines and cosines, but instead by doing something like this instead:</p> <pre><code>def positional_encoding_concat(seq_len: int, d_model: int) -&gt; list[list[float]]:\n    \"\"\"Given a sequence length, and a model embedding size, return a matrix\n    of seq_len x d_model encoding the positions.\n\n    The difference this time is that you don't interleave the sines and the cosines\n    but follow the tensor2tensor style of just concatenating sines and cosines\n\n    Args:\n        seq_len: number of tokens\n        d_model: dimensions of the input to the embedding vector\n\n    Returns:\n        a matrix of seq_len x d_model\n    \"\"\"\n    def _get_angle(pos, d_i):\n        denom = 10_000 ** (d_i / (d_model/2))\n        return pos / denom\n\n    out = []\n    for pos in range(seq_len):\n        row = []\n        for d_i in range(d_model//2):\n            angle = _get_angle(pos, d_i)\n            row.append(math.sin(angle))\n        for d_i in range(d_model//2):\n            angle = _get_angle(pos, d_i)\n            row.append(math.cos(angle))\n        out.append(row)\n\n    return out\n</code></pre> <p>for which you'd get the plot  The main difference being in 1. the <code>_get_angle</code> method 2. the for loop where the values are being appended</p> <p>According to the Tensorflow tutorial on transformers, these 2 are functionally equivalent. You can see why - the main aim of positional encodings is to make sure that embeddings that are near each other have a \"similar\" positional encoding, while those far away have different positional encodings. The second implementation is easier to do in a vectorised manner.</p> <p>The advantage of positional encodings that use such a sinusoidal pattern is that it can generalise to sequence lengths not seen before during training, because it is a deterministic function with respect to the token's position in the input.</p> <p>So the final embedding layer that includes positional encoding will look something like: </p> <pre><code>import random \n\nclass PurePythonEmbeddingLayer:\n    def __init__(self, vocab_size: int, embedding_dim: int):\n        self.weight = [[random.random() for _ in range(embedding_dim)] for _ in range(vocab_size)]  # &lt;- the trainable lookup!\n        self.embedding_dim = embedding_dim\n\n    def forward(self, inputs: list[int]):\n        out = []\n        positional_encodings = positional_encoding_concat(seq_len=len(inputs), d_model=self.embedding_dim)\n        for i, inp in enumerate(inputs):\n            token_embedding = self.weight[inp]\n            positional_encoding = positional_encodings[i]\n            out.append([_t + _p for _t, _p in zip(token_embedding, positional_encoding)])\n        return out\n    ...\n</code></pre> <p>Why don't we just make the model learn how to embed position instead of hardcoding it?  The answer to that is, you can! The BERT paper, which basically takes the original transformer paper and scales it up immensely, does precisely this. The <code>positional_embedding</code> is just another learnable parameter in the BERT encoder, as opposed to the deterministic vector you see in the original transformer paper. This increases the number of parameters a model has. The disadvantage of a scheme like this is that at inference time, you cannot have sequence lengths that are longer than what the model has been trained at </p> <p>Other more sophisticated embedding schemes include rotary positional embeddings (RoPE). RoPE is what the later versions of the GPT class of models use. This will not be discussed in greater detail here. </p> <p>Now that you have the embeddings ready, we are finally in a position to explain how the self attention mechanisms works. </p>"},{"location":"blog/2024/01/05/self-attention/#self-attention","title":"Self-attention","text":""},{"location":"blog/2024/01/05/self-attention/#why-self-attention-why-not-just-use-fully-connected-layers","title":"Why self-attention? Why not just use fully connected layers?","text":"<p>The UDL book makes a very compelling argument, that will be reproduced in part here. </p> <p>Here's an example of an actual review from Amazon:</p> <pre>\nin 1989, I managed a crummy bicycle shop, \"Full Cycle\" in Boulder, Colorado. The Samples had just recorded this album and they played most nights, at \"Tulagi's\" - a bar on 13th street. They told me they had been so broke and hungry, that they lived on the free samples at the local supermarkets - thus, the name. i used to fix their bikes for free, and even feed them, but they won't remember. That Sean Kelly is a gifted songwriter and singer.\n</pre> <p>There are immediately 3 observations you can make about using just fully connected layers:</p> <ol> <li>This is 83 word review, and roughly 110 tokens (remember - everything is in terms of token length!) worth of text. Imagine now that each token is represented by a vector of size 1024 (the embedding dimension \\(\\textbf{D}\\)). If you were to have a model consisting only of fully connected layers, each layer would contain \\(N^2D^2\\)  parameters, and with \\(k\\) such layers, you'd have \\(kN^2D^2\\) such parameters. This is a lot of parameters.</li> <li>Each review on Amazon can have a variable number of inputs. How would you take that into account when designing a model consisting only of fully connected layers? You could technically just fix the upper bound of N to some arbitrarily large value, and fill the positions with no text with some arbitrary \"pad\" tokens, but this feels like an inefficient way of learning representations. </li> <li>Language is inherently ambiguous, and it is unclear from syntax alone what the semantic meaning of each token is. <ol> <li>In the example above, the word <code>That</code> in the last sentence doesn't specifically refer to any object or subject. </li> <li>This means that we want an architecture where there must be connections between word representations and that the strength of these connections should depend on the words themselves. </li> <li>Technically you could have fully connected layers that do the same thing. In such a case, you'd hope the superfluous weights go to zero, and all weights that connect neurons that belong to the same group of tokens to other such groups would all be the same during the learning process. But this is an incredibly inefficient way of learning. This is probably easier to see in the diagram below. <ol> <li>In the top part, you want all the lines of the same colour to be the same weight.</li> <li>In the second part, you see that it is also technically possible to do this with fully connected layers, but this adds a lot of parameters for training without adding any real value, because we know the constraints we would like them to fulfill. </li> </ol> </li> </ol> </li> </ol> <p></p>"},{"location":"blog/2024/01/05/self-attention/#self-attention-block","title":"Self attention block","text":"<p>We can now turn our attention to what the self attention block does. We will proceed from the outputs of the embedding layer as input to this block</p> <p>If we were to use a standard neural network, each \\(D\\times1\\) input would have a linear transformation applied to it, followed by a non-linear activation function (softmax, ReLU etc.) applied to it. We can represent the mapping of a standard neural network on an input \\(\\mathbf{x}\\) as </p> \\[ f(\\mathbf{x}) = \\text{someNonLinearFunction}(\\mathbf{Ax + b}) \\] <p>How is a self attention block different? The simplest way to think of a self attention block is in 2 steps.</p> <p>Assume an input of \\(N\\) vectors \\(x_1, x_2, .... x_N\\), each of dimension \\(D\\). </p> <p>The first step in an a self-attention block is the following - for each of the \\(N\\) vectors, a corresponding value is computed using the standard way - </p> \\[ \\mathbf{v}_m = \\mathbf{A}_v\\mathbf{x}_m + \\mathbf{b}_v \\] <p>where \\(\\mathbf{A} \\in \\mathbb{R}^{D\\times D}, \\mathbf{b} \\in \\mathbb{R}^{D}\\) , are shared across all input vectors. You can see this in the description section of the original transformers paper. </p> <p>The second step is then computing a weighted sum across this set of values \\(\\mathbf{v}_1,  ..., \\mathbf{v}_N\\) for each \\(i \\in [1, N]\\):</p> \\[ \\text{out}_i = \\sum_{j=1}^{N}a(\\mathbf{x}_j, \\mathbf{x}_i)\\mathbf{v}_j \\] <p>where \\(a(\\mathbf{x}_j, \\mathbf{x}_i)\\) is the attention that the \\(i^{th}\\) output pays to the \\(j^{th}\\) input. A weighted sum means that for each \\(i\\), the sum \\(\\sum_{j=1}^{N}a(\\mathbf{x}_j, \\mathbf{x}_i) = 1\\) , and each of the weights \\(a(\\mathbf{x}_j, \\mathbf{x}_i)\\) is non-negative.</p> <p>So what is the attention function? It's two inputs - the first is \\(\\mathbf{x}_m\\), i.e. the \\(m^{th}\\) input vector, whose corresponding value vector \\(\\mathbf{v}_m\\) is what the result of the attention computation will be multiplied with. The second is \\(\\mathbf{x}_i\\), the \\(i^{th}\\) input vector, where \\(i\\) is also the position of the output vector we are trying to compute. We will need to compute 2 linear transformations first - the first being</p> \\[ \\mathbf{q}_i = \\mathbf{A}_q\\mathbf{x}_i + \\mathbf{b}_q \\] <p>which is called the query value, computed on the output's \\(i^{th}\\) position. The second linear transformation is </p> \\[ \\mathbf{k}_m = \\mathbf{A}_k\\mathbf{x}_m + \\mathbf{b}_k \\] <p>which is called the key value, computed on the \\(m^{th}\\) input vector. </p> <p>The attention function \\(a(\\mathbf{x}_j, \\mathbf{x}_i)\\) will then be defined as:</p> \\[ a(\\mathbf{x}_j, \\mathbf{x}_i) := \\text{softmax}_m(\\mathbf{k}^T_{\\circ}\\mathbf{q}_i) \\\\ = \\frac{\\text{exp}(\\mathbf{k}^T_j\\mathbf{q}_i)}{\\sum_{j'=1}^{N}\\text{exp}(\\mathbf{k}_{j'}^T\\mathbf{q}_i)} \\] <p>The following diagram from the UDL book is instructive to visualise the 2 different stages of calculating the </p> <p></p> <p>All this seems pretty unintuitive - what is the correct \"mental model\" to have for the key, query, and value vectors we have just introduced? I'm going to modify the insight Eugene Yan's blog here has on this - </p> <ol> <li>Say you are in a library, and you want some knowledge. To get knowledge, you basically want the answers to N queries you have. </li> <li>Now in the library, there are a lot of books. Each book has a spine with its title, which you can think of as its key, and each book has some content, which is it's value. </li> <li>Now, for each query you have, you look at the spine of each book to decide how much attention should give to the contents of that book for that particular query, and you do it for all books present in the library.</li> </ol> <p>Keys and values tend to be computed on the same input matrices, but query values are often computed on other input matrices (it is called self-attention when all these operations are on the same inputs). As an example of these operations being applied to different inputs, you have to look no further than the \"Attention is all you need\" paper, particularly this diagram: </p> <p></p> <p>In the decoder, at the point in the model where the decoder takes into account the encoder outputs as well, the key and values will be computed as:</p> \\[ K = A_kX_{\\text{enc}} + B_k \\] \\[ V = A_vX_{\\text{enc}} + B_v \\] <p>while the queries will be on the decoder representation of the outputs:</p> \\[ Q = A_qX_{\\text{dec}} + B_q \\] <p>where \\(X_{\\text{enc}}, X_{\\text{dec}}\\) are the matrices as illustrated in the diagram above - \\(X_{\\text{enc}}\\) being the encoder  output, and \\(X_{\\text{dec}}\\) being the decoder representation of the output embedding at that layer (assume column first ordering for this series of equations). </p> <p>This is sometimes called cross-attention, and if you inspect the Huggingface code, you will see this term cropping up. Let's focus just on self-attention again.</p> <p>What does this all look like in code? You can look at the notebooks provided by the author of the UDL book for a clearer understanding, and I'm going to reproduce something very similar to the book here.</p> <p>For our toy example, let's assume the output of the embeddings are 3 input vectors \\(\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3\\) , each with dimension \\(D = 4\\).  </p> <pre><code>import numpy as np\n# Set seed so we get the same random numbers\nnp.random.seed(3)\n# Number of inputs\nN = 3\n# Number of dimensions of each input\nD = 4\n\nall_x = []\n# Create elements x_n and append to list\nfor n in range(N):\n  all_x.append(np.random.normal(size=(D,1)))  # &lt;- doesn't really matter for this toy example\n\nprint(all_x)\n</code></pre> <p>Now, let's initialise the weights for \\(A_q, A_k, A_v\\) and biases \\(b_q, b_k, b_v\\): </p> <pre><code># all the weights are of dimension DxD \nA_q = np.random.normal(size=(D,D))\nA_k = np.random.normal(size=(D,D))\nA_v = np.random.normal(size=(D,D))\n\n# all the biases are of dimension Dx1\nb_q = np.random.normal(size=(D,1))\nb_k = np.random.normal(size=(D,1))\nb_v = np.random.normal(size=(D,1))\n</code></pre> <p>We can now compute the keys, queries, and values for each of inputs:</p> <pre><code># Make three lists to store queries, keys, and values\nall_queries = []\nall_keys = []\nall_values = []\n# For every input\nfor x in all_x:\n  query = A_q @ x + b_q\n  key = A_k @ x + b_k \n  value = A_v @ x + b_v\n\n  all_queries.append(query)\n  all_keys.append(key)\n  all_values.append(value)\n</code></pre> <p>Side note regarding multiplying numpy arrays: the <code>@</code> operator is a standard matrix multiplication operation, and requires 2 matrices of dimensions \\(A \\times B\\),  and \\(B \\times C\\), and will result in a \\(A\\times C\\) matrix. The <code>*</code> operator can only be used if your matrix pair \\(M_1, M_2\\) are either vectors or square matrices, and is often not what you want. </p> <p>We then need the softmax function: <pre><code>def softmax(items_in: list):\n  e_x = np.exp(items_in - np.max(items_in))\n  return e_x / e_x.sum()\n</code></pre> Subtracting the maximum value in the exponent is a common technique to make sure the values in the numerator (<code>e_x</code> in the snippet above) do not become too large, and you have to start worrying about things like overflow etc. in the underlying programming language. Functionally they don't change the softmax function, since</p> \\[ \\frac{e^{x_i}}{\\sum^{N}_{i=1}{e^{x_i}}} = \\frac{(\\frac{1}{e^C})e^{x_i}}{(\\frac{1}{e^C})\\sum^{N}_{i=1}{e^{x_i}}} = \\frac{e^{x_i - C}}{\\sum^{N}_{i=1}{e^{x_i - C}}} \\] <p>For a pretty interesting discussion about why we use softmax, and the interpretation of this non-linear function, feel free to look at Pinecone's handy guide here.  </p> <p>We are now in a position to compute each of the outputs. As a reminder, we are going to have \\(N\\) output vectors \\(\\text{out}_1, \\text{out}_2, \\text{out}_3, ... \\text{out}_N\\), each of which is a weighted average of the value vectors. </p> <pre><code># Make three lists to store queries, keys, and values\nall_queries = []\nall_keys = []\nall_values = []\n# For every input\nfor x in all_x:\n  query = A_q @ x + b_q\n  key = A_k @ x + b_k \n  value = A_v @ x + b_v\n\n  all_queries.append(query)\n  all_keys.append(key)\n  all_values.append(value)\n\n#\u00a0compute the outs\nall_outs = []\n\nfor i in range(N):\n    all_kj_qi = [] # &lt;-- will be a 1 x N vector\n    q_i = all_queries[i]\n    for key_j in all_keys:\n        dot_product = np.dot(key_j.T, q_i).squeeze()\n        all_kj_qi.append(dot_product)\n\n    attention = softmax(all_kj_qi) # &lt;-- 1 x N vector that sums to 1\n    out_i = sum(attention[i] * all_values[i] for i in range(N))\n    all_outs.append(out_i)\n</code></pre> <p>And that is it! You have basically implemented the self-attention mechanism from scratch using just (mostly) raw python loops! If you want to involve more matrix operations, you can do it the following way: </p> <pre><code>def softmax_cols(data_in):\n    # Exponentiate all of the values\n    _data_in = data_in - np.max(data_in, axis=1, keepdims=True)\n    exp_values = np.exp(_data_in)\n    # Sum over rows\n    denom = np.sum(exp_values, axis=1, keepdims=True)\n    # Compute softmax\n    softmax = exp_values / denom\n    # return the answer\n    return softmax\n\ndef self_attention(X, A_v, A_q, A_k, b_v, b_q, b_k):\n    \"\"\"Self attention in a vectorized manner\n\n    Assumption here is that each column of X is a data point. In literature, each data point is usually a row, and not a column. Doesn't change the main thrust of this function\n\n    Args:\n        X: X is a DxN matrix, where D is the dimension of the input vectors, and N is the number of input vectors\n        A_v: A_v is a DxD matrix\n        A_q: A_q is a DxD matrix\n        A_k: A_k is a DxD matrix\n        b_v: b_v is a Dx1 vector\n        b_q: b_q is a Dx1 vector\n        b_k: b_k is a Dx1 vector\n\n    Returns:\n        a DxN matrix, where each column is the output of the self attention mechanism\n    \"\"\"\n    # 1. Compute queries, keys, and values\n    Q = X @ A_q.T + b_q.T\n    K = X @ A_k.T + b_k.T\n    V = X @ A_v.T + b_v.T\n    # 2. Compute dot products\n    dot_pdts = Q @ K.T\n    # 3. Apply softmax to calculate attentions\n    attention = softmax_cols(dot_pdts)\n    print(attention.shape) # &lt;-- This will now be a NxN matrix!\n    # 4. Weight values by attentions\n    out = attention @ V\n\n    return out\n\nX = np.array(all_x).squeeze()\nout = self_attention(X, A_v, A_q, A_k, b_v, b_q, b_k)\n</code></pre> <p>This function presents the famous self-attention equation:</p> \\[ \\text{attentionOutput}(V, Q, K) = V\\text{softmax}(QK^T) \\] <p>more naturally. </p> <p>If you inspect the values of the \\(N\\times N\\) <code>attention</code> matrix, you'll notice the extreme values - some values are very close to 1, and many values are nearly 0. This is because there is a large variance in the values of \\(QK^T\\) - they become either too big a positive value, or too big a negative value. </p> <p>We ideally want to scale the values in the attention such that the variance in the input values to the softmax function is reduced to avoid the vanishing gradient problem. A hand-wavy justification for this is the following:</p> <p>Weights in a neural network are updated using backpropagation (you can read up the details of this method elsewhere). Let the \\(i^{th}\\) output of the softmax function be defined as \\(p_i\\) where</p> \\[ p_i = \\frac{e^{x_i}}{\\sum^{N}_{i=1}{e^{x_i}}} \\] <p>Then, when we look at the gradients of this function, we can see that the partial derivative of \\(p_i\\) with respect to \\(x_i\\) is -</p> \\[ \\frac{\\partial p_i}{\\partial x_i} = \\frac{\\partial}{x_i} \\left( \\frac{e^{x_i}}{e^{x_i}+ C} \\right) = \\frac{(e^{x_i} + C)e^{x_i} - e^{x_i}e^{x_i}}{(e^{x_i} + C)^2} = (\\frac{e^{x_i}}{e^{x_i}+ C})(\\frac{C}{e^{x_i}+ C}) = p_i(1 - p_i) \\] <p>where we use \\(C\\) in that hand-wavy way non-mathematicians use to express terms that can be treated as a \"constant\" in a partial derivative. Then for \\(j\\neq i\\), </p> \\[ \\frac{\\partial p_i}{\\partial x_j} = \\frac{\\partial}{x_i}\\left(\\frac{C'}{e^{x_j}+ C}\\right) = \\frac{-C'e^{x_j}}{(e^{x_j}+ C)^2} = -p_jp_i \\] <p>The last characteristic you need to remember about the softmax is the following: </p> \\[ \\sum_{i=1}^N p_i = 1 \\] <p>So if any \\(p_i\\) is very close to one, the partial derivatives will be close to 0 because the other term (either \\(1 - p_i\\) or \\(p_j\\)) is going to be very close to 0. Naturally, the partial derivatives will also be close to 0 if any of the \\(p_i\\)'s is close to 0. </p> <p>Scaling the inputs the softmax function is typically done by dividing the \\(\\mathbf{Q}\\mathbf{K^T}\\) result with \\(\\sqrt{D_k}\\) , i.e. the dimension of the keys (and the dimension of the queries). Why this particular constant? This is explained in the paper. As the dimensions of the keys and the queries increase, it is likely that the final result of \\(\\mathbf{Q}\\mathbf{K^T}\\) increases in value. Intuitively, this is because if \\(q\\) and \\(k\\) are independent random variables with 0 mean and 1 variance, the dot product \\(q \\cdot k = \\sum_{i=1}^{d_k} q_ik_i\\) will have mean 0 and variance \\(d_k\\).  This is easy to derive from first principles, remembering that \\(\\text{Var}[x]) = \\mathbb{E}[x^2] - (\\mathbb{E}[x])^2\\) . The mean calculation of \\(\\mathbf{k}^T \\cdot \\mathbf{q}\\) is the following (with \\(D\\) being the dimension of the key and the query)</p> \\[ \\mathbb{E}[\\mathbf{k}^T \\cdot \\mathbf{q}] = \\mathbb{E}\\left[ \\sum_{i=1}^{D} k_iq_i \\right] = \\sum_{i=1}^D \\mathbb{E}[q_ik_i] \\] <p>and since \\(k_i, q_i\\) are independent, you have </p> \\[ \\sum_{i=1}^D \\mathbb{E}[q_ik_i] = \\sum_{i=1}^D \\mathbb{E}[q_i]\\mathbb{E}[k_i] = 0 \\] <p>As for variance we need to consider the following:  </p> \\[ \\text{Var}[\\mathbf{k}^T \\cdot \\mathbf{q}] = \\mathbb{E}\\left[(\\mathbf{k}^T \\cdot \\mathbf{q})^2\\right] - \\left(\\mathbb{E}[\\mathbf{k}^T \\cdot \\mathbf{q}] \\right)^2 \\] <p>We only have to consider the first term of the right hand side, because as we've just established, \\(\\mathbb{E}[\\mathbf{k}^T \\cdot \\mathbf{q}]=0\\). Given that each of these variables have variance 1, this means that:</p> \\[ \\text{Var}(k_i) = 1 =&gt; \\mathbb{E}[k_i^2] - (\\mathbb{E}[k_i])^2 = 1 \\] <p>and since \\(\\mathbb{E}[k_i]=0\\), we know \\(\\mathbb{E}[k_i^2] = 1\\). The same holds for \\(q_i\\).</p> <p>So, it easily follows:</p> \\[ \\text{Var}[\\mathbf{k}^T \\cdot \\mathbf{q}] = \\mathbb{E}\\left[(\\mathbf{k}^T \\cdot \\mathbf{q})^2\\right] = \\mathbb{E}\\left[ \\left( \\sum_{i=1}^Dk_iq_i \\right) \\left( \\sum_{j=1}^Dk_jq_j \\right) \\right] =\\left[ \\sum_{i=1}^D \\sum_{j=1}^D  \\mathbb{E}(q_iq_jk_ik_j)\\right] \\] <p>and since the \\(k\\)'s and the \\(q\\)'s are independent:</p> \\[ \\left[ \\sum_{i=1}^D \\sum_{j=1}^D  \\mathbb{E}(q_iq_jk_ik_j)\\right] = \\left[ \\sum_{i=1}^D \\sum_{j=1}^D  \\mathbb{E}(q_iq_j)\\mathbb{E}(k_ik_j)\\right] \\] <p>now, for cases where \\(i \\neq j\\), you get \\(\\mathbb{E}(q_iq_j) = \\mathbb{E}(q_i)\\mathbb{E}(q_j) = 0\\), and so you only have to care about the cases where \\(i = j\\). This then simplifies everything to: </p> \\[  \\left[ \\sum_{i=1}^D \\sum_{j=1}^D  \\mathbb{E}(q_iq_j)\\mathbb{E}(k_ik_j)\\right] = \\sum_{i=1}^D \\mathbb{E}(q_i^2) \\mathbb{E}(k_i^2) =\\sum_{i=1}^D 1= D \\] <p>So to summarise this last part - the reason we scale everything by \\(\\sqrt{D_k}\\) is because under the assumption that \\(k\\)'s and \\(q\\)'s are independent variables with 0 mean and unit variance, this is the scaling factor we need to keep the variance of </p> \\[ \\mathbf{k}^T \\cdot \\mathbf{q} \\] <p>to 1, and the mean to be 0. </p> <p>The self attention block is the basic unit of the transformer, and its details are often not appreciated. Hopefully, this post has given you a better intuition for it by decomposing every step into its most basic form.  </p>"},{"location":"blog/2024/01/04/manacher-algo/","title":"Manacher's Algorithm","text":"<p>Super annoying algorithm, but it has uses in bioinfomatics. </p> <p>Here's the task: Given a string\u00a0<code>s</code>, return\u00a0the longest palindromic substring \u00a0in\u00a0<code>s</code>.</p> <p>Example inputs and outputs:</p> <p>Input: s = \"babad\" Output: \"bab\" Explanation: \"aba\" is also a valid answer.</p> <p>Input: s = \"cbbd\" Output: \"bb\"</p> <p>To build some intuition for how this algorithm works, let's see an example of a brute force implementation. In this implementation, given a string <code>_str</code>, we want to see for any <code>i</code>, what is the maximum length of a palindrome centred around the character <code>_str[i]</code>. Something simple would be something like this: </p> <pre><code>def manacher_brute_force(_str: str) -&gt; list:\n  _str_len = len(_str)\n  out = [0 for _ in range(_str_len + 2)]\n  # make sure first and last chars are different and do not\n  #\u00a0happen in _str itself\n  _str = f\"${_str}^\"\n\n  for i in range(1, _str_len + 1):\n\n    while _str[i - out[i]] == _str[i + out[i]]:\n      out[i] += 1\n  return out\n\nif __name__ == \"__main__\":\n  print(manacher_brute_force(\"abbcbba\"))\n</code></pre> <p>the output will be something like <code>[0, 1, 1, 1, 4, 1, 1, 1, 0]</code></p> <p>But in this approach, when we sweep from left to right, we are not using the work we have already done to the left of <code>i</code>. This is where Manacher comes in. </p>","tags":["algorithms"]},{"location":"blog/2024/01/04/manacher-algo/#what-dont-we-have-to-search","title":"What don't we have to search?","text":"<p>We can exploit the nature of a palindrome - assume we have a palindrome of length <code>l</code> centred around index <code>i</code>, and say we take 2 indexes <code>i'</code> and <code>i''</code> that are distance <code>d</code> left and right to <code>i</code> respectively s.t. <code>d &lt; l</code>, then we basically know that any palindrome that is centred around <code>i'</code> will also likely be centred around <code>i''</code>!</p> <p>From this simple observation, we can already amend the inner <code>while</code> loop so that we are not searching all of the characters to the left and right of a particular index <code>i''</code> of an index that is to the right of an index <code>i</code> we have already done work for.  </p> <p>The rest of the complexity comes from the need to handle the case where the borders of the inner palindrome reaches the border of the outer palindrome. All you have to do there is make sure you always check whenever you go beyond the borders of the longest current palindrome. </p> <pre><code>def manacher_odd(_str: str) -&gt; list:\n    _str_len = len(_str)\n    out = [0 for _ in range(_str_len + 2)]\n    # make sure first and last chars are different and do not\n    #\u00a0happen in _str itself\n    _str = f\"${_str}^\"\n    l, r = 1, 1\n    for i in range(1, _str_len + 1):\n        dist_to_border = r - i\n        inner_palindrome_len = min(dist_to_border, out[l + dist_to_border])\n        out[i] = max(0, inner_palindrome_len)\n        while _str[i - out[i]] == _str[i + out[i]]:\n          out[i] += 1\n        if i + out[i] &gt; r:\n            l = i - out[i]\n            r = i + out[i]\n    return out\n</code></pre> <p>Code is a translation of the cpp code here</p> <p>NOTE:</p> <p>The above algorithm is only for odd length. In practice, you can make any string odd length by doing something like </p> <pre><code>_str = \"\u00a3\".join(_str)\n_str = f\"#{_str}^\"\n</code></pre> <p>the <code>.join</code> adds <code>n-1</code> characters to the string, so the total number of characters will be odd, since <code>even + odd = odd</code></p>","tags":["algorithms"]},{"location":"blog/2024/01/04/manacher-algo/#where-does-the-time-saving-come-from-that-makes-it-linear","title":"Where does the time saving come from that makes it linear?","text":"<p>In the brute force method, consider the number of times a character at index <code>i</code> is compared to some other character. You will quickly realise it is <code>O(n)</code>, and that is where the overall \\(O(n^2)\\) complexity for the brute force method comes from. </p> <p>But with Manacher's algorithm, the while loop is no longer independent of the outer for loop. The outer loop is keeping track of the <code>centre</code> of palindromes and is always increasing. We only do additional comparison operations when <code>r</code> variable (i.e. the rightmost boundary, the <code>centre + radius</code> value) increases - and this quantity never decreases in value! Therefore, the total number of operations in the outer and in the inner loop adds to <code>n</code>. </p>","tags":["algorithms"]},{"location":"blog/2024/01/04/manacher-algo/#final","title":"Final:","text":"<pre><code>def manacher(s: str) -&gt; str:\n        if len(s) &lt;= 1:\n            return s\n\n        s = f\"#{'#'.join(s)}#\"\n        s_len = len(s)\n        out = [0 for _ in range(s_len)]\n        max_radius = 1\n        max_str = s[1]\n\n        l, r = 1, 1\n        for i in range(1, s_len - 1):\n            dist_to_edge = r - i\n            allowable_dist = min(dist_to_edge, out[l + dist_to_edge])\n            out[i] = max(0, allowable_dist)\n            while i - out[i]&gt;= 0 and i + out[i] &lt; s_len and s[i - out[i]] == s[i + out[i]]:\n                out[i] += 1\n            if i + out[i] &gt; r:\n                r = i + out[i]\n                l = i - out[i]\n            if out[i] &gt; max_radius:\n                max_radius = out[i]\n                max_str = s[l + 1 : r].replace(\"#\", \"\") # if you want s[l: r + 1] you need to offset in the while loop like so: \n                # while i - out[i] - 1&gt;= 0 and i + out[i] + 1 &lt; s_len and s[i - out[i] - 1] == s[i + out[i] + 1]\n        return max_str\n</code></pre> <p>or if you want to use the original <code>manacher_odd</code> kind of notation:</p> <pre><code>    def manacher(s: str) -&gt; str:\n        if len(s) &lt;= 1:\n            return s\n\n        s = f\"\u00a3#{'#'.join(s)}#^\"\n        s_len = len(s)\n        out = [0 for _ in range(s_len)]\n        max_radius = 1\n        max_str = s[1]\n\n        l, r = 1, 1\n        for i in range(1, s_len - 1):\n            dist_to_edge = r - i\n            allowable_dist = min(dist_to_edge, out[l + dist_to_edge])\n            out[i] = max(0, allowable_dist)\n            while s[i - out[i]] == s[i + out[i]]:\n                out[i] += 1\n            if i + out[i] &gt; r:\n                r = i + out[i]\n                l = i - out[i]\n            if out[i] &gt; max_radius:\n                max_radius = out[i]\n                max_str = s[l + 1 : r].replace(\"#\", \"\")\n        return max_str\n</code></pre>","tags":["algorithms"]},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/algorithms/","title":"algorithms","text":""}]}